{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Utilizando o Gemini para cenários de educação\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/education/use_cases_for_education.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Executar no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/education/use_cases_for_education.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> Ver no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/education/use_cases_for_education.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Abrir no Workbench da Vertex AI\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Visão Geral\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini é uma família de modelos generativos de IA desenvolvidos pelo Google DeepMind e projetados para casos de uso multimodais. A API Gemini dá acesso aos modelos Gemini Pro Vision e Gemini Pro.\n",
    "\n",
    "### API Vertex AI Gemini\n",
    "\n",
    "A API Vertex AI Gemini fornece uma interface unificada para interagir com modelos Gemini. Atualmente existem dois modelos disponíveis na API Gemini:\n",
    "\n",
    "- **Modelo Gemini Pro** (`gemini-pro`): Projetado para lidar com tarefas de linguagem natural, bate-papo multivoltas de texto e código e geração de código.\n",
    "- **Modelo Gemini Pro Vision** (`gemini-pro-vision`): Suporta prompts multimodais. Você pode incluir texto, imagens e vídeo em suas solicitações de prompt e obter respostas em texto ou código.\n",
    "\n",
    "Você pode interagir com a API Gemini usando os seguintes métodos:\n",
    "\n",
    "- Use o [Vertex AI Studio](https://cloud.google.com/generative-ai-studio) para testes rápidos e geração de comandos\n",
    "- Use o SDK da Vertex AI\n",
    "\n",
    "Este notebook se concentra no uso do **Vertex AI SDK para Python** para chamar a API Vertex AI Gemini.\n",
    "\n",
    "Para obter mais informações, consulte a documentação [IA Generativa na Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVL_vGs4q3pg"
   },
   "source": [
    "### Objetivos\n",
    "\n",
    "O principal objetivo deste notebook é demonstrar uma variedade de casos de uso educacionais que podem se beneficiar dos modelos Gemini.\n",
    "\n",
    "As etapas executadas incluem:\n",
    "\n",
    "- Instalando o Python SDK\n",
    "- Usando a API Vertex AI Gemini\n",
    "   - Usando um modelo de texto (`gemini-pro`)\n",
    "     - Raciocínio em diferentes níveis\n",
    "     - Raciocínio sobre o texto\n",
    "     - Raciocínio sobre números\n",
    "   - Usando um modelo multimodal (`gemini-pro-vision`)\n",
    "     - Raciocínio sobre uma única imagem\n",
    "     - Raciocínio sobre múltiplas imagens\n",
    "     - Raciocínio em um vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRdtKLfTsQ27"
   },
   "source": [
    "### Custos\n",
    "\n",
    "Este tutorial usa os seguintes componentes de Google Cloud que podem gerar custos em sua fatura:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Saiba mais sobre [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing) e use a [calculadora de preços](https://cloud.google.com/products/calculator/) para gerar uma estimativa de custo com base no uso projetado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeiros passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBGrQE22sVrt"
   },
   "source": [
    "### Instale a SDK da Vertex AI\n",
    "**Importante:** só descomente a linha abaixo se você **não estiver** executando este laboratório no Qwiklabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hqq5vomsW_P",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### **Reinicie o kernel do seu jupyter notebook** \n",
    "\n",
    "Como a instalação está sendo realizada com a opção `--user` é necessário reiniciar o kernel para que os novos módulos se tornem acessíveis.\n",
    "\n",
    "**Importante:** só descomente a linha abaixo se você **não estiver** executando este laboratório no Qwiklabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ O kernel do notebook está sendo reiniciado. Por favor aguarde este processo ser finalizado antes de continuar com os próximos passos. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### **Somente para uso no Colab - Autentique o seu ambiente de notebook** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para realizar a autenticação da sua sessão de notebook com a Google Cloud Esse passo é importante **para utilização no Colab** para garantir que as chamadas a APIs de Google Cloud funcionem sem problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab4Y6eSIUknb"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Additional authentication is required for Google Colab\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Authenticate user to Google Cloud\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc4WxYmLSBW5"
   },
   "source": [
    "### **Somente para uso no Colab - defina o projeto Google Cloud a ser utilizado** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para definir qual projeto Google Cloud será utilizado pelo Colab na execução deste notebook. Senão, siga para as próximas instruções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Define project information\n",
    "#     PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "#     LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "#     # Initialize Vertex AI\n",
    "#     import vertexai\n",
    "\n",
    "#     vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdINrwJZsj1d"
   },
   "source": [
    "### Importe as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1702337383496,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "stNmWCsRsotM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v63w6fWs9Dx"
   },
   "source": [
    "### Defina algumas funções auxiliares\n",
    "\n",
    "Defina funções auxiliares para carregar e exibir imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702337383496,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "OGvJLH4DmZfw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import io\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "Contents = str | list[str | Image | Part]\n",
    "\n",
    "\n",
    "def generate_content(\n",
    "    model: GenerativeModel,\n",
    "    contents: Contents,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 0.0,\n",
    "    top_k: int = 1,\n",
    ") -> list[GenerationResponse]:\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        candidate_count=1,\n",
    "        max_output_tokens=2048,\n",
    "    )\n",
    "\n",
    "    responses = model.generate_content(\n",
    "        contents,\n",
    "        generation_config=generation_config,\n",
    "        stream=True,\n",
    "    )\n",
    "    return [responses] if isinstance(responses, GenerationResponse) else list(responses)\n",
    "\n",
    "\n",
    "def print_contents(contents: Contents):\n",
    "    if not isinstance(contents, list):\n",
    "        contents = [contents]\n",
    "\n",
    "    print(\" Conteúdo \".center(80, \"-\"))\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_image(content)\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_responses(responses: list[GenerationResponse], as_markdown: bool = True):\n",
    "    # Consolidate the text\n",
    "    text = \"\".join(\n",
    "        part.text\n",
    "        for response in responses\n",
    "        for part in response.candidates[0].content.parts\n",
    "    )\n",
    "    # Remove potential leading/trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    print(\" Início das respostas \".center(80, \"-\"))\n",
    "    if as_markdown:\n",
    "        IPython.display.display(IPython.display.Markdown(text))\n",
    "    else:\n",
    "        print(text)\n",
    "    print(\" Fim das respostas \".center(80, \"-\"))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def display_image(image: Image, max_width: int = 600, max_height: int = 350):\n",
    "    pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        # Modes such as RGBA are not yet supported by all Jupyter environments\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "\n",
    "    image_width, image_height = pil_image.size\n",
    "    if max_width < image_width or max_height < image_height:\n",
    "        # Resize to display a smaller notebook image\n",
    "        pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "\n",
    "    display_image_compressed(pil_image)\n",
    "\n",
    "\n",
    "def display_image_compressed(pil_image: PIL_Image.Image):\n",
    "    \"\"\"Display the image in a compressed format to reduce the notebook size.\"\"\"\n",
    "    image_io = io.BytesIO()\n",
    "    pil_image.save(image_io, \"jpeg\", quality=80, optimize=True)\n",
    "    image_bytes = image_io.getvalue()\n",
    "    ipython_image = IPython.display.Image(image_bytes)\n",
    "    IPython.display.display(ipython_image)\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9R3nV5bE22Z"
   },
   "source": [
    "## Importe o modelo `Gemini 1.0 Pro`\n",
    "\n",
    "O Gemini Pro (`gemini-1.0-pro`) ajuda na realização de tarefas utilizando linguagem natural, chats multiturno de texto e código e para a geração de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702337383834,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "UnUmflwsE22a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fm71OTvpyqD"
   },
   "source": [
    "### *Reasoning* em diferentes níveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgf3t9odFlIj"
   },
   "source": [
    "Você pode realizar perguntas diretas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1702337384797,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "tBpDHLmMv2un",
    "outputId": "da0a9444-7d16-4522-ee30-76eec7ee8f2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "O que aconteceu com os dinossauros? Quando?\n",
    "Explique simplesmente em uma frase.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rfs1qLAoFrZu"
   },
   "source": [
    "… como também perguntas que requerem respostas com mais nuances:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "executionInfo": {
     "elapsed": 6374,
     "status": "ok",
     "timestamp": 1702337391168,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "AcVfT9bowUKW",
    "outputId": "64df65b5-06c7-4c2b-9434-e9b32106f422",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Temos 100% de certeza sobre o que aconteceu com os dinossauros?\n",
    "Caso contrário, detalhe as principais hipóteses atuais.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KPNK1hNGCk4"
   },
   "source": [
    "Você pode realizar interações pedindo respostas simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1702337392408,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "SPoEA7UUqlvy",
    "outputId": "5e3695e0-a571-44b5-9b5d-51f747d6ec47",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Explique por que é inverno aqui na França e verão na Austrália.\n",
    "Explique de uma forma lúdica e engraçada, para que uma criança de 4 anos consiga entender.\n",
    "Responsa incluindo os 3 motivos principais.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMlbj1-pGKkA"
   },
   "source": [
    "… ou pedindo respostas mais detalhadas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 3762,
     "status": "ok",
     "timestamp": 1702337396166,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "LdiaJBtbpXjM",
    "outputId": "578bc987-533d-4153-f0ce-528f8567718d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Explique por que temos marés.\n",
    "Forneça uma resposta detalhada e usando bullets.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV6ULv8KHkxO"
   },
   "source": [
    "Você pode realizar perguntas com um escopo bem fechado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "executionInfo": {
     "elapsed": 3798,
     "status": "ok",
     "timestamp": 1702337399959,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "dGk1uEPN0XXq",
    "outputId": "489716c2-c28b-4b0a-ab0b-c6523d8ed676",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Quando foram os dois últimos anos bissextos?\n",
    "Liste 3 competições internacionais que aconteceram durante o penúltimo.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzmiMEq_HsNF"
   },
   "source": [
    "… e também perguntas mais abertas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 6829,
     "status": "ok",
     "timestamp": 1702337406783,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "HqcYe_w5BvLT",
    "outputId": "176f5e86-d787-46cb-eb43-47ffbf74a79d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "O que veio primeiro, a galinha ou o ovo? Explique a partir de 3 perspectivas diferentes.\n",
    "O que chamamos de problema do “ovo e da galinha”? Dê 1 exemplo que pode ocorrer na educação.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoqqOTjfL6HX"
   },
   "source": [
    "### *Reasoning* com textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dEt9vXqmGN5"
   },
   "source": [
    "Você pode utilizar o Gemini para realizar sumarizações e traduções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "executionInfo": {
     "elapsed": 1899,
     "status": "ok",
     "timestamp": 1702337408678,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "jAbIQ5U3mGN5",
    "outputId": "d9183b50-3125-42b3-e6ba-dcd0c573767a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Resuma o texto a seguir em três frases, em português, usando apenas o texto.\n",
    "\n",
    "TEXTO:\n",
    "- Les hommes naissent et demeurent libres et égaux en droits. Les distinctions sociales ne peuvent être fondées que sur l'utilité commune.\n",
    "- Le but de toute association politique est la conservation des droits naturels et imprescriptibles de l'homme. Ces droits sont la liberté, la propriété, la sûreté et la résistance à l'oppression.\n",
    "- Le principe de toute souveraineté réside essentiellement dans la Nation. Nul corps, nul individu ne peut exercer d'autorité qui n'en émane expressément.\n",
    "- La liberté consiste à pouvoir faire tout ce qui ne nuit pas à autrui : ainsi, l'exercice des droits naturels de chaque homme n'a de bornes que celles qui assurent aux autres membres de la société la jouissance de ces mêmes droits. Ces bornes ne peuvent être déterminées que par la loi.\n",
    "La loi n'a le droit de défendre que les actions nuisibles à la société. Tout ce qui n'est pas défendu par la loi ne peut être empêché, et nul ne peut être contraint à faire ce qu'elle n'ordonne pas.\n",
    "- La loi est l'expression de la volonté générale. Tous les citoyens ont droit de concourir personnellement ou par leurs représentants à sa formation. Elle doit être la même pour tous, soit qu'elle protège, soit qu'elle punisse. Tous les citoyens, étant égaux à ses yeux, sont également admissibles à toutes dignités, places et emplois publics, selon leur capacité et sans autre distinction que celle de leurs vertus et de leurs talents.\n",
    "- Nul homme ne peut être accusé, arrêté ou détenu que dans les cas déterminés par la loi et selon les formes qu'elle a prescrites. Ceux qui sollicitent, expédient, exécutent ou font exécuter des ordres arbitraires doivent être punis ; mais tout citoyen appelé ou saisi en vertu de la loi doit obéir à l'instant ; il se rend coupable par la résistance.\n",
    "- La loi ne doit établir que des peines strictement et évidemment nécessaires, et nul ne peut être puni qu'en vertu d'une loi établie et promulguée antérieurement au délit, et légalement appliquée.\n",
    "Tout homme étant présumé innocent jusqu'à ce qu'il ait été déclaré coupable, s'il est jugé indispensable de l'arrêter, toute rigueur qui ne serait pas nécessaire pour s'assurer de sa personne doit être sévèrement réprimée par la loi.\n",
    "- Nul ne doit être inquiété pour ses opinions, même religieuses, pourvu que leur manifestation ne trouble pas l'ordre public établi par la loi.\n",
    "\n",
    "RESUMO:\n",
    "-\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri8Yx5vtE22b"
   },
   "source": [
    "… como também pedir que o modelo ajude com mais ideias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 4657,
     "status": "ok",
     "timestamp": 1702337413331,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "k5pXqmNJmGN5",
    "outputId": "a8e71ad6-f437-4e18-c557-b62fa38b1c9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Forneça um esboço de 5 pontos-chave para uma apresentação “chocolate no mundo”.\n",
    "Uma parte deve ser sobre sua origem no México (minha professora tem família lá).\n",
    "A última será uma degustação com todos da sala de aula.\n",
    "\"\"\"\n",
    "\n",
    "# For more creative/diverse answers, let's increase the level of randomness.\n",
    "# Successive requests will likely return different responses.\n",
    "temperature = 0.7\n",
    "top_p = 0.8\n",
    "top_k = 40\n",
    "\n",
    "responses = generate_content(model, contents, temperature, top_p, top_k)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf25rsTSkdXu"
   },
   "source": [
    "You can also ask for text corrections:\n",
    "\n",
    "Below, you can provide some examples of expected responses (i.e. few-shot prompting) so that the model can understand what kind of response you are expecting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 5001,
     "status": "ok",
     "timestamp": 1702337418328,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "jgKEDKVkL8Ub",
    "outputId": "b3c0b14f-e42d-40a1-bf71-bd764acedf4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Eu não sou um falante nativo de inglês.\n",
    "Verifique se as frases a seguir estão corretas.\n",
    "Quando incorreto, forneça uma correção e uma explicação.\n",
    "Use a mesma estrutura dos exemplos dados.\n",
    "EXEMPLOS:\n",
    "- **Hi!**\n",
    "  - Status: ✔️\n",
    "- **Your my best freind!**\n",
    "  - Status: ❌\n",
    "  - Correção: **You're my best friend!**\n",
    "  - Explicação:\n",
    "    - \"**Your**\" está incorreto. Parece que você tentou dizer \"You're\", que é a forma contraída de \"You are\".\n",
    "    - \"**freind**\" é um erro de escrita. A forma correta é \"**friend**\".\n",
    "\n",
    "SENTENÇAS:\n",
    "- They're twins, isn't it?\n",
    "- I assisted to the meeting.\n",
    "- You recieved important informations.\n",
    "- I digged a hole in the ice and saw lots of fishes.\n",
    "- That's all folks!\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ach9JtNikxVz"
   },
   "source": [
    "… como também pedir contextos mais elaborados e em múltiplos idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 4016,
     "status": "ok",
     "timestamp": 1702337422329,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "P0xdsfQDE22b",
    "outputId": "0b74d75b-fae7-4ac2-dc3c-a03f49802620",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Traduza o texto abaixo para os idiomas listados:\n",
    "\n",
    "TEXTO:\n",
    "Hello folks! I hope you're all doing well. Let's get this workshop started!\n",
    "We'll stick to English because, actually, I can't speak all those languages.\n",
    "\n",
    "IDIOMAS:\n",
    "Alemão, Francês, Grego, Búlgaro e Japonês\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 4647,
     "status": "ok",
     "timestamp": 1702337426964,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "LSNoxDSeZWw-",
    "outputId": "f8bf5a60-60c7-4599-8022-ff2a42f0fc06",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Não sou falante nativo de inglês e cometi erros nas frases a seguir.\n",
    "Adivinhe minha língua nativa.\n",
    "Explique por que esses são erros típicos.\n",
    "Se houver várias possibilidades, aqui vai uma dica: gosto de queijo.\n",
    "\n",
    "SENTENÇAS:\n",
    "- They are twin sisters, isn't it?\n",
    "- I assisted to the meeting.\n",
    "- I saw lots of fishes.\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIl7R_jBUsaC"
   },
   "source": [
    "### *Reasoning* com números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm61coMZJX-o"
   },
   "source": [
    "> Nota: Como qualquer LLM, Gemini gera resultados que parecem plausíveis, mas ainda pode ter alucinações. Dependendo das entradas e dos parâmetros, as saídas podem ser imprecisas, incluindo operações matemáticas. Como prática recomendada, você pode considerar fornecer ao LLM instruções passo a passo para reduzir as alucinações ou usar uma biblioteca de calculadoras em vez de um LLM.\n",
    "\n",
    "Você pode perguntar sobre problemas da vida real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 3792,
     "status": "ok",
     "timestamp": 1702337430752,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "nmvIJfDUmGN6",
    "outputId": "44b38862-e1b1-4d5b-a8f1-af360ef66bc9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Patricia é uma excelente corredora e corre em média 12 km/h.\n",
    "- Na segunda-feira ela correu por 1,5 hora. Que distância ela correu?\n",
    "- Na terça ela correu 21 km. Quanto tempo ela correu?\n",
    "- Na quarta-feira ela correu 150 minutos. Que distância ela correu?\n",
    "- Em seguida, ela pretende fazer uma maratona (42 km). Quanto deve demorar?\n",
    "- Para completar uma maratona em 3 horas, quanto mais rápido ela precisa correr?\n",
    "\n",
    "Detalhe as respostas passo a passo.\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nZwoSe0mGN6"
   },
   "source": [
    "… e também sobre problemas mais clássicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1702337432266,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "R6dmn9_Q0Be0",
    "outputId": "c574f8a2-6ebd-4e0d-cf5c-af456f93a632",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Acabei de pegar emprestado 1.000 reais de um amigo.\n",
    "Acordamos uma taxa de juro simples de 4,5% ao ano.\n",
    "Quero saber quanto terei que reembolsar em 1, 2 ou 3 anos.\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBv4oOPpU50E"
   },
   "source": [
    "## Importe o modelo Gemini 1.0 Pro Vision\n",
    "\n",
    "Gemini Pro Vision (`gemini-1.0-pro-vision`) é um modelo multimodal que suporta prompts multimodais. Você pode incluir texto, imagem(s) e vídeo em suas solicitações de prompt e obter respostas em texto ou código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1702337432266,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "ACD_LaIAE22c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvdNYQKJE22c"
   },
   "source": [
    "### *Reasoning* com uma única imagem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode perguntar coisas simples sobre a imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 6317,
     "status": "ok",
     "timestamp": 1702337438579,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "aFvAEetIcSA7",
    "outputId": "d4efe8d5-ed41-44e0-eb81-9c26d284738d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"O que aparece nesta imagem?\"\n",
    "# Image by Crissy Jarvis on Unsplash: https://unsplash.com/photos/cHhbULJbPwM\n",
    "image_abacus = load_image_from_url(\n",
    "    \"https://unsplash.com/photos/cHhbULJbPwM/download?w=600\"\n",
    ")\n",
    "\n",
    "contents = [prompt, image_abacus]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjqqXuauE22c"
   },
   "source": [
    "… ou perguntas específicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "executionInfo": {
     "elapsed": 8531,
     "status": "ok",
     "timestamp": 1702337447098,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "H0SOqKjyi1tH",
    "outputId": "6dfb3763-facf-4ffb-c3e0-5e2599efecb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas sobre esta imagem.\n",
    "Retorne os resultados em formato JSON contendo pares de \"perguntas\" e \"respostas\".\n",
    "\n",
    "QUESTÕES:\n",
    "- O que a imagem mostra?\n",
    "- Como funciona?\n",
    "- Quando foi inventado?\n",
    "- Qual é o nome deste objeto em francês, italiano, espanhol, holandês e alemão?\n",
    "- Quais são as cores mais proeminentes na imagem?\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt, image_abacus]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9SIjHnbmGN8"
   },
   "source": [
    "Você também pode incluir perguntas de *follow-up* na requisição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "executionInfo": {
     "elapsed": 6669,
     "status": "ok",
     "timestamp": 1702337453762,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "bGIsUZFAmGN8",
    "outputId": "69645032-83ea-4748-ff61-10577bc7dc87",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image by Brett Jordan on Unsplash: https://unsplash.com/photos/E1por_SGvJE\n",
    "image_tiles = load_image_from_url(\n",
    "    \"https://unsplash.com/photos/E1por_SGvJE/download?w=600\"\n",
    ")\n",
    "prompt = \"\"\"\n",
    "- Que expressão pode ser lida nesta imagem? Como é apresentada?\n",
    "- Qual é a expressão oposta?\n",
    "- Qual é a recomendação, a partir desta expressão, que um professor poderia dar aos seus alunos para um exame?\n",
    "- Com a expressão oposta?\n",
    "\"\"\"\n",
    "\n",
    "contents = [image_tiles, prompt]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trT6xm249rqo"
   },
   "source": [
    "A informação pode ter diferentes formas. Pode ser objetos, texto impresso, texto manuscrito, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7922,
     "status": "ok",
     "timestamp": 1702337461673,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "VKsArAoJmGN8",
    "outputId": "2209af8b-8d05-4ba4-d55f-49c16e7d9092",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Siga as instruções.\n",
    "Escreva expressões matemáticas em LaTex.\n",
    "Use uma tabela com uma linha para cada instrução e seu resultado.\n",
    "\n",
    "INSTRUÇÕES:\n",
    "- Extraia a fórmula.\n",
    "- Qual é o símbolo logo antes do Pi? O que isso significa?\n",
    "- Esta é uma fórmula famosa? Isso tem um nome?\n",
    "- Por que é especial?\n",
    "- Extraia a legenda.\n",
    "- Qual é o objeto no fundo?\n",
    "- Para que foi usado?\n",
    "- Quais são as cores da legenda e da fórmula?\n",
    "\"\"\"\n",
    "\n",
    "image_euler = load_image_from_url(\n",
    "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/math_beauty.jpg\"\n",
    ")\n",
    "\n",
    "contents = [prompt, image_euler]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHHKFGbMU-Re"
   },
   "source": [
    "Você também pode solicitar interpretações e sugestões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "executionInfo": {
     "elapsed": 9201,
     "status": "ok",
     "timestamp": 1702337470869,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "cJ0TsH-7unIE",
    "outputId": "c7a22ff9-9bd5-4598-a52f-0a526646fdbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas sobre a imagem.\n",
    "Apresente os resultados em uma tabela com uma linha para cada questão e sua resposta.\n",
    "\n",
    "QUESTÕES:\n",
    "- O que é visível?\n",
    "- Quais são as razões pelas quais é engraçado?\n",
    "- O que poderia ser uma legenda divertida?\n",
    "- O que poderia acontecer a seguir?\n",
    "- Como você alteraria a imagem? Ainda seria engraçado e por quê?\n",
    "- Como você tornaria isso mais engraçado?\n",
    "\"\"\"\n",
    "\n",
    "image_classroom = load_image_from_url(\n",
    "    \"https://unsplash.com/photos/4ApmfdVo32Q/download?w=600\"\n",
    ")\n",
    "\n",
    "contents = [prompt, image_classroom]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU5Tm-FQE22c"
   },
   "source": [
    "### *Reasoning* com múltiplas imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vbf1HALvE22d"
   },
   "source": [
    "Você também pode utilizar múltiplas imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7944,
     "status": "ok",
     "timestamp": 1702337478799,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "sLUesezCE22d",
    "outputId": "e85991c8-b5a1-4c0f-f5c3-8b46bd64d765",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Responda às seguintes perguntas para cada imagem.\n",
    "Apresente os resultados em uma tabela com uma linha para cada imagem e uma coluna para cada questão.\n",
    "\n",
    "QUESTÕES:\n",
    "- O que podemos ver na imagem?\n",
    "- Onde se passa?\n",
    "\"\"\"\n",
    "\n",
    "caption_b1 = \"Imagem 1:\"\n",
    "caption_b2 = \"Imagem 2:\"\n",
    "caption_b3 = \"Imagem 3:\"\n",
    "image_b1 = load_image_from_url(\"https://unsplash.com/photos/zzjLGF_6dx4/download?w=600\")\n",
    "image_b2 = load_image_from_url(\"https://unsplash.com/photos/ndAHi2Wxcok/download?w=600\")\n",
    "image_b3 = load_image_from_url(\"https://unsplash.com/photos/5mZ_M06Fc9g/download?w=600\")\n",
    "\n",
    "contents = [prompt, caption_b1, image_b1, caption_b2, image_b2, caption_b3, image_b3]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrMHz6A3E22d"
   },
   "source": [
    "… ou realizar comparações entre imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8002,
     "status": "ok",
     "timestamp": 1702337486791,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "u7w31hUtE22d",
    "outputId": "f14cd388-e7c3-4d82-d006-b8b94961fe10",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas sobre as imagens, com uma resposta curta e um motivo detalhado para a resposta.\n",
    "Apresente os resultados em uma tabela com uma linha para cada pergunta, resposta e motivo.\n",
    "\n",
    "QUESTÕES:\n",
    "- O que as imagens têm em comum?\n",
    "- Qual deles interessaria a um matemático?\n",
    "- Qual deles indica que é o fim das férias?\n",
    "- Qual deles sugere que possamos tomar um café lá?\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt, caption_b1, image_b1, caption_b2, image_b2, caption_b3, image_b3]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqFSjI4_mGN9"
   },
   "source": [
    "Você pode usar o nível de linguagem e compreensão visual do Gemini para trabalhar conceitos ou até mesmo obter sugestões de novas imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6093,
     "status": "ok",
     "timestamp": 1702337492875,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "QjbP2y26mGN9",
    "outputId": "7a072645-e7ff-4049-93a7-f27bb6e27c7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas sobre as imagens, com uma resposta curta e um motivo detalhado para a resposta.\n",
    "Apresente os resultados em uma tabela com uma linha para cada pergunta, resposta e motivo.\n",
    "\n",
    "QUESTÕES:\n",
    "- O que representa a primeira imagem?\n",
    "- O que a segunda imagem representa?\n",
    "- Qual poderia ser a próxima imagem lógica?\n",
    "\"\"\"\n",
    "\n",
    "caption_w1 = \"Imagem 1:\"\n",
    "caption_w2 = \"Imagem 2:\"\n",
    "image_w1 = load_image_from_url(\"https://unsplash.com/photos/TA5bUTySOrg/download?w=600\")\n",
    "image_w2 = load_image_from_url(\"https://unsplash.com/photos/Nw_D8v79PM4/download?w=600\")\n",
    "\n",
    "contents = [prompt, caption_w1, image_w1, caption_w2, image_w2]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJdBiddamGN9"
   },
   "source": [
    "E o limite é a sua imaginação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9644,
     "status": "ok",
     "timestamp": 1702337502502,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "iFhubVwFmGN9",
    "outputId": "4f2f0ada-b96b-41c2-d2ba-9f0d1e3042c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas, com uma resposta curta e um motivo detalhado para a resposta.\n",
    "Apresente os resultados em uma tabela com uma linha para cada pergunta, resposta e motivo.\n",
    "\n",
    "QUESTÕES:\n",
    "- Que tema estas imagens ilustram?\n",
    "- Qual poderia ser outra imagem para substituir a primeira?\n",
    "- Que outra imagem poderia substituir a segunda?\n",
    "- Qual seria uma alternativa à terceira imagem?\n",
    "- E para o último?\n",
    "\"\"\"\n",
    "\n",
    "caption_s1 = \"Imagem 1:\"\n",
    "caption_s2 = \"Imagem 2:\"\n",
    "caption_s3 = \"Imagem 3:\"\n",
    "caption_s4 = \"Imagem 4:\"\n",
    "image_s1 = load_image_from_url(\"https://unsplash.com/photos/eriuKJwcdjI/download?w=600\")\n",
    "image_s2 = load_image_from_url(\"https://unsplash.com/photos/QldMpmrmWuc/download?w=600\")\n",
    "image_s3 = load_image_from_url(\"https://unsplash.com/photos/rN3m7aTH3io/download?w=600\")\n",
    "image_s4 = load_image_from_url(\"https://unsplash.com/photos/FhdN5QVrBfY/download?w=600\")\n",
    "\n",
    "contents = [\n",
    "    prompt,\n",
    "    caption_s1,\n",
    "    image_s1,\n",
    "    caption_s2,\n",
    "    image_s2,\n",
    "    caption_s3,\n",
    "    image_s3,\n",
    "    caption_s4,\n",
    "    image_s4,\n",
    "]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkjzQsgKGS7o"
   },
   "source": [
    "### *Reasoning* com vídeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 24732,
     "status": "ok",
     "timestamp": 1702337527230,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "o9dOaKJoUknq",
    "outputId": "a870895c-ca0a-4003-e641-2f3485678e30",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas usando apenas o vídeo.\n",
    "Apresente os resultados em uma tabela com uma linha para cada questão e sua resposta.\n",
    "\n",
    "QUESTÕES:\n",
    "- Qual é o principal animal visível ao longo do vídeo?\n",
    "- Quais dispositivos eletrônicos estão visíveis?\n",
    "- Que animais são os personagens de desenhos animados tirando uma selfie em close?\n",
    "- Quais marcas famosas são visíveis?\n",
    "- Qual é o texto visível no final?\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://cloud-samples-data/video/animals.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "contents = [prompt, video]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWT14Lw6Uknq"
   },
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste tutorial, você viu exemplos de como usar o Gemini para educação e se beneficiar de modelos textuais e multimodais para gerar conteúdo a partir de textos, imagens e vídeos.\n",
    "\n",
    "Você também pode explorar outros tutoriais que enfocam diferentes domínios ou especificidades da API Vertex AI Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
