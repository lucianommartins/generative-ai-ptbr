{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Chamando funções (*functions calling*) com a Vertex AI Gemini API\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Executar no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> Ver no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Abrir no Workbench da Vertex AI\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHPv2myT2cx"
   },
   "source": [
    "## Visão geral\n",
    "\n",
    "### Gêmeos\n",
    "\n",
    "Gemini é uma família de modelos generativos de IA desenvolvidos pelo Google DeepMind e projetados para casos de uso multimodais.\n",
    "\n",
    "### Chamando funções do Gemini\n",
    "\n",
    "[Chamada de função](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) permite que os desenvolvedores criem uma descrição de uma função no código e depois passem essa descrição para uma linguagem modelo em uma solicitação. A resposta do modelo inclui o nome de uma função que corresponde à descrição e os argumentos para chamá-la.\n",
    "\n",
    "A chamada de função é semelhante às [Extensões da Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/extensions/overview), pois ambas geram informações sobre funções. A diferença entre eles é que a chamada de função retorna dados JSON com o nome de uma função e os argumentos a serem usados em seu código, enquanto as extensões Vertex AI retornam a função e a chamam para você."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrkcqHrrwMAo"
   },
   "source": [
    "### Objetivos\n",
    "\n",
    "Neste tutorial, você aprenderá como usar a API Vertex AI Gemini com o SDK Vertex AI para Python para fazer chamadas de função por meio do modelo Gemini Pro (`gemini-pro`).\n",
    "\n",
    "Você executará as seguintes tarefas:\n",
    "\n",
    "- Instalar o SDK da Vertex AI para Python\n",
    "- Use a API Vertex AI Gemini para interagir com o modelo Gemini Pro (`gemini-pro`):\n",
    "     - Gere chamadas de função a partir de um prompt de texto para obter a previsão do tempo para um determinado local\n",
    "     - Gere chamadas de função a partir de um prompt de texto e chame uma API externa para geocodificar endereços\n",
    "     - Gere chamadas de função a partir de um prompt de chat para ajudar usuários de varejo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9nEPojogw-g"
   },
   "source": [
    "### Custos\n",
    "\n",
    "Este tutorial usa os seguintes componentes de Google Cloud que podem gerar custos em sua fatura:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Saiba mais sobre [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing) e use a [calculadora de preços](https://cloud.google.com/products/calculator/) para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11Gu7qNgx1p"
   },
   "source": [
    "## Primeiros passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Instale a SDK da Vertex AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7UyNVSiyQ96"
   },
   "source": [
    "### **Somente para uso no Colab - Reinicie o kernel do notebook** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para realizar o restart do kernel do notebook (etapa importante para que o Colab reconheça a nova versão da SDK) e a execute. Senão, siga para as próximas instruções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### **Somente para uso no Colab - Autentique o seu ambiente de notebook** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para realizar a autenticação da sua sessão de notebook com a Google Cloud Esse passo é importante **para utilização no Colab** para garantir que as chamadas a APIs de Google Cloud funcionem sem problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Additional authentication is required for Google Colab\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Authenticate user to Google Cloud\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### **Somente para uso no Colab - defina o projeto Google Cloud a ser utilizado** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para definir qual projeto Google Cloud será utilizado pelo Colab na execução deste notebook. Senão, siga para as próximas instruções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Define project information\n",
    "#     PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "#     LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "#     # Initialize Vertex AI\n",
    "#     import vertexai\n",
    "\n",
    "#     vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lslYAvw37JGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import requests\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por que chamar funções?\n",
    "\n",
    "Ao trabalhar com um modelo de texto generativo, pode ser difícil forçar o LLM a fornecer respostas consistentes em um formato estruturado como JSON. A chamada de função facilita o trabalho com LLMs por meio de prompts e entradas não estruturadas, e faz com que o LLM retorne uma resposta estruturada que pode ser usada para chamar uma função externa.\n",
    "\n",
    "Você pode pensar na chamada de função como uma forma de obter saída estruturada de prompts do usuário e definições de função, usar essa saída estruturada para fazer uma solicitação de API a um sistema externo e, em seguida, retornar a resposta da função ao LLM para gerar uma resposta ao usuário. Em outras palavras, a chamada de função no Gemini extrai parâmetros estruturados de textos não estruturados ou mensagens de usuários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "## Importe o modelo `Gemini Pro`\n",
    "\n",
    "O Gemini Pro (`gemini-pro`) ajuda na realização de tarefas utilizando linguagem natural, chats multiturno de texto e código e para a geração de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um exemplo simples de chamadas de função"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar, você usará a chamada de função para configurar uma solicitação de API meteorológica para que os usuários obtenham as condições atuais em um determinado local. Os parâmetros de função são especificados como um dicionário Python de acordo com o [formato de esquema JSON OpenAPI](https://spec.openapis.org/oas/v3.0.3#schemawr).\n",
    "\n",
    "Considere um exemplo de função da API meteorológica que recebe um argumento para a localização do usuário, como em:\n",
    "\n",
    "```python\n",
    "def get_current_weather(location):\n",
    "     ...\n",
    "```\n",
    "\n",
    "Você começará especificando uma declaração de função e os parâmetros necessários para fazer uma solicitação em nosso exemplo de API meteorológica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_current_weather_func = FunctionDeclaration(\n",
    "    name=\"get_current_weather\",\n",
    "    description=\"Get the current weather in a given location\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Location\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode então definir uma ferramenta para o LLM chamar que inclua `get_current_weather_func`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_tool = Tool(\n",
    "    function_declarations=[get_current_weather_func],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode então instruir o modelo para gerar conteúdo, incluir a `tool` que acabou de criar, para gerar uma resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Como está o tempo em Belém?\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\"temperature\": 0},\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the function call portion of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resposta inclui uma assinatura de função que pode ser usada para chamar a API meteorológica. Neste ponto, você tem tudo o que precisa para formar um corpo de solicitação e fazer uma chamada de API para um sistema externo. Bom trabalho!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um exemplo mais complexo de chamadas de funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, você gerará uma chamada de função que possui uma estrutura mais complexa. Você usará a resposta da função para fazer uma chamada de API que converte um endereço em coordenadas de latitude e longitude.\n",
    "\n",
    "Comece definindo uma declaração de função dentro de uma ferramenta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_location = FunctionDeclaration(\n",
    "    name=\"get_location\",\n",
    "    description=\"Busque a latitude e a longitude para uma dada localização\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"poi\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Point of interest\"\n",
    "        },\n",
    "        \"street\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Street name\"\n",
    "        },\n",
    "        \"city\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"City name\"\n",
    "        },\n",
    "        \"county\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"County name\"\n",
    "        },\n",
    "        \"state\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"State name\"\n",
    "        },\n",
    "        \"country\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Country name\"\n",
    "        },\n",
    "        \"postal_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Postal code\"\n",
    "        },\n",
    "    },\n",
    "},\n",
    ")\n",
    "\n",
    "location_tool = Tool(\n",
    "    function_declarations=[get_location],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora você pode chamar o modelo para gerar uma resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Quero obter as coordenadas lat/lon do seguinte endereço:\n",
    "Avenida Brigadeiro Faria Lima, 3477, São Paulo, SP, BR\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\"temperature\": 0},\n",
    "    tools=[location_tool],\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora você pode extrair os resultados da resposta da função e fazer uma chamada de API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = response.candidates[0].content.parts[0].function_call.args\n",
    "\n",
    "url = \"https://nominatim.openstreetmap.org/search?\"\n",
    "for i in x:\n",
    "    url += '{}=\"{}\"&'.format(i, x[i])\n",
    "url += \"format=json\"\n",
    "\n",
    "x = requests.get(url)\n",
    "content = x.json()\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você foi capaz de construir uma função e uma ferramenta que o LLM usou para gerar os parâmetros necessários para uma chamada de função e, em seguida, fez a chamada de função para obter as coordenadas do local especificado.\n",
    "\n",
    "Aqui usamos a [API OpenStreetMap Nominatim](https://nominatim.openstreetmap.org/ui/search.html) para geocodificar um endereço para facilitar o uso e o aprendizado neste notebook. Se você estiver trabalhando com grandes quantidades de mapas ou dados de geolocalização, poderá usar a [API de geocodificação do Google Maps](https://developers.google.com/maps/documentation/geocoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamadas de função em interações de chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, você usará o modelo de chat no Gemini para ajudar os clientes a obter informações sobre os produtos de uma loja.\n",
    "\n",
    "Você começará definindo várias funções em uma ferramenta para obter informações sobre produtos, obter a localização das lojas e fazer um pedido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_product_info_func = FunctionDeclaration(\n",
    "    name=\"get_product_sku\",\n",
    "    description=\"Busque a SKU de um produto\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"product_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Nome do produto\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")\n",
    "\n",
    "get_store_location_func = FunctionDeclaration(\n",
    "    name=\"get_store_location\",\n",
    "    description=\"Busque a localização da loja mais próxima\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Localização\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")\n",
    "\n",
    "place_order_func = FunctionDeclaration(\n",
    "    name=\"place_order\",\n",
    "    description=\"Faça um pedido de compra\",\n",
    "    parameters={\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"product\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Nome do produto\"\n",
    "        },\n",
    "        \"account\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Informações da conta\"\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Endereço de entrega\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    ")\n",
    "\n",
    "retail_tool = Tool(\n",
    "    function_declarations=[get_product_info_func, \n",
    "                           get_store_location_func, \n",
    "                           place_order_func,\n",
    "                          ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que você também pode usar chamadas de função em uma sessão de chat multiturno e pode especificar ferramentas ao criar um modelo para evitar ter que enviá-las em cada solicitação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\", \n",
    "                        generation_config={\"temperature\": 0},\n",
    "                        tools=[retail_tool])\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Você tem o Pixel 8 Pro em estoque?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperado, a resposta inclui uma chamada de função estruturada que podemos usar para nos comunicarmos com sistemas externos.\n",
    "\n",
    "Na realidade, você executaria chamadas de função em um sistema ou banco de dados externo. Como este notebook se concentra na capacidade de extrair parâmetros de função e gerar chamadas de função, você usará dados simulados para alimentar as respostas ao modelo, em vez de usar um servidor de API real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# É aqui que você faria uma solicitação de API para retornar o status do pedido.\n",
    "# Use dados sintéticos para simular uma resposta de uma API externa.\n",
    "\n",
    "api_response = {\"sku\": \"GA04834-US\", \"em_stock\": \"sim\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos incluir detalhes da chamada de API externa e gerar uma resposta ao usuário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_product_sku\",\n",
    "        response={\n",
    "            \"content\": api_response,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, o usuário pode perguntar onde pode comprar um telefone em uma loja próxima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Onde posso comprá-lo perto de Mountain View, CA?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtemos uma resposta com outra chamada de função estruturada, desta vez configurada para usar uma função diferente da nossa ferramenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# É aqui que você faria uma solicitação de API para obter a localização da loja mais próxima do usuário.\n",
    "# Use dados sintéticos para simular uma resposta de uma API externa.\n",
    "\n",
    "api_response = {\"store\": \"1600 Amphitheatre Pkwy, Mountain View, CA 94043, US\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's include details from the external API call and generate a response to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_store_location\",\n",
    "        response={\n",
    "            \"content\":  api_response,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, o usuário pode solicitar o pedido de um telefone e enviá-lo para um endereço:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Gostaria de encomendar um Pixel 8 Pro e enviá-lo para 1155 Borregas Ave, Sunnyvale, CA 94089.\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraímos o produto desejado e o endereço do usuário, buscamos o número da conta e agora podemos chamar uma API para fazer o pedido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# É aqui que você faria uma solicitação de API para retornar o status do pedido.\n",
    "# Use dados sintéticos para simular uma carga útil de resposta de uma API externa.\n",
    "\n",
    "api_response = {\"status_pagamento\": \"pago\", \"numero_pedido\": 12345, \"prazo_entrega\": \"2 dias\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, vamos incluir detalhes da chamada de API externa e gerar uma resposta ao usuário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"place_order\",\n",
    "        response={\n",
    "            \"content\":  api_response,\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você teve uma conversa de várias voltas com o Gemini, gerou chamadas de função, tratou da passagem (simulada) de dados de volta para o modelo e gerou mensagens que fizeram uso das respostas da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
