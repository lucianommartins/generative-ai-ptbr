{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9",
    "tags": []
   },
   "source": [
    "# Gemini: Um Overview de cenários multimodais\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Executar no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> Ver no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Abrir no Workbench da Vertex AI\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Visão Geral\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini é uma família de modelos generativos de IA desenvolvidos pelo Google DeepMind e projetados para casos de uso multimodais. A API Gemini dá acesso aos modelos Gemini Pro Vision e Gemini Pro.\n",
    "\n",
    "### API Vertex AI Gemini\n",
    "\n",
    "A API Vertex AI Gemini fornece uma interface unificada para interagir com modelos Gemini. Atualmente existem dois modelos disponíveis na API Gemini:\n",
    "\n",
    "- **Modelo Gemini Pro** (`gemini-pro`): Projetado para lidar com tarefas de linguagem natural, bate-papo multivoltas de texto e código e geração de código.\n",
    "- **Modelo Gemini Pro Vision** (`gemini-pro-vision`): Suporta prompts multimodais. Você pode incluir texto, imagens e vídeo em suas solicitações de prompt e obter respostas em texto ou código.\n",
    "\n",
    "Você pode interagir com a API Gemini usando os seguintes métodos:\n",
    "\n",
    "- Use o [Vertex AI Studio](https://cloud.google.com/generative-ai-studio) para testes rápidos e geração de comandos\n",
    "- Use o SDK da Vertex AI\n",
    "\n",
    "Este notebook se concentra no uso do **Vertex AI SDK para Python** para chamar a API Vertex AI Gemini.\n",
    "\n",
    "Para obter mais informações, consulte a documentação [IA Generativa na Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objetivos\n",
    "\n",
    "Este notebook demonstra uma variedade de casos de uso multimodais para os quais o Gemini pode ser usado.\n",
    "\n",
    "#### Casos de uso multimodais\n",
    "\n",
    "Em comparação com LLMs somente texto, a multimodalidade do Gemini Pro Vision pode ser usada para muitos novos casos de uso:\n",
    "\n",
    "Exemplos de casos de uso com **texto e imagem(s)** como entrada:\n",
    "\n",
    "- Detectando objetos em fotos\n",
    "- Compreender telas e interfaces\n",
    "- Compreensão de desenho e abstração\n",
    "- Compreender gráficos e diagramas\n",
    "- Recomendação de imagens com base nas preferências do usuário\n",
    "- Comparar imagens em busca de semelhanças, anomalias ou diferenças\n",
    "\n",
    "Exemplos de casos de uso com **texto e vídeo** como entrada:\n",
    "\n",
    "- Gerando uma descrição de vídeo\n",
    "- Extração de tags de objetos ao longo de um vídeo\n",
    "- Extração de destaques/mensagens de um vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhsUe0fyc-ER"
   },
   "source": [
    "### Custos\n",
    "\n",
    "Este tutorial usa os seguintes componentes de Google Cloud que podem gerar custos em sua fatura:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Saiba mais sobre [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing) e use a [calculadora de preços](https://cloud.google.com/products/calculator/) para gerar uma estimativa de custo com base no uso projetado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeiros passos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Instale a SDK da Vertex AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7UyNVSiyQ96"
   },
   "source": [
    "### **Somente para uso no Colab - Reinicie o kernel do notebook** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para realizar o restart do kernel do notebook (etapa importante para que o Colab reconheça a nova versão da SDK) e a execute. Senão, siga para as próximas instruções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### **Somente para uso no Colab - Autentique o seu ambiente de notebook** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para realizar a autenticação da sua sessão de notebook com a Google Cloud Esse passo é importante **para utilização no Colab** para garantir que as chamadas a APIs de Google Cloud funcionem sem problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Additional authentication is required for Google Colab\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Authenticate user to Google Cloud\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGB8Txa_e4V0"
   },
   "source": [
    "### **Somente para uso no Colab - defina o projeto Google Cloud a ser utilizado** \n",
    "\n",
    "Caso você esteja executando este notebook no Google Colab, descomente a célula abaixo para definir qual projeto Google Cloud será utilizado pelo Colab na execução deste notebook. Senão, siga para as próximas instruções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGOJHtgDe5-r"
   },
   "outputs": [],
   "source": [
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Define project information\n",
    "#     PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "#     LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "#     # Initialize Vertex AI\n",
    "#     import vertexai\n",
    "\n",
    "#     vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Importe as bibliotecas necessárias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTk488WDPBtQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from vertexai.preview.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7rZuTClfNs0"
   },
   "source": [
    "## Importe o modelo `Gemini Pro Vision`\n",
    "\n",
    "Gemini Pro Vision (`gemini-pro-vision`) é um modelo multimodal que suporta prompts multimodais. Você pode incluir texto, imagem(s) e vídeo em suas solicitações de prompt e obter respostas em texto ou código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpL3OkSCfIAR"
   },
   "source": [
    "### Defina algumas funções auxiliares\n",
    "\n",
    "Defina funções auxiliares para carregar e exibir imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7QMAHXse339",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_images([content])\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OWurhO4mu4J"
   },
   "source": [
    "## Compreensão de contexto em múltiplas imagens\n",
    "\n",
    "Uma das capacidades do Gemini é ser capaz de raciocinar através de múltiplas imagens.\n",
    "\n",
    "Este é um exemplo de uso do Gemini para calcular o custo total de mantimentos usando uma imagem de frutas e uma lista de preços:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "UyRoVquPmy9H",
    "outputId": "79fc24c3-6e92-4ae0-cf2e-53be0fb5577d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_grocery_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/banana-apple.jpg\"\n",
    "image_prices_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pricelist.jpg\"\n",
    "image_grocery = load_image_from_url(image_grocery_url)\n",
    "image_prices = load_image_from_url(image_prices_url)\n",
    "\n",
    "instructions = \"Instruções: Considere a imagem a seguir que contem frutas:\"\n",
    "prompt1 = \"Quanto eu pagarei portas frutas considerando esta tabela de preços?\"\n",
    "prompt2 = \"\"\"\n",
    "Responda à pergunta através destas etapas:\n",
    "Passo 1: Identifique que tipo de fruta existe na primeira imagem.\n",
    "Passo 2: Conte a quantidade de cada fruta.\n",
    "Passo 3: Para cada item da primeira imagem, verifique o preço do item na tabela de preços.\n",
    "Passo 4: Calcule o preço subtotal de cada tipo de fruta.\n",
    "Passo 5: Calcule o preço total das frutas usando os subtotais.\n",
    "\n",
    "Responda e descreva as etapas realizadas:\"\"\"\n",
    "\n",
    "contents = [\n",
    "    instructions,\n",
    "    image_grocery,\n",
    "    prompt1,\n",
    "    image_prices,\n",
    "    prompt2,\n",
    "]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy-me3PdgMUH",
    "tags": []
   },
   "source": [
    "## Compreensão de telas e interfaces\n",
    "\n",
    "O Gemini também pode extrair informações de telas de dispositivos, interfaces de usuário, capturas de tela, ícones e layouts.\n",
    "\n",
    "Por exemplo, se você inserir a imagem de um fogão, poderá pedir ao Gemini que forneça instruções para ajudar um usuário a navegar na interface e responder em diferentes idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDjN4thV8orx",
    "outputId": "9191a4ef-4dcc-4060-999d-c5425a986e1b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_stove_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\"\n",
    "image_stove = load_image_from_url(image_stove_url)\n",
    "\n",
    "prompt = \"\"\"Como posso acertar o relógio deste aparelho?\n",
    "Se as instruções incluírem botões, explique também onde esses botões estão fisicamente localizados.\n",
    "Forneça três versões de intruções: em português, em inglês e francês.\n",
    "\"\"\"\n",
    "\n",
    "contents = [image_stove, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbhfMO478orx"
   },
   "source": [
    "**Nota:** A resposta pode não ser totalmente precisa, pois o modelo pode ter alucinações; entretanto, o modelo é capaz de identificar a localização dos botões e traduzir em uma única consulta. Para mitigar as alucinações, uma abordagem é fundamentar o LLM com técnicas como RAG, o que está fora do escopo deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "## Noções básicas sobre relacionamentos de entidades em diagramas técnicos\n",
    "\n",
    "O Gemini possui recursos multimodais que permitem compreender diagramas e executar etapas práticas, como otimização ou geração de código. Este exemplo demonstra como o Gemini pode decifrar um diagrama de relacionamento de entidade (ER), entender os relacionamentos entre tabelas, identificar requisitos para otimização em um ambiente específico como o BigQuery e até mesmo gerar o código correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klY4yBEiKmET",
    "outputId": "82320aea-57c4-4558-e267-9b76dcb13273"
   },
   "outputs": [],
   "source": [
    "image_er_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/er.png\"\n",
    "image_er = load_image_from_url(image_er_url)\n",
    "\n",
    "prompt = \"Documente as entidades e relacionamentos neste diagrama entidade relacionamento (DER).\"\n",
    "\n",
    "contents = [prompt, image_er]\n",
    "\n",
    "# Use a more deterministic configuration with a low temperature\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXrvQxpiQKp6"
   },
   "source": [
    "## Recomendações baseadas em múltiplas imagens\n",
    "\n",
    "Gemini é capaz de comparar imagens e fornecer recomendações. Isso pode ser útil em setores como ecommece e varejo.\n",
    "\n",
    "Abaixo está um exemplo de escolha de qual par de óculos seria mais adequado para um rosto oval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7M7B9q7L1X7",
    "outputId": "a33ed40b-a2a1-4dcc-908f-5b70cd60af0b"
   },
   "outputs": [],
   "source": [
    "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
    "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
    "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
    "image_glasses2 = load_image_from_url(image_glasses2_url)\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Qual desses óculos você me recomenda com base no formato do meu rosto?\n",
    "Eu tenho um rosto oval.\n",
    "----\n",
    "Óculos 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "----\n",
    "Óculos 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "----\n",
    "Explique como você toma essa decisão.\n",
    "Forneça sua recomendação com base no formato do meu rosto e no raciocínio para cada um.\n",
    "Forneça a resposta em formato JSON.\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt1, image_glasses1, prompt2, image_glasses2, prompt3]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBrdsvIU7Zkf"
   },
   "source": [
    "## Semelhanças e Diferenças\n",
    "\n",
    "Os modelos Gemini podem comparar imagens e identificar semelhanças ou diferenças entre objetos.\n",
    "\n",
    "O exemplo a seguir mostra duas cenas da Marienplatz em Munique, Alemanha, que são ligeiramente diferentes. O Gemini pode comparar as imagens e encontrar semelhanças/diferenças:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUSJduLh8457",
    "outputId": "1743bf58-0717-44a6-cdb4-3f8f9e4c1093"
   },
   "outputs": [],
   "source": [
    "image_landmark1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark1.jpg\"\n",
    "image_landmark2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark2.jpg\"\n",
    "image_landmark1 = load_image_from_url(image_landmark1_url)\n",
    "image_landmark2 = load_image_from_url(image_landmark2_url)\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Considere as seguintes imagens:\n",
    "Imagem 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "Imagem 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "1. O que é mostrado na Imagem 1?\n",
    "2. O que há de semelhante entre as duas imagens?\n",
    "3. Qual a diferença entre a Imagem 1 e a Imagem 2 em termos de conteúdo ou pessoas mostradas?\"\"\"\n",
    "\n",
    "contents = [prompt1, image_landmark1, prompt2, image_landmark2, prompt3]\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.0,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN8nVlITK5kz"
   },
   "source": [
    "## Gerando uma descrição de vídeo\n",
    "\n",
    "O Gemini também pode extrair tags de um vídeo:\n",
    "\n",
    "> Vídeo: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tT2nArvxZv-P",
    "outputId": "81383d32-7f43-47d6-c375-d93cc5be5f4d"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "O que é mostrado neste vídeo?\n",
    "Onde devo ir para ver isso?\n",
    "Quais são 5 outros lugares do mundo com esta aparência?\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9RdLpH128Ao"
   },
   "source": [
    "> Você pode confirmar que o local é realmente Antalya, Turquia, visitando a página da Wikipedia: https://en.wikipedia.org/wiki/Antalya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksjZiIfnO0zQ"
   },
   "source": [
    "## Extraindo tags de objetos ao longo do vídeo\n",
    "\n",
    "O Gemini também pode extrair tags de um vídeo.\n",
    "\n",
    "> Vídeo: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/photography.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9qE2GGIA975",
    "outputId": "bbe9dcee-772e-4d8d-8d8c-6ee03606ea78"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas usando apenas o vídeo:\n",
    "- O que está no vídeo?\n",
    "- Qual é a ação no vídeo?\n",
    "- Fornecer as 10 melhores tags para este vídeo?\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiQzvIJfQxbQ"
   },
   "source": [
    "## Fazendo mais perguntas sobre um vídeo\n",
    "\n",
    "Abaixo está outro exemplo para uso com o Gemini neste cenário de perguntas sobre um vídeo.\n",
    "\n",
    "> Vídeo: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\n",
    "> _Observação: embora este vídeo contenha áudio, o Gemini atualmente não suporta entrada de áudio e só responderá com base no vídeo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQIV9SwCz5WM",
    "outputId": "646439d7-cae1-4788-e172-59d45afce1bf"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda às seguintes perguntas usando apenas o vídeo:\n",
    "Qual é a profissão da pessoa principal?\n",
    "Quais são as principais funcionalidades do telefone destacadas?\n",
    "Em que cidade isso foi gravado?\n",
    "Forneça a resposta JSON.\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LurOmNuRpDr"
   },
   "source": [
    "## Recuperando informações extras além do vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CY-zlixU87O",
    "outputId": "360ee28f-6f87-49df-f005-12b7ded61785"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Que linha é essa?\n",
    "onde isso vai?\n",
    "Quais são as estações/paradas?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Resposta--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZrxMm_83Vps"
   },
   "source": [
    "> Você pode confirmar que esta é realmente a Linha da Confederação na Wikipedia aqui: https://en.wikipedia.org/wiki/Confederation_Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
