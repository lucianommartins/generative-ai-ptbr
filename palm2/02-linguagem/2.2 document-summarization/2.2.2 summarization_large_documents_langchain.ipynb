{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxCkB_DXTHzf"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# Sumariza√ß√£o de textos em documentos grandes com LangChain ü¶úüîó\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Vis√£o geral\n",
    "\n",
    "O resumo de texto √© o processo de criar uma vers√£o mais curta de um documento de texto, preservando as informa√ß√µes mais importantes. Isso pode ser √∫til para v√°rias finalidades, como examinar rapidamente um documento longo, obter a ess√™ncia de um artigo ou compartilhar um resumo com outras pessoas.\n",
    "\n",
    "Embora resumir um par√°grafo curto seja uma tarefa trivial, h√° alguns desafios a serem superados se voc√™ quiser resumir um documento grande, como um arquivo PDF com v√°rias p√°ginas. \n",
    "\n",
    "Neste notebook, voc√™ utilizar√° o LangChain, o framework open source para desenvolver aplica√ß√µes com LLMs, para aplicar diferentes estrat√©gias de sumariza√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXsvgIuwTPZw"
   },
   "source": [
    "### Objetivo\n",
    "\n",
    "Neste tutorial, voc√™ aprender√° como usar o LangChain em conjunto com as APIs do PaLM2 dispon√≠veis na Vertex AI para resumir informa√ß√µes de texto trabalhando com os seguintes exemplos:\n",
    "\n",
    "- M√©todo de *stuffing*\n",
    "- M√©todo MapReduce\n",
    "- MapReduce com Refinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Saiba mais sobre poss√≠veis custos envolvidos [pre√ßos da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "e use a [Calculadora de pre√ßos](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvKl-BtQTRiQ"
   },
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwFMpIMrTV_4"
   },
   "source": [
    "### Instalando os SDK da Vertex AI e outras depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aP6JVlZkS-m",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo apt -y -qq install tesseract-ocr\n",
    "!sudo apt -y -qq install libtesseract-dev\n",
    "!sudo apt-get -y -qq install poppler-utils #required by PyPDF2 for page count and other pdf utilities\n",
    "!sudo apt-get -y -qq install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDmNq5__Trl4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "! pip3 install --user --upgrade pytesseract pypdf PyPDF2 textract langchain transformers google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWwtjLV5TY6H"
   },
   "source": [
    "**Somente Colab:** Descomente a c√©lula a seguir para reiniciar o kernel ou use o bot√£o para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opUxT_k5TdgP"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se voc√™ estiver usando o **Colab** para executar este notebook, descomente a c√©lula abaixo e continue.\n",
    "* Se voc√™ estiver usando o **Vertex AI Workbench**, confira as instru√ß√µes de configura√ß√£o [aqui](../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbNgv4q1T2Mi"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5fXfvzhTkYN"
   },
   "source": [
    "### Importando as bibliotecas necess√°rias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a c√©lula a seguir para realizar o processo adequado de inicializa√ß√£o da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjSsu6cmUdEx"
   },
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"[your-project-name]\" # @param {type:\"string\"}\n",
    "# REGION = \"us-central1\"\n",
    "\n",
    "# import vertexai\n",
    "\n",
    "# vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRkcfnQMT9vD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAGaTjPVTmhP"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITUmZiNZcMUW"
   },
   "outputs": [],
   "source": [
    "vertex_llm_text = VertexAI(model_name=\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKG-ZTJ_02wq"
   },
   "source": [
    "## Summarization with Large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JELITHdBhnf0"
   },
   "source": [
    "### Preparando arquivos de dados\n",
    "\n",
    "Para come√ßar, voc√™ precisar√° baixar um arquivo pdf para as tarefas de resumo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3INtovxreI_"
   },
   "outputs": [],
   "source": [
    "# le o arquivo pdf e cria uma lista de p√°ginas\n",
    "pdf_file = './documentos/documento.pdf'\n",
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDVwBFSjZ7ws"
   },
   "source": [
    "## M√©todo 1: Stuffing\n",
    "\n",
    "A maneira mais simples de passar dados para um modelo de linguagem √© envi√°-los no prompt como contexto. Isso significa simplesmente incluir todas as informa√ß√µes relevantes no prompt, na ordem em que voc√™ deseja que o modelo as processe.\n",
    "\n",
    "Com o framework LangChain, voc√™ usar√° o m√©todo `StuffDocumentsChain` como parte da chamada ao m√©todo `load_summarize_chain`. Voc√™ necessita somente definir `stuff` como `chain_type` do seu chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhEi-XqKnv2v"
   },
   "source": [
    "### Prompt design com um `Stuffing` chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-ljajUen1YO"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Escreva um sum√°rio conciso do texto abaixo delimitado por tr√™s aspas invertidas.\n",
    "                  Retorne sua resposta em bullets que cubram os pontos principais do texto.\n",
    "                  ```{text}```\n",
    "                  SUMARIO EM BULLETS:\n",
    "                  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aVrDWkJs3Y"
   },
   "source": [
    "Inicie a chain com `stuff` e processe tr√™s p√°ginas do documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_hoizIgObe9"
   },
   "outputs": [],
   "source": [
    "stuff_chain = load_summarize_chain(vertex_llm_text, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1_zwxwgTnlV"
   },
   "outputs": [],
   "source": [
    "three_pages = pages[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jEUfOn7UFI2"
   },
   "outputs": [],
   "source": [
    "three_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnXUwWxkrLu4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_chain.run(three_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKb_fBEedZqu"
   },
   "source": [
    "Como pode ser notado, com o m√©todo `stuff` voc√™ sumariza em uma √∫nica chamada todas as informa√ß√µes presentes no seu prompt.\n",
    "\n",
    "Por√©m, dependendo do tamanho do seu documento, o m√©todo `stuff` pode n√£o funcionar por exceder o limite de input do seu modelo LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtgemmBzkddX",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_chain.run(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtp21WX3T7d_"
   },
   "source": [
    "Como esperado, o c√≥digo **retorna um erro** devido ao tamanho do contexto do prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqZrKM32h-o2"
   },
   "source": [
    "### Considera√ß√µes\n",
    "\n",
    "O m√©todo `stuffing` √© uma forma de resumir o texto alimentando o documento inteiro em um modelo de linguagem grande (LLM) em uma √∫nica chamada. Este m√©todo tem pr√≥s e contras.\n",
    "\n",
    "O m√©todo stuffing requer apenas uma √∫nica chamada para o LLM, que pode ser mais r√°pido do que outros m√©todos que requerem m√∫ltiplas chamadas. Ao resumir o texto, o LLM tem acesso a todos os dados de uma s√≥ vez, o que pode resultar em um resumo melhor.\n",
    "\n",
    "Por√©m, os LLMs t√™m um limite de contexto, que √© o n√∫mero m√°ximo de tokens que podem ser processados em uma √∫nica chamada. Se o documento for maior que o comprimento do contexto, o m√©todo de preenchimento n√£o funcionar√°. Al√©m disso, o m√©todo de preenchimento n√£o √© adequado para resumir documentos grandes, pois pode ser lento e n√£o produzir um bom resumo.\n",
    "\n",
    "Vamos explorar outras abordagens para ajudar a lidar com o texto mais longo do que o limite de comprimento de contexto dos LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM3V1JARZ9-k"
   },
   "source": [
    "## M√©todo 2: MapReduce\n",
    "\n",
    "O m√©todo `MapReduce` implementa um resumo em v√°rios est√°gios. √â uma t√©cnica para resumir grandes trechos de texto, primeiro resumindo trechos menores de texto e depois combinando esses resumos em um √∫nico resumo.\n",
    "\n",
    "Com o framework LangChain, voc√™ pode usar o m√©todo `MapReduceDocumentsChain` como parte da chamada ao m√©todo `load_summarize_chain`. Voc√™ s√≥ precisa definir `map_reduce` como `chain_type` do seu chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lagLXEamlPY2"
   },
   "source": [
    "### Prompt design com um `MapReduce` chain\n",
    "\n",
    "Em nosso exemplo, voc√™ tem um documento de 25 p√°ginas que precisa resumir.\n",
    "\n",
    "Com LangChain, na chain `map_reduce` o documento ser√° dividido em no m√°ximo 1.024 peda√ßos de token. Em seguida, ele executa o prompt inicial definido em cada bloco para gerar um resumo desse bloco. No exemplo abaixo, voc√™ usa o primeiro est√°gio ou prompt do mapa a seguir.\n",
    "\n",
    "```Escreva um sum√°rio conciso do texto abaixo delimitado por tr√™s aspas invertidas. Retorne sua resposta em bullets que cubram os pontos principais do texto. '''{text}''' SUMARIO EM BULLETS:```\n",
    "\n",
    "Depois que os resumos de todos os blocos s√£o gerados, ele executa um prompt diferente para combinar esses resumos em um √∫nico resumo. No exemplo abaixo, voc√™ usa o seguinte segundo est√°gio ou prompt combinado.\n",
    "\n",
    "```Escreva um sum√°rio conciso do texto abaixo delimitdo por aspas invertidas. Retorne sua resposta em bullets que cubram os pontos principais do texto.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6oHEtdSmsTn"
   },
   "outputs": [],
   "source": [
    "map_prompt_template = \"\"\"\n",
    "                      Escreva um resumo deste peda√ßo de texto que inclua os pontos principais e quaisquer detalhes importantes.\n",
    "                      {text}\n",
    "                      \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                      Escreva um sum√°rio conciso do texto abaixo delimitdo por aspas invertidas. \n",
    "                      Retorne sua resposta em bullets que cubram os pontos principais do texto.\n",
    "                      ```{text}```\n",
    "                      SUMARIO EM BULLETS:\n",
    "                      \"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXoz0uLDMoWD"
   },
   "source": [
    "### Gere sumariza√ß√µes utilizando o m√©todo MapReduce\n",
    "\n",
    "Depois de definir os prompts, voc√™ iniciarizar√° o chain `map_reduce_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRGJcBZeVdEa"
   },
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6fekDDr0hrJ"
   },
   "source": [
    "Em seguida, voc√™ gera resumos usando o chain. Observe que o LangChain utilizar√° um tokenizer com limite de 1.024 tokens por padr√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSC6w2TBV35q"
   },
   "outputs": [],
   "source": [
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": pages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meH2ELuz2H46"
   },
   "source": [
    "Ap√≥s a gera√ß√£o dos sum√°rios, voc√™ pode valid√°-los organizando as entradas sa√≠das em um Dataframe do Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6FRSR7xRLew"
   },
   "outputs": [],
   "source": [
    "final_mp_data = []\n",
    "for doc, out in zip(\n",
    "    map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_mp_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA9cnh8YaNbF"
   },
   "outputs": [],
   "source": [
    "pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "pdf_mp_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the dataframe by filename and page_number\n",
    "pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_mp_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yA0eM1K3cvH2"
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_mp_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_mp_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_mp_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_mp_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROrE1-HKpg7y"
   },
   "source": [
    "### Considera√ß√µes\n",
    "\n",
    "Com o m√©todo `MapReduce`, o modelo √© capaz de resumir um grande artigo superando o limite de contexto do m√©todo `Stuffing` com processamento paralelo.\n",
    "\n",
    "No entanto, o `MapReduce` requer m√∫ltiplas chamadas ao modelo e potencialmente perda de contexto entre as p√°ginas.\n",
    "\n",
    "Para lidar com esse desafio, voc√™ pode tentar outro m√©todo para resumir v√°rias p√°ginas por vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxdB-5PqgCf-"
   },
   "source": [
    "## M√©todo 3: MapReduce com Refinamento\n",
    "\n",
    "O m√©todo de MapReduce com Refinamento √© umaa alternativa para lidar com resumos de documentos grandes. Ele funciona primeiro executando um prompt inicial em um pequeno peda√ßo de dados, gerando uma sa√≠da inicial. Ent√£o, para cada documento subsequente, a sa√≠da do documento anterior √© transmitida junto com o novo documento, e o LLM √© solicitado a refinar a sa√≠da com base no novo documento.\n",
    "\n",
    "No LangChain, voc√™ pode usar `MapReduceDocumentsChain` como parte da chaamada ao m√©todo `load_summarize_chain`. O que voc√™ precisa fazer √© definir `refine` como `chain_type` da sua cadeia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjj2UZilDF4Q"
   },
   "source": [
    "### Design de prompt com um `Refine` chain\n",
    "\n",
    "Usando LangChain, o chain `refine` requer dois prompts.\n",
    "\n",
    "O prompt de pergunta para gerar a sa√≠da para a tarefa subsequente. O prompt de refinamento para refinar a sa√≠da com base no conte√∫do gerado.\n",
    "\n",
    "Neste exemplo, o prompt da pergunta √©:\n",
    "\n",
    "```\n",
    "Forne√ßa um resumo do texto a seguir.\n",
    "TEXTO: {texto}\n",
    "RESUMO:\n",
    "```\n",
    "\n",
    "e o prompt de refinamento √©:\n",
    "\n",
    "```\n",
    "Escreva um resumo conciso do texto a seguir delimitado por aspas triplas.\n",
    "Retorne sua resposta em marcadores que cubram os pontos-chave do texto.\n",
    "'''{texto}'''\n",
    "SUMARIO EM BULLETS:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiZX45Z5VTwS"
   },
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                  Forne√ßa um resumo do texto a seguir.\n",
    "                  TEXTO: {text}\n",
    "                  RESUMO:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Escreva um resumo conciso do texto a seguir delimitado por aspas triplas.\n",
    "              Retorne sua resposta em marcadores que cubram os pontos-chave do texto.\n",
    "              ```{text}```\n",
    "              SUMARIO EM BULLETS:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-USlaSPbM0rs"
   },
   "source": [
    "### Gere sumariza√ß√µes com o m√©todo de MapReduce com Refinamento\n",
    "\n",
    "Depois de definir os prompts, voc√™ inicia uma cadeia de resumo usando o tipo de chain `refine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-Sv3HO1U3hi"
   },
   "outputs": [],
   "source": [
    "refine_chain = load_summarize_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9EZCDK-MQJH"
   },
   "source": [
    "Em seguida, voc√™ usar√° a chain para sumarizar o documento utilizando o m√©todo de MapReduce com Refinamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHwwab7vXNa1"
   },
   "outputs": [],
   "source": [
    "refine_outputs = refine_chain({\"input_documents\": pages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUqpki5EMYEr"
   },
   "source": [
    "Below you can see the resulting summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j5cUGStZ5WF"
   },
   "outputs": [],
   "source": [
    "final_refine_data = []\n",
    "for doc, out in zip(\n",
    "    refine_outputs[\"input_documents\"], refine_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_refine_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_7Mm9cEmGOV"
   },
   "outputs": [],
   "source": [
    "pdf_refine_summary = pd.DataFrame.from_dict(final_refine_data)\n",
    "pdf_refine_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the datafram by filename and page_number\n",
    "pdf_refine_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_refine_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvLVCs8Gbwbw"
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_refine_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_refine_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_refine_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_refine_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dwgbRTrM5Cb"
   },
   "source": [
    "### Considera√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H0Y5pPcXbgm"
   },
   "source": [
    "O m√©todo de MapReduce com Refinamento pode extrair contextos mais relevantes e ter menos perdas do que MapReduce tradicional. No entanto, requer muito mais chamadas para o LLM do que para o Stuffing, e essas chamadas n√£o s√£o independentes, o que significa que n√£o podem ser paralelizadas. Al√©m disso, existe alguma depend√™ncia potencial na ordena√ß√£o dos documentos. Documentos mais recentes podem se tornar mais relevantes, pois este m√©todo sofre de vi√©s de atualidade."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "summarization_large_documents_langchain.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
