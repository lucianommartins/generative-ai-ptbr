{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Perguntas e respostas com modelos generativos na Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Execute no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      Veja no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Execute no Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Visão geral\n",
    "\n",
    "Grandes modelos de linguagem podem ser usados para várias tarefas de processamento de linguagem natural, incluindo perguntas e respostas (Q&A). Esses modelos são treinados em uma grande quantidade de dados de texto e podem gerar respostas de alta qualidade para uma ampla gama de perguntas. Uma coisa a observar aqui é que a maioria dos modelos tem datas limite em relação ao seu conhecimento, e perguntar qualquer coisa muito recente pode resultar em uma resposta incompleta, imaginativa ou incorreta (ou seja, uma alucinação).\n",
    "\n",
    "Este bloco de anotações aborda os fundamentos dos prompts para responder a perguntas usando um modelo generativo. Além disso, apresenta o `domínio aberto` (conhecimento disponível na internet pública) e o `domínio fechado` (conhecimento que é mais privado - normalmente conhecimento empresarial ou pessoal).\n",
    "\n",
    "Saiba mais sobre o design de prompt na [documentação oficial](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview#prompt_structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objetivo\n",
    "\n",
    "Ao final deste notebook, você será capaz de escrever prompts para os seguintes cenários:\n",
    "\n",
    "* Perguntas de **domínio aberto**:\n",
    "     * Solicitação one-shot\n",
    "     * Solicitação few-shot\n",
    "\n",
    "\n",
    "* Perguntas de **domínio fechado**:\n",
    "     * Fornecer conhecimento personalizado como contexto\n",
    "     * Instrução de ajuste das saídas\n",
    "     * Solicitação few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Saiba mais sobre possíveis custos envolvidos [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "e use a [Calculadora de preços](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Instalando os SDK da Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ad0c445061",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para reiniciar o kernel ou use o botão para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hsqwn4hkLKE"
   },
   "outputs": [],
   "source": [
    "# # Reinicia automaticamente o kernel após as instalações para que seu ambiente possa acessar os novos pacotes\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se você estiver usando o **Colab** para executar este notebook, descomente a célula abaixo e continue.\n",
    "* Se você estiver usando o **Vertex AI Workbench**, confira as instruções de configuração [aqui](../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para realizar o processo adequado de inicialização da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[seu-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Perguntas e respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNNEz7vGFYUP"
   },
   "source": [
    "Os recursos de resposta a perguntas exigem o fornecimento de um prompt ou uma pergunta que o modelo possa usar para gerar uma resposta. O prompt pode ser algumas palavras ou algumas frases completas, dependendo da complexidade da pergunta.\n",
    "\n",
    "Ao criar um prompt de resposta a perguntas, é essencial ser específico e fornecer o máximo de contexto possível. Isso ajuda o modelo a entender a intenção por trás da pergunta e gerar uma resposta relevante. Por exemplo, se você quiser perguntar:\n",
    "\n",
    "```\n",
    "\"Qual é a capital da França?\",\n",
    "\n",
    "então um bom prompt poderia ser:\n",
    "\n",
    "\"Diga-me o nome da cidade que é capital da França.\"\n",
    "\n",
    "```\n",
    "\n",
    "Além de ser específico, o prompt também deve ser gramaticalmente correto e livre de erros ortográficos. Isso ajuda o modelo a gerar uma resposta fácil de entender e contém menos erros ou imprecisões.\n",
    "\n",
    "Ao fornecer prompts específicos e ricos em contexto, você pode ajudar o modelo a entender a intenção por trás da pergunta e gerar respostas precisas e relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5N9ZnlECm-z"
   },
   "source": [
    "Abaixo estão algumas diferenças entre as categorias **domínio aberto** e **domínio fechado** para prompts de resposta a perguntas.\n",
    "\n",
    "* **Domínio aberto**: Todas as perguntas cujas respostas já estão disponíveis online. Eles podem pertencer a qualquer categoria, como história, geografia, países, política, química, etc. Isso inclui perguntas triviais ou de conhecimento geral, como:\n",
    "\n",
    "```\n",
    "P: Quem ganhou o ouro olímpico na natação?\n",
    "P: Quem é o presidente de [dado país]?\n",
    "P: Quem escreveu [livro específico]\"?\n",
    "```\n",
    "\n",
    "Lembre-se do corte de treinamento de modelos generativos, pois perguntas envolvendo informações mais recentes do que aquelas com as quais o modelo foi treinado podem fornecer respostas incorretas ou imaginativas.\n",
    "\n",
    "\n",
    "* **Domínio fechado**: Se você possui alguma base de conhecimento interna não disponível na internet pública, então ela pertence à categoria _domínio fechado_.\n",
    "Você pode passar esse conhecimento \"privado\" como contexto para o modelo. Se solicitado corretamente, é mais provável que o modelo responda dentro do contexto fornecido e menos provável que dê respostas além da Internet aberta.\n",
    "\n",
    "Considere o exemplo de criar um bot de perguntas e respostas sobre a documentação interna do produto. Nesse caso, você pode passar a documentação completa para o modelo e solicitar que ele responda apenas com base nisso.\n",
    "\n",
    "Prompt típico para **domínio fechado**:\n",
    "\n",
    "```\n",
    "Prompt: f\"\"\" Resposta do contexto abaixo: \\n\\n\n",
    "contexto: {sua base de conhecimento} \\n\n",
    "pergunta: {pergunta específica para essa base de conhecimento} \\n\n",
    "resposta: {a ser previsto pelo modelo} \\n\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Abaixo estão alguns exemplos para entender esses diferentes tipos de prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBoN6zixDSiX"
   },
   "source": [
    "### Domínio aberto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJnv8XhnDXQm"
   },
   "source": [
    "#### Prompt zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaYoQuRwCm-z"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Q: Quem era o presidente do Brasil em 2010?\\n\n",
    "            A:\n",
    "         \"\"\"\n",
    "print(\n",
    "    generation_model.predict(prompt, max_output_tokens=256, temperature=0.1).text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qcUdUgwCm-z"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Q: Qual a montanha mais alta do mundo?\\n\n",
    "            A:\n",
    "         \"\"\"\n",
    "print(\n",
    "    generation_model.predict(prompt, max_output_tokens=20, temperature=0.1).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HShw52X2Dcmx"
   },
   "source": [
    "#### Prompt few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj_2hHAWE8vh"
   },
   "source": [
    "Digamos que você queira obter uma resposta curta do modelo (como apenas um nome específico). Para fazer isso, você pode aproveitar um prompt few-shot e fornecer exemplos ao modelo para ilustrar o comportamento esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE5yCAaqDg7m"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Q: Quem é atualmente o presidente da França?\\n\n",
    "         A: Emmanuel Macron \\n\\n\n",
    "\n",
    "         Q: Quem inventou o telefone? \\n\n",
    "         A: Alexander Graham Bell \\n\\n\n",
    "\n",
    "         Q: Quem escreveu o livro \"1984\"?\n",
    "         A: George Orwell\n",
    "\n",
    "         Q: Quem descobriu a penicilina?\n",
    "         A:\n",
    "         \"\"\"\n",
    "print(\n",
    "    generation_model.predict(prompt, max_output_tokens=20, temperature=0.1).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGvs0jFsUlvM"
   },
   "source": [
    "#### Prompt zero-shot vs few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yjsAMuMUfZC"
   },
   "source": [
    "O prompt zero-shot pode ser útil para gerar texto rapidamente para novas tarefas, mas a qualidade do texto gerado pode ser inferior à de um prompt few-shot com exemplos bem escolhidos. O prompt few-shot geralmente é mais adequado para tarefas que exigem um alto grau de especificidade ou conhecimento específico do domínio, mas requer algum pensamento adicional e potencialmente dados para configurar o prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6UiJTxXEs4t"
   },
   "source": [
    "### Domínio fechado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03ZITm4AGBvP"
   },
   "source": [
    "#### Adicionando conhecimento como contexto nos prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkhqjmB6VqPx"
   },
   "source": [
    "Imagine um cenário em que você gostaria de criar um bot de perguntas e respostas que aceita a documentação interna e permite que os usuários façam perguntas sobre ela.\n",
    "\n",
    "No exemplo abaixo, o Google Cloud Storage e a documentação da política de conteúdo são adicionados ao prompt, para que a API PaLM possa usá-lo para responder a perguntas subsequentes com o contexto fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Política de armazenamento e conteúdo \\n\n",
    "Qual é a durabilidade dos meus dados no Cloud Storage? \\n\n",
    "O armazenamento em nuvem foi projetado para durabilidade anual de 99,999999999% (11 9), o que é apropriado até mesmo para armazenamento primário e\n",
    "aplicativos críticos para os negócios. Esse alto nível de durabilidade é alcançado por meio de codificação de eliminação que armazena partes de dados de forma redundante\n",
    "em vários dispositivos localizados em várias zonas de disponibilidade.\n",
    "Os objetos gravados no Cloud Storage devem ser armazenados de forma redundante em pelo menos duas zonas de disponibilidade diferentes antes do\n",
    "a gravação é reconhecida como bem-sucedida. As somas de verificação são armazenadas e revalidadas regularmente para verificar proativamente se os dados\n",
    "integridade de todos os dados em repouso, bem como para detectar corrupção de dados em trânsito. Se necessário, as correções são automaticamente\n",
    "feito usando dados redundantes. Os clientes podem, opcionalmente, habilitar o controle de versão do objeto para adicionar proteção contra exclusão acidental.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Como podemos alcançar alta disponibilidade?\"\n",
    "\n",
    "prompt = f\"\"\"Responda a questão abaixo dada o contexto abaixo:\n",
    "Contexto: {context} \\n\n",
    "Questão: {question} \\n\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(prompt).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagWC4VcQIw6"
   },
   "source": [
    "#### Ajustando as instruções de output do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9UkogWHXM6N"
   },
   "source": [
    "Outra maneira de ajudar os modelos de linguagem é fornecer instruções adicionais para enquadrar a saída no prompt. Para garantir que o modelo não responda a nada fora do contexto, o prompt pode especificar que a resposta deve ser \"Informações não disponíveis no contexto fornecido\", se for o caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouq8FfwSQIBT"
   },
   "outputs": [],
   "source": [
    "question = \"Qual é a vantagem de habilitar o controle de versão no Cloud Storage?\"\n",
    "prompt = f\"\"\"Responda a pergunta abaixo utilizando as informações disponíveis como {{Context:}}. \\n\n",
    "Se a resposta não estiver disponível no {{Context:}} e você não esteja confiante no output, por favor\n",
    "diga \"Informação não disponível no contexto disponibilizado\". \\n\\n\"\"\"\n",
    "prompt += f\"Contexto: {context} \\n\"\n",
    "prompt += f\"Pergunta: {question} \\n\"\n",
    "prompt += \"Resposta: \"\n",
    "print(f\"[Pergunta]\\n{question}\")\n",
    "print(\"[Resposta]\")\n",
    "print(\n",
    "    generation_model.predict(prompt, max_output_tokens=256, temperature=0.3).text\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "question = \"Qual tipo de máquinas são requeridas para fazer o hosting de modelos na Vertex AI?\"\n",
    "prompt = f\"\"\"Responda a pergunta abaixo utilizando as informações disponíveis como {{Context:}}. \\n\n",
    "Se a resposta não estiver disponível no {{Context:}} e você não esteja confiante no output, por favor\n",
    "diga \"Informação não disponível no contexto disponibilizado\". \\n\\n\"\"\"\n",
    "prompt += f\"Contexto: {context} \\n\"\n",
    "prompt += f\"Pergunta: {question} \\n\"\n",
    "prompt += \"Resposta: \"\n",
    "print(f\"[Pergunta]\\n{question}\")\n",
    "print(\"[Resposta]\")\n",
    "print(\n",
    "    generation_model.predict(prompt, max_output_tokens=256, temperature=0.3).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZJfZShPRGqU"
   },
   "source": [
    "#### Prompt few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdSEQeQIS6pt"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Contexto:\n",
    "O termo \"inteligência artificial\" foi cunhado pela primeira vez por John McCarthy em 1956. Desde então, a IA se desenvolveu em um vasto\n",
    "campo com inúmeras aplicações, desde carros autônomos até assistentes virtuais como Siri e Alexa.\n",
    "\n",
    "Pergunta:\n",
    "O que é inteligência artificial?\n",
    "\n",
    "Responder:\n",
    "A inteligência artificial refere-se à simulação da inteligência humana em máquinas programadas para pensar e aprender como humanos.\n",
    "\n",
    "---\n",
    "\n",
    "Contexto:\n",
    "Os irmãos Wright, Orville e Wilbur, foram dois pioneiros da aviação americana a quem se atribui a invenção e\n",
    "construindo o primeiro avião bem-sucedido do mundo e fazendo o primeiro voo humano controlado, motorizado e sustentado mais pesado que o ar,\n",
    "  em 17 de dezembro de 1903.\n",
    "\n",
    "Pergunta:\n",
    "Quem eram os irmãos Wright?\n",
    "\n",
    "Responder:\n",
    "Os irmãos Wright foram pioneiros da aviação americana que inventaram e construíram o primeiro avião de sucesso do mundo.\n",
    "e fez o primeiro voo humano controlado, motorizado e sustentado mais pesado que o ar, em 17 de dezembro de 1903.\n",
    "\n",
    "---\n",
    "\n",
    "Contexto:\n",
    "A Mona Lisa é um retrato do século XVI pintado por Leonardo da Vinci durante o Renascimento italiano. é um dos\n",
    "as pinturas mais famosas do mundo, conhecidas pelo sorriso enigmático da mulher retratada na pintura.\n",
    "\n",
    "Pergunta:\n",
    "Quem pintou a Mona Lisa?\n",
    "\n",
    "Responder:\n",
    "\"\"\"\n",
    "print(generation_model.predict(prompt,).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leYIui80Q4tH"
   },
   "source": [
    "### Perguntas e resposta extrativas\n",
    "\n",
    "No próximo exemplo, o modelo generativo é guiado para entender o significado da pergunta e da passagem e identificar as informações relevantes na passagem que responde à pergunta. O modelo recebe uma pergunta e uma passagem de texto e é solicitado a encontrar a resposta para a pergunta dentro da passagem. A resposta é tipicamente uma frase ou sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPZqm0QJQ4tH"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Background: Há evidências de que houve mudanças significativas na vegetação da floresta amazônica ao longo dos últimos 21.000 anos através do Último Máximo Glacial (LGM) e subsequente deglaciação.\n",
    "Análises de depósitos de sedimentos de paleolagos da bacia amazônica e do leque amazônico indicam que a precipitação na bacia durante o LGM foi menor do que no presente, e isso quase certamente foi\n",
    "associada à reduzida cobertura de vegetação tropical úmida na bacia. Há um debate, no entanto, sobre quão extensa foi essa redução. Alguns cientistas argumentam que a floresta tropical foi reduzida a pequenas\n",
    "refúgios isolados separados por floresta aberta e pastagens; outros cientistas argumentam que a floresta tropical permaneceu praticamente intacta, mas estendeu-se menos ao norte, sul e leste do que é visto hoje.\n",
    "Este debate tem se mostrado difícil de resolver porque as limitações práticas de trabalhar na floresta tropical significam que a amostragem de dados é desviada do centro da bacia amazônica, e ambos\n",
    "explicações são razoavelmente bem suportadas pelos dados disponíveis.\n",
    "\n",
    "P: O que significa LGM?\n",
    "R: Último Máximo Glacial.\n",
    "\n",
    "P: O que indica a análise dos depósitos de sedimentos?\n",
    "R: A precipitação na bacia durante o LGM foi menor do que no presente.\n",
    "\n",
    "P: Quais são alguns dos argumentos dos cientistas?\n",
    "R: A floresta tropical foi reduzida a pequenos refúgios isolados, separados por floresta aberta e pastagens.\n",
    "\n",
    "P: Houve grandes mudanças na vegetação da floresta amazônica nos últimos quantos anos?\n",
    "R: 21.000.\n",
    "\n",
    "P: O que causou mudanças na vegetação da floresta amazônica?\n",
    "R: O Último Máximo Glacial (LGM) e subsequente deglaciação\n",
    "\n",
    "P: O que foi analisado para comparar as chuvas da Amazônia no passado e no presente?\n",
    "R: Depósitos de sedimentos.\n",
    "\n",
    "P: A que foi atribuída a menor precipitação na Amazônia durante o LGM?\n",
    "R:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94d80fb55f48"
   },
   "source": [
    "### Avaliação de respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b620d23a7634"
   },
   "source": [
    "Você pode avaliar os resultados da tarefa de pergunta e resposta se as respostas de verdade de cada pergunta estiverem disponíveis. No prompt zero-shot, você só pode usar perguntas de 'domínio aberto'. No entanto, com perguntas de 'domínio fechado', você pode adicionar contexto e avaliar de forma semelhante. Para mostrar como isso funciona, comece criando um dataframe simples com perguntas e respostas de verdade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e813a463531"
   },
   "outputs": [],
   "source": [
    "qa_data = {\n",
    "    \"perguntas\": [\n",
    "        \"Na barra de endereços dos navegadores, o que significa “www”?\",\n",
    "        \"Quem foi a primeira mulher a ganhar um prêmio Nobel\",\n",
    "        \"Qual é o nome do maior oceano da Terra?\",\n",
    "    ],\n",
    "    \"respostas_groundtruth\": [\"World Wide Web\", \"Marie Curie\", \"O oceano pacífico\"],\n",
    "}\n",
    "qa_data_df = pd.DataFrame(qa_data)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "951a147dc79d"
   },
   "source": [
    "Agora que você tem os dados com perguntas e respostas básicas, pode chamar o modelo de geração PaLM 2 para cada linha de revisão usando a função `apply`. Cada linha usará o prompt dinâmico para prever a resposta usando a API PaLM. Vamos salvar os resultados na coluna `answer_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffc47e0cb5b9"
   },
   "outputs": [],
   "source": [
    "def geraResposta(row):\n",
    "    prompt = f\"\"\"Responda as perguntas abaixo da forma mais precisa possível.\\n\\n\n",
    "                 pergunta: {row}\n",
    "                 resposta:\n",
    "              \"\"\"\n",
    "    return generation_model.predict(prompt=prompt).text\n",
    "\n",
    "qa_data_df[\"respostas_geradas\"] = qa_data_df[\"perguntas\"].apply(geraResposta)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fe997dbf788"
   },
   "source": [
    "Você pode querer avaliar as respostas previstas pela API do PaLM. No entanto, será mais complexo do que a classificação do texto, pois as respostas podem diferir da verdade e podem ser apresentadas em um pouco mais/menos palavras.\n",
    "\n",
    "Por exemplo, você pode observar a pergunta \"Qual é o nome do maior oceano da Terra?\" e veja que o modelo previu \"Oceano Pacífico\" quando um rótulo de verdade é \"O Oceano Pacífico\" com o extra \"O\". Agora, se você usar as métricas de classificação simples, considerará isso uma previsão errada, pois as strings originais e previstas têm uma diferença. No entanto, você pode ver que a resposta está correta, pois um \"The\" extra está causando o problema. É um problema simples de comparação de strings.\n",
    "\n",
    "A solução para a comparação de strings em que `ground_thruth` e `predicted` podem ter algumas letras extras ou menos, uma abordagem é usar um algoritmo de correspondência difusa.\n",
    "A correspondência de string difusa usa [Distância Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance) para calcular as diferenças entre duas strings.\n",
    "\n",
    "Aqui um exemplo usando a biblioteca `fuzzywuzzy`, que nos dá a `distância de Levenshtein` entre duas strings, mas em proporção. A pontuação bruta de proporção mede a similaridade da string como um int no intervalo [0, 100]. Para duas strings X e Y, a pontuação é definida por `int(round((2.0 * M / T) * 100))` onde `T` é o número total de caracteres em ambas as strings e `M` é o número de correspondências nas duas strings.\n",
    "\n",
    "Leia mais aqui sobre a [fórmula de proporção](https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Ratio.html):\n",
    "\n",
    "Você pode ver um exemplo para entender melhor.\n",
    "```\n",
    "String1: \"isto é um teste\"\n",
    "String2: \"isto é um teste!\"\n",
    "\n",
    "Razão Fuzz => 97 #\n",
    "\n",
    "Fuzz Partial Ratio => 100 #Como a maioria dos caracteres são iguais e em uma sequência semelhante, o algoritmo calcula a proporção parcial como 100 e ignora adições simples (novos caracteres).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b170579a455"
   },
   "source": [
    "Primeiro instale os pacotes `fuzzywuzzy` e `python-Levenshtein`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c55ea0eaed0"
   },
   "outputs": [],
   "source": [
    "!pip install -q python-Levenshtein --upgrade --user\n",
    "!pip install -q fuzzywuzzy --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f048152519f"
   },
   "source": [
    "E então calcule o match fuzzy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "040c1f9a175b"
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def get_fuzzy_match(df):\n",
    "    return fuzz.partial_ratio(df[\"respostas_groundtruth\"], df[\"respostas_geradas\"])\n",
    "\n",
    "\n",
    "qa_data_df[\"match_score\"] = qa_data_df.apply(get_fuzzy_match, axis=1)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11e266c49860"
   },
   "source": [
    "Agora que você tem a pontuação de correspondência individual (parcial), pode obter a média ou a média de toda a coluna para ter uma noção dos dados gerais.\n",
    "\n",
    "Pontuações próximas a 100 significam que o PaLM 2 pode prever com mais precisão; se a pontuação estiver próxima de 50 ou 0, não teve um bom desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dae6a92a7650"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Na média, o score de todas as predições do PaLM 2 foi: \",\n",
    "    qa_data_df[\"match_score\"].mean(),\n",
    "    \" %\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9e78972cad1"
   },
   "source": [
    "Nesse caso, você obteve um score de mais ou menos 97% como pontuação média, pois algumas previsões faltem algumas palavras. Isso significa que você está muito próximo da verdade básica e algumas respostas estão apenas perdendo a escrita exata da verdade básica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "question_answering.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
