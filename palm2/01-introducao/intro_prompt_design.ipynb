{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Boas pr√°ticas para a cria√ß√£o de prompts\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Execute no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      Veja no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Execute no Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Vis√£o geral\n",
    "\n",
    "Este notebook aborda os fundamentos de design de prompts, incluindo algumas pr√°ticas recomendadas.\n",
    "\n",
    "Saiba mais sobre o design de prompt na [documenta√ß√£o oficial](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objetivo\n",
    "\n",
    "Neste notebook, voc√™ aprender√° as pr√°ticas recomendadas sobre o design de prompts -- como projetar prompts para melhorar a qualidade de suas respostas.\n",
    "\n",
    "Este notebook abrange as seguintes pr√°ticas recomendadas para engenharia imediata:\n",
    "\n",
    "- Ser conciso\n",
    "- Seja espec√≠fico e com um texto bem definido\n",
    "- Pe√ßa uma tarefa de cada vez\n",
    "- Transforme tarefas generativas em tarefas de classifica√ß√£o\n",
    "- Melhore a qualidade da resposta incluindo exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea013f50403c"
   },
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Studio\n",
    "\n",
    "Saiba mais sobre poss√≠veis custos envolvidos [pre√ßos da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "e use a [Calculadora de pre√ßos](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e663cb43fa0"
   },
   "source": [
    "### Instalando os SDK da Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ad0c445061",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cebd6983cbad"
   },
   "source": [
    "**Somente Colab:** Descomente a c√©lula a seguir para reiniciar o kernel ou use o bot√£o para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bea801acf6b5"
   },
   "outputs": [],
   "source": [
    "# # Reinicia automaticamente o kernel ap√≥s as instala√ß√µes para que seu ambiente possa acessar os novos pacotes\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a386d25fa8f"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se voc√™ estiver usando o **Colab** para executar este notebook, descomente a c√©lula abaixo e continue.\n",
    "* Se voc√™ estiver usando o **Vertex AI Workbench**, confira as instru√ß√µes de configura√ß√£o [aqui](../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bd1dca8e9a7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Importando as bibliotecas necess√°rias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a c√©lula a seguir para realizar o processo adequado de inicializa√ß√£o da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[seu-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Boas pr√°ticas de design de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df7d153f4928"
   },
   "source": [
    "A engenharia de prompt trata de como projetar seus prompts para que a resposta seja o que voc√™ realmente esperava ver.\n",
    "\n",
    "A ideia de usar prompts \"desagrad√°veis\" √© minimizar o ru√≠do em seu prompt para reduzir a possibilidade de o LLM interpretar mal a inten√ß√£o do prompt. Abaixo est√£o algumas diretrizes sobre como projetar prompts \"desagrad√°veis\".\n",
    "\n",
    "Nesta se√ß√£o, voc√™ abordar√° as seguintes pr√°ticas recomendadas quando a engenharia solicitar:\n",
    "\n",
    "* Ser conciso\n",
    "* Seja espec√≠fico e com texto bem definido\n",
    "* Pe√ßa uma tarefa de cada vez\n",
    "* Melhore a qualidade da resposta incluindo exemplos\n",
    "* Transforme tarefas generativas em tarefas de classifica√ß√£o para melhorar a seguran√ßa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43c1169ac435"
   },
   "source": [
    "### Seja conciso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0f380f1620e"
   },
   "source": [
    "üõë N√£o recomendado. O prompt abaixo √© desnecessariamente detalhado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6a1697c3603"
   },
   "outputs": [],
   "source": [
    "prompt = \"Que nomes voc√™ acha que seriam interessantes para uma floricultura que se especializa mais em buqu√™s de flores secas do que frescas? Muit√≠ssimo brigado!\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2307f56a9b75"
   },
   "source": [
    "‚úÖ Recomendado. O prompt abaixo √© direto ao ponto e conciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc666404f47c"
   },
   "outputs": [],
   "source": [
    "prompt = \"Sugira cinco nomes para uma floricultura que vende buqu√™s de flores secas\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17f6c48bba91"
   },
   "source": [
    "### Seja espec√≠fico e escreva textos bem definidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "269b428e1563"
   },
   "source": [
    "Suponha que voc√™ queira pensar em maneiras criativas de descrever a Terra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6436ee2ff406"
   },
   "source": [
    "üõë N√£o recomendado. O prompt abaixo √© muito gen√©rico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "261b7f6e94c5"
   },
   "outputs": [],
   "source": [
    "prompt = \"Fale-me sobre a Terra\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bebfecd2912"
   },
   "source": [
    "‚úÖ Recomendado. O prompt abaixo √© espec√≠fico e bem definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "242b1b3bae6e"
   },
   "outputs": [],
   "source": [
    "prompt = \"Gere uma lista de motivos que fazem a Terra ser √∫nica comparada √† outros planetas\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20dca9a05eab"
   },
   "source": [
    "### Pe√ßa uma tarefa de cada vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9019d443179"
   },
   "source": [
    "üõë N√£o recomendado. O prompt abaixo tem duas partes para a pergunta que pode ser feita separadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70b3b5e5825d"
   },
   "outputs": [],
   "source": [
    "prompt = \"Qual √© a melhor forma para ferver √°gua e por que o c√©u √© azul?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7936fb58c16a"
   },
   "source": [
    "‚úÖ Recomendado. Os prompts abaixo solicitam uma tarefa por vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2564dad6c8db"
   },
   "outputs": [],
   "source": [
    "prompt = \"Qual √© a melhor forma para ferver √°gua?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "770c695ade92"
   },
   "outputs": [],
   "source": [
    "prompt = \"Por que o c√©u √© azul?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff606011aa86"
   },
   "source": [
    "### Cuidado com as alucina√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "956ce45b06a7"
   },
   "source": [
    "Embora os LLMs tenham sido treinados em uma grande quantidade de dados, eles podem gerar textos contendo declara√ß√µes n√£o fundamentadas na verdade ou na realidade; essas respostas do LLM s√£o frequentemente chamadas de \"alucina√ß√µes\" devido √†s suas capacidades limitadas de memoriza√ß√£o. Observe que simplesmente solicitar que o LLM forne√ßa uma cita√ß√£o n√£o √© uma solu√ß√£o para esse problema, pois h√° inst√¢ncias de LLMs que fornecem cita√ß√µes falsas ou imprecisas. \n",
    "\n",
    "Lidar com alucina√ß√µes √© um desafio fundamental dos LLMs e uma √°rea de pesquisa em andamento, por isso √© importante estar ciente de que os LLMs podem parecer dar a voc√™ declara√ß√µes confiantes e corretas que, na verdade, s√£o incorretas.\n",
    "\n",
    "Observe que, se voc√™ pretende usar LLMs para os casos de uso criativo, alucinar pode ser bastante √∫til."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c9d5f66179a"
   },
   "source": [
    "Tente o prompt como o abaixo repetidamente. Voc√™ pode notar que √†s vezes ele dir√° com confian√ßa, mas imprecisamente, \"O primeiro elefante a visitar a lua foi Luna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d813b9061b08"
   },
   "outputs": [],
   "source": [
    "prompt = \"Quem foi o primeiro elefante a visitar a lua?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "029e23abfd56"
   },
   "source": [
    "### Transforme tarefas generativas em tarefas de classifica√ß√£o para reduzir a variabilidade de sa√≠da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d943941d6e59"
   },
   "source": [
    "#### Tarefas generativas levam a uma maior variabilidade de sa√≠da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37528e6c9754"
   },
   "source": [
    "O prompt abaixo resulta em uma resposta aberta, √∫til para brainstorming, mas a resposta √© altamente vari√°vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8e2dc39e9ae"
   },
   "outputs": [],
   "source": [
    "prompt = \"Sou um estudante de ensino m√©dio. Recomende-me atividades de programa√ß√£o para melhorar meus skills\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f71a6fa2b4bb"
   },
   "source": [
    "#### Tarefas de classifica√ß√£o reduzem a variabilidade de sa√≠da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "917517465dac"
   },
   "source": [
    "O prompt abaixo resulta em uma escolha e pode ser √∫til se voc√™ quiser que a sa√≠da seja mais f√°cil de controlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3feb93d9df81"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Sou um estudante de ensino m√©dio. Quais destas atividades voc√™ recomenda e porqu√™:\n",
    "a) Aprender Python\n",
    "b) Aprender Javascript\n",
    "c) Aprender Fortran\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32290ac9fb2b"
   },
   "source": [
    "### Melhore a qualidade da resposta incluindo exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "132834f5db2c"
   },
   "source": [
    "Outra maneira de melhorar a qualidade da resposta √© adicionar exemplos em seu prompt. O LLM aprende no contexto dos exemplos sobre como responder. Normalmente, um a cinco exemplos (shots) s√£o suficientes para melhorar a qualidade das respostas. Incluir muitos exemplos pode fazer com que o modelo ajuste demais os dados e reduza a qualidade das respostas.\n",
    "\n",
    "Semelhante ao treinamento de modelo cl√°ssico, a qualidade e a distribui√ß√£o dos exemplos s√£o muito importantes. Escolha exemplos representativos dos cen√°rios que voc√™ precisa que o modelo aprenda e mantenha a distribui√ß√£o dos exemplos (por exemplo, n√∫mero de exemplos por classe no caso de classifica√ß√£o) alinhada com sua distribui√ß√£o real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46520d938b6a"
   },
   "source": [
    "#### Zero-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46d3b47e6cea"
   },
   "source": [
    "Abaixo est√° um exemplo de prompt zero-shot, onde voc√™ n√£o fornece nenhum exemplo para o LLM dentro do pr√≥prio prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cbe03eb0b71"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Decida se um Tweet apresenta um sentimento positivo, negativo ou neutro.\n",
    "\n",
    "Tweet: Eu amei os seus videos mais recentes no YouTube!\n",
    "Sentimento: \n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0daabca1359"
   },
   "source": [
    "#### One-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42c4652fc5c2"
   },
   "source": [
    "Abaixo est√° um exemplo one-shot, onde voc√™ fornece um exemplo para o LLM dentro do prompt para fornecer alguma orienta√ß√£o sobre o tipo de resposta que voc√™ deseja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfe584860787"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Decida se um Tweet apresenta um sentimento positivo, negativo ou neutro.\n",
    "\n",
    "Tweet: Eu amei os seus videos mais recentes no YouTube!\n",
    "Sentimento: positivo\n",
    "\n",
    "Tweet: Foi dif√≠cil. Super chato üò†\n",
    "Sentimento:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef58c35005c0"
   },
   "source": [
    "#### Few-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b630e8947b60"
   },
   "source": [
    "Abaixo est√° um exemplo de few-shot, onde voc√™ fornece um exemplo para o LLM dentro do prompt para dar alguma orienta√ß√£o sobre o tipo de resposta que voc√™ deseja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb3ba21bbd11"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Decida se um Tweet apresenta um sentimento positivo, negativo ou neutro.\n",
    "\n",
    "Tweet: Eu amei os seus videos mais recentes no YouTube!\n",
    "Sentimento: positivo\n",
    "\n",
    "Tweet: Foi dif√≠cil. Super chato üò†\n",
    "Sentimento: negativo\n",
    "\n",
    "Tweet: Uma coisa me surpreendeu neste video - Ele foi realmente original. Ele n√£o foi o mesmo velho conte√∫do reciclado. Assista - voc√™ n√£o se arrepender√°.\n",
    "Sentimento:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4023be726eb"
   },
   "source": [
    "#### Escolhendo entre os m√©todos zero-shot, one-shot ou few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d7870ff75cc"
   },
   "source": [
    "Qual t√©cnica de prompt usar depender√° exclusivamente do seu objetivo. Os prompts zero-shot s√£o mais abertos e podem fornecer respostas criativas, enquanto os prompts one-shot e few-shot ensinam o modelo a se comportar para que voc√™ possa obter respostas mais previs√≠veis que sejam consistentes com os exemplos fornecidos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_prompt_design.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
