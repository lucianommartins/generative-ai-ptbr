{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Primeiros passos com a Vertex AI PaLM API e Python SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Execute no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      Veja no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Execute no Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Visão geral\n",
    "\n",
    "### O que são LLMs?\n",
    "Modelos de linguagem grandes (ou *Large Language Models* ou simplesmente *LLMs*) são modelos de aprendizado profundo treinados em conjuntos de dados massivos de texto. Os LLMs podem traduzir linguagem, resumir texto, gerar escrita criativa, gerar código, capacitar chatbots e assistentes virtuais e complementar mecanismos de pesquisa e sistemas de recomendação.\n",
    "\n",
    "### PaLM 2\n",
    "Seguindo seu antecessor, [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), [PaLM 2](https://ai.google/ Discover/palm2) é um LLM do Google que se baseia nas pesquisas do Google em aprendizado de máquina e IA responsável. O PaLM 2 se destaca em tarefas como criação de textos, tradução e geração de código devido à forma como foi construído.\n",
    "\n",
    "PaLM 2 [se destaca](https://ai.google/static/documents/palm2techreport.pdf) em tarefas como geração de código e tarefas matemática, classificação e respostas a perguntas, tradução e proficiência multilíngue e geração de linguagem natural melhor do que LLMs anteriores, incluindo PaLM. Ele pode realizar essas tarefas devido à forma como foi construído – reunindo dimensionamento otimizado para computação, uma mistura aprimorada de conjunto de dados e melhorias na arquitetura do modelo.\n",
    "\n",
    "O PaLM 2 é baseado na abordagem do Google para criar e implantar IA de forma responsável. Ele foi avaliado rigorosamente para analisar possíveis danos e vieses, recursos e usos posteriores em pesquisas e aplicações em produtos. Ele está sendo usado em outros modelos de última geração, como Med-PaLM 2 e Sec-PaLM, e está alimentando recursos e ferramentas generativas de IA no Google, como Bard e a API PaLM.\n",
    "\n",
    "O PaLM 2 é pré-treinado em uma ampla gama de dados de texto usando uma abordagem de aprendizado não supervisionado, sem nenhuma tarefa específica. Durante esse processo de pré-treinamento, o PaLM aprende a prever a próxima palavra em uma frase, considerando as palavras anteriores. Isso permite que o modelo gere texto coerente e fluente, semelhante à escrita humana.\n",
    "Esse tamanho grande permite aprender padrões e relacionamentos complexos na linguagem e gerar texto de alta qualidade para vários aplicativos. É por isso que modelos como o PaLM são chamados de \"modelos fundamentais\".\n",
    "\n",
    "A criação de um LLM requer grandes quantidades de dados, recursos de computação significativos e habilidades especializadas. Como os LLMs exigem um grande investimento para serem criados, eles visam casos de uso amplos e não específicos. Na Vertex AI, você pode personalizar um modelo de base para tarefas ou domínios de conhecimento mais específicos usando design de prompt e ajuste de modelo.\n",
    "\n",
    "### Vertex AI PaLM API\n",
    "A Vertex AI PaLM API, [lançada em 10 de maio de 2023](https://cloud.google.com/vertex-ai/docs/generative-ai/release-notes#may_10_2023), é desenvolvida com [PaLM 2]( https://ai.google/discover/palm2).\n",
    "\n",
    "### Usando a API Vertex AI PaLM\n",
    "\n",
    "Você pode interagir com a API Vertex AI PaLM usando os seguintes métodos:\n",
    "\n",
    "* Use a UI da [Vertex AI Studio](https://cloud.google.com/generative-ai-studio) para testes rápidos e geração de comandos.\n",
    "* Faça chamadas REST no Cloud Shell.\n",
    "* Use o Python SDK em um notebook Jupyter\n",
    "\n",
    "Este notebook se concentra no uso do Python SDK para chamar a Vertex AI PaLM API. Para obter mais informações sobre como usar a Vertex AI Studio sem escrever código, você pode explorar [Introdução às instruções da interface do usuário](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/getting-started/getting_started_ui.md)\n",
    "\n",
    "Para obter mais informações, confira a [documentação sobre suporte de IA generativa para Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objetivos\n",
    "\n",
    "Neste tutorial, você aprenderá como usar a API PaLM 2 com o Python SDK e explorar seus vários parâmetros.\n",
    "\n",
    "Ao final do notebook, você será capaz de entender várias nuances de parâmetros de modelo generativo como `top_k`, `top_p`, `temperature`,  e como cada parâmetro afeta os resultados.\n",
    "\n",
    "As etapas realizadas incluem:\n",
    "\n",
    "- Instalando o Python SDK\n",
    "- Usando a API Vertex AI PaLM\n",
    "   - Modelo de geração de texto com `text-bison@001`\n",
    "     - Entendendo os parâmetros do modelo (`temperature`, `max_output_tokens`, `top_k`, `top_p`)\n",
    "   - Modelo de chat com `chat-bison@001`\n",
    "   - Modelo de embeddings com `textembedding-gecko@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y6_3dTwV2fI"
   },
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Studio\n",
    "\n",
    "Saiba mais sobre possíveis custos envolvidos [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "e use a [Calculadora de preços](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ae098456471"
   },
   "source": [
    "### Segurança de dados\n",
    "**P: O Google usa dados de clientes para melhorar seus modelos de base?**\n",
    "R: Não, o Google não usa dados de clientes para melhorar os modelos de fundação. Os dados do cliente são usados apenas para gerar uma resposta do modelo.\n",
    "\n",
    "**P: Os funcionários do Google veem os dados que envio ao modelo?**\n",
    "R: Não, os funcionários do Google não têm acesso aos dados do cliente e todos os dados são criptografados em trânsito, em uso e em repouso.\n",
    "\n",
    "**P: O Google armazena algum dos dados do cliente que são enviados para o modelo?**\n",
    "R: Não, o Google não armazena dados de clientes. No entanto, o Google pode armazenar em cache temporariamente os dados do cliente durante a solicitação, como pipeline de ajuste de prompt e uso em batch.\n",
    "\n",
    "**P: O Google registra dados?**\n",
    "R: Não, o Google não loga os dados dos clientes. Os logs do lado do sistema ajudam o Google a garantir a integridade e a disponibilidade do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc389a25bf64"
   },
   "source": [
    "### IA Responsável\n",
    "LLMs podem traduzir linguagem, resumir texto, gerar escrita criativa, gerar código, chatbots e assistentes virtuais, além de complementar mecanismos de pesquisa e sistemas de recomendação. Ao mesmo tempo, como uma tecnologia em estágio inicial, seus recursos e usos em evolução podem criar aplicações incorretas, uso indevido e consequências não intencionais ou imprevistas. LLMs podem gerar resultados inesperados, incluindo texto ofensivo, insensível ou incorreto.\n",
    "\n",
    "Além disso, a incrível versatilidade dos LLMs também é o que torna difícil prever exatamente que tipos de saídas não intencionais ou imprevistas eles podem produzir. Dados esses riscos e complexidades, a API PaLM foi projetada com os [AI Principles do Google](https://ai.google/principles/) em mente. No entanto, é importante que os desenvolvedores entendam e testem seus modelos para implantá-los com segurança e responsabilidade. Para ajudar os desenvolvedores, a Vertex AI Studio possui filtragem de conteúdo integrada e a API PaLM possui pontuação de atributo de segurança para ajudar os clientes a testar os filtros de segurança do Google e definir limites de confiança adequados para seu caso de uso e negócios. Consulte a seção [Filtros e atributos de segurança](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_filters_and_attributes) para saber mais.\n",
    "\n",
    "Quando a API PaLM é integrada ao caso de uso e contexto exclusivos de um cliente, considerações adicionais de IA Responsável e [limitações PaLM](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#palm_limitations) precisam ser considerados. Incentivamos os clientes a usar *fairness*, interpretabilidade, privacidade e segurança [práticas recomendadas](https://ai.google/responsabilidades/responsible-ai-practices/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando os SDK da Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para reiniciar o kernel ou use o botão para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "# # Reinicia automaticamente o kernel após as instalações para que seu ambiente possa acessar os novos pacotes\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se você estiver usando o **Colab** para executar este notebook, descomente a célula abaixo e continue.\n",
    "* Se você estiver usando o **Vertex AI Workbench**, confira as instruções de configuração [aqui](../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GckO4EysV5BT"
   },
   "source": [
    "## Modelos disponíveis na Vertex AI PaLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDYqwDmTLgEy"
   },
   "source": [
    "A Vertex AI PaLM API permite que você teste, personalize e implemente instâncias de LLM do Google, chamado PaLM, para que você possa aproveitar os recursos do PaLM em seus aplicativos.\n",
    "\n",
    "### Nomenclatura dos modelos\n",
    "Os nomes dos modelos de fundação têm três componentes: caso de uso, tamanho do modelo e número da versão. A convenção de nomenclatura está no formato:\n",
    "`<caso de uso>-<tamanho do modelo>@<número da versão>`\n",
    "\n",
    "Por exemplo, text-bison@001 representa o modelo de texto Bison, versão 001.\n",
    "\n",
    "Os tamanhos dos modelos são os seguintes:\n",
    "- **Bison**: O melhor valor em termos de capacidade e custo.\n",
    "- **Gecko**: O modelo menor e mais barato para tarefas simples.\n",
    "\n",
    "### Modelos disponíveis\n",
    "\n",
    "Atualmente, a Vertex AI PaLM API oferece suporte a três modelos:\n",
    "\n",
    "* `text-bison@001` : Ajustado para seguir instruções de linguagem natural e é adequado para uma variedade de tarefas de linguagem.\n",
    "* `chat-bison@001`: Ajustado para casos de uso de conversas em vários turnos, como a construção de um chatbot.\n",
    "* `textembedding-gecko@001` : Retorna incorporações de modelo para entradas de texto.\n",
    "* `code-bison@001`: Ajustado para gerar códigos a partir de solicitações em linguagem natural (por exemplo, ele pode gerar um teste unitário para uma função).\n",
    "* `code-gecko@001`: Ajustado para facilitar cenários de complementação de código, levando em consideração o contexto que o código original foi escrito.\n",
    "* `codechat-bison@001`: Ajustado para interações em formato de chat e com entendimento de perguntas relacionadas a geração, complementação e explicação de código.\n",
    "\n",
    "\n",
    "Você pode encontrar mais informações sobre as propriedades desses [modelos fundamentais na documentação da Vertex AI Studio](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para realizar o processo adequado de inicialização da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[seu-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zjV4alsiVql"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from vertexai.language_models import TextGenerationModel, \\\n",
    "                                     TextEmbeddingModel, \\\n",
    "                                     ChatModel, \\\n",
    "                                     InputOutputTextPair, \\\n",
    "                                     CodeGenerationModel, \\\n",
    "                                     CodeChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mU6EZEhakVu"
   },
   "source": [
    "## Geração de texto com `text-bison@001`\n",
    "\n",
    "O modelo de geração de texto da API PaLM que você usará neste notebook é `text-bison@001`.\n",
    "Ele é ajustado para seguir instruções de linguagem natural e é adequado para uma variedade de tarefas de linguagem, como:\n",
    "\n",
    "- Classificação\n",
    "- Análise de sentimentos\n",
    "- Extração de entidades\n",
    "- Perguntas-respostas extrativas\n",
    "- Resumo\n",
    "- Reescrever o texto em um estilo diferente\n",
    "- Geração de cópia do anúncio\n",
    "- Ideação do conceito\n",
    "- Simplificação do conceito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5d006f3813"
   },
   "source": [
    "#### Design de prompts\n",
    "O design de prompts é o processo de criação de prompts que extraem a resposta desejada de um modelo de linguagem. O design de prompt é uma parte importante do uso de modelos de linguagem porque permite que não especialistas controlem a saída do modelo com pouco esforço.\n",
    "\n",
    "Ao elaborar cuidadosamente os prompts, você pode ajustar o modelo para gerar o resultado desejado. O design de prompt pode ser uma maneira eficiente de experimentar a adaptação de um LLM para um caso de uso específico. O processo iterativo de atualizar repetidamente os prompts e avaliar as respostas do modelo às vezes é chamado de engenharia de prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEAJ0ipmbndQ"
   },
   "source": [
    "#### Hello PaLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCgBDJvNRCF5"
   },
   "source": [
    "Crie o seu primeiro prompt e envie para o modelo de geração de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx_o455SRCF5"
   },
   "outputs": [],
   "source": [
    "prompt = \"O que é um large language model?\"\n",
    "\n",
    "response = generation_model.predict(prompt=prompt, max_output_tokens=1024)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste os seus prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quais são as 10 principais tendências na indústria de tecnologia?\n",
    "- Quais são os maiores desafios enfrentados pelo setor de saúde?\n",
    "- Quais são os últimos desenvolvimentos na indústria automotiva?\n",
    "- Quais são as maiores oportunidades no setor de varejo?\n",
    "- (Tente suas próprias ideias!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Quais são as top-10 tendências para computação?\" # try your own prompt\n",
    "\n",
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsglQtgDRCF5"
   },
   "source": [
    "#### Templates de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BP1BKWiRCF6"
   },
   "source": [
    "Os modelos de prompt são úteis se você encontrou uma boa maneira de estruturar seu prompt que pode ser reutilizado. Isso também pode ser útil para limitar a abertura de prompts de forma livre. Existem muitas maneiras de implementar modelos de prompt, e abaixo está apenas um exemplo usando f-strings de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2USfPyOuFhlB",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "minha_industria = \"esportes\" # altere esta variável para tentar com indústrias diferentes\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=\"Quais são as top-10 tendências para \" + minha_industria + \"?\",\n",
    "    max_output_tokens=1024\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m65AyLt8yvdB"
   },
   "source": [
    "### Parâmetros do modelo `text-bison@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQuZh6GT0Yn4"
   },
   "source": [
    "Você pode personalizar como a API do PaLM se comporta em resposta ao seu prompt usando os seguintes parâmetros para `text-bison@001`:\n",
    "\n",
    "  - `max_output_tokens`: define o número máximo de tokens na saída\n",
    "  - `top_k`: maior significa que irá amostrar de mais tokens possíveis\n",
    "  - `top_p`: maior significa que ele extrairá mais possíveis próximos tokens, com base na probabilidade cumulativa\n",
    "  - `temperature`: maior significa respostas mais \"criativas\"\n",
    "\n",
    "A seção abaixo cobre cada parâmetro e como usá-los."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krop7HXOIy8f",
    "tags": []
   },
   "source": [
    "#### O parâmetro `top_k` (intervalo: 0,0 - 40, padrão 40)\n",
    "\n",
    "##### O que é _top_k_?\n",
    "`top_k` muda como o modelo seleciona tokens para saída. Um `top_k` de 1 significa que o token selecionado é o mais provável entre todos os tokens no vocabulário do modelo (também chamado de decodificação *greedy* ou gananciosa). Em contraste, um `top_k` de 3 significa que o próximo token é selecionado entre os 3 tokens mais prováveis (usando a temperatura). Para cada etapa de seleção de token, os tokens `top_k` com as maiores probabilidades são amostrados. Em seguida, os tokens são filtrados com base em `top_p` com o token final selecionado usando amostragem de temperatura.\n",
    "\n",
    "##### Como _top_k_ afeta a resposta?\n",
    "\n",
    "Especifique um valor mais baixo para respostas menos aleatórias e um valor mais alto para respostas mais aleatórias.\n",
    "\n",
    "Para obter mais informações sobre o parâmetro `top_k` para modelos de texto, consulte a [documentação sobre parâmetros de modelo](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qK76o1hYO3ej"
   },
   "outputs": [],
   "source": [
    "prompt_top_k_example = \"Escreva um itinerário de 2 dias em Paris\"\n",
    "top_k_val = 1\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_k_example,\n",
    "    max_output_tokens=512, \n",
    "    temperature=0.9, \n",
    "    top_k=top_k_val,\n",
    ")\n",
    "\n",
    "print(f\"[top_k = {top_k_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hhu9d23vPGmK"
   },
   "outputs": [],
   "source": [
    "top_k_val = 40\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_k_example,\n",
    "    max_output_tokens=512,\n",
    "    temperature=0.9,\n",
    "    top_k=top_k_val\n",
    ")\n",
    "\n",
    "print(f\"[top_k = {top_k_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD3S2XsnHL50"
   },
   "source": [
    "#### O parâmetro `top_p` (intervalo: 0,0 - 1,0, padrão 0,95)\n",
    "\n",
    "##### O que é _top_p_?\n",
    "`top_p` controla como o modelo seleciona tokens para saída ajustando a distribuição de probabilidade da próxima palavra no texto gerado com base em um corte de probabilidade cumulativo. Especificamente, ele seleciona o menor conjunto de tokens cuja probabilidade cumulativa excede a probabilidade de corte dada _p_ e amostras desse conjunto uniformemente.\n",
    "\n",
    "Por exemplo, suponha que os tokens A, B e C tenham uma probabilidade de 0,3, 0,2 e 0,1 e o valor `top_p` seja 0,5. Nesse caso, o modelo selecionará A ou B como o próximo token (usando temperatura) e não considerará C, porque a probabilidade cumulativa de top_p é <= 0,5. Especifique um valor mais baixo para respostas menos aleatórias e um valor mais alto para respostas mais aleatórias.\n",
    "\n",
    "##### Como _top_p_ afeta a resposta?\n",
    "\n",
    "O parâmetro `top_p` é usado para controlar a diversidade do texto gerado. Um valor de parâmetro `top_p` mais alto resulta em saídas mais \"diversas\" e \"interessantes\", com o modelo podendo amostrar de um conjunto maior de possibilidades. Em contraste, um valor de parâmetro `top_p` mais baixo resultou em saídas mais previsíveis, com o modelo sendo restrito a um conjunto menor de tokens possíveis.\n",
    "\n",
    "##### Exemplo:\n",
    "\n",
    "`top_p = 0.1`:\n",
    "\n",
    "- O gato sentou-se no tapete.\n",
    "- O gato sentou no chão.\n",
    "\n",
    "`top_p = 0.9`:\n",
    "\n",
    "- O gato sentou-se no parapeito da janela, absorvendo os raios do sol.\n",
    "- O gato sentou-se na beirada da cama, observando os pássaros lá fora.\n",
    "\n",
    "Para obter mais informações sobre o parâmetro `top_p` para modelos de texto, consulte a [documentação sobre parâmetros de modelo](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAJiVYprNle1"
   },
   "outputs": [],
   "source": [
    "top_p_val = 0.0\n",
    "prompt_top_p_example = (\n",
    "    \"Crie uma campanha de marketing para jaquetas jeans que envolva elefantes azuis e abacates\"\n",
    ")\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_p_example, \n",
    "    temperature=0.9, \n",
    "    top_p=top_p_val,\n",
    "    max_output_tokens=512\n",
    ")\n",
    "\n",
    "print(f\"[top_p = {top_p_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm69kbcyN2gg"
   },
   "outputs": [],
   "source": [
    "top_p_val = 1.0\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_top_p_example, \n",
    "    temperature=0.9, \n",
    "    top_p=top_p_val,\n",
    "    max_output_tokens=512\n",
    ")\n",
    "\n",
    "print(f\"[top_p = {top_p_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JF76AKzaF2IP"
   },
   "source": [
    "#### O parâmetro `temperature` (intervalo: 0,0 - 1,0, padrão 0)\n",
    "\n",
    "##### O que é _temperature_?\n",
    "A temperatura é usada para amostragem durante a geração da resposta, que ocorre quando top_p e top_k são aplicados. A temperatura controla o grau de aleatoriedade na seleção do token.\n",
    "\n",
    "##### Como a _temperatura_ afeta a resposta?\n",
    "Temperaturas mais baixas são boas para prompts que exigem uma resposta mais determinística e menos aberta. Em comparação, temperaturas mais altas podem levar a resultados mais \"criativos\" ou diversos. Uma temperatura de `0` é determinística: a resposta de maior probabilidade é sempre selecionada. Para a maioria dos casos de uso, tente iniciar com uma temperatura de `0,2`.\n",
    "\n",
    "Um valor de temperatura mais alto resultará em uma saída mais exploratória, com maior probabilidade de gerar palavras ou frases raras ou incomuns. Por outro lado, um valor de temperatura mais baixo resultará em uma saída mais conservadora, com maior probabilidade de gerar palavras ou frases comuns ou esperadas.\n",
    "\n",
    "##### Exemplo:\n",
    "\n",
    "Por exemplo,\n",
    "\n",
    "`temperatura = 0,0`:\n",
    "\n",
    "* _O gato sentou no sofá, observando os pássaros lá fora._\n",
    "* _O gato sentou-se no parapeito da janela, tomando sol._\n",
    "\n",
    "`temperatura = 0,9`:\n",
    "\n",
    "* _O gato sentou-se na lua, miando para as estrelas._\n",
    "* _O gato sentou-se no cheeseburger, ronronando de alegria._\n",
    "\n",
    "**Observação**: é importante observar que, embora o parâmetro de temperatura possa ajudar a gerar um texto mais diversificado e interessante, ele também pode aumentar a probabilidade de gerar texto sem sentido ou inapropriado (ou seja, alucinações). Portanto, é importante usá-lo com cuidado e levando em consideração o resultado desejado.\n",
    "\n",
    "Para obter mais informações sobre o parâmetro `temperatura` para modelos de texto, consulte a [documentação sobre parâmetros de modelo](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMEz2P18SBW-"
   },
   "source": [
    "Se você executar a célula a seguir várias vezes, ela sempre retornará a mesma resposta, pois `temperature=0` é determinístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxPM1A8uR81l"
   },
   "outputs": [],
   "source": [
    "temp_val = 0.0\n",
    "prompt_temperature = \"Complete a frase: Enquanto me preparava para pendurar o quadro, peguei na caixa de ferramentas um:\"\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_temperature,\n",
    "    temperature=temp_val,\n",
    "    max_output_tokens=512\n",
    ")\n",
    "\n",
    "print(f\"[temperature = {temp_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B91rIFiSBW_"
   },
   "source": [
    "Se você executar a célula a seguir várias vezes, ela pode retornar respostas diferentes, pois valores de temperatura mais altos podem levar a resultados \n",
    "mais diversos, mesmo que o prompt seja o mesmo da célula acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2mKiDB5SBW_"
   },
   "outputs": [],
   "source": [
    "temp_val = 1.0\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt_temperature,\n",
    "    temperature=temp_val,\n",
    "    max_output_tokens=512\n",
    ")\n",
    "\n",
    "print(f\"[temperature = {temp_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRYTOKQpGpfP"
   },
   "source": [
    "#### O parâmetro `max_output_tokens` (intervalo: 1 - 1024, padrão 128)\n",
    "\n",
    "##### Tokens\n",
    "Um único token pode ser menor que uma palavra. Por exemplo, um token tem aproximadamente quatro caracteres. Portanto, 100 tokens correspondem a cerca de 60 a 80 palavras. É essencial estar ciente dos tamanhos dos tokens, pois os modelos têm um limite de tokens de entrada e saída.\n",
    "\n",
    "##### O que é _max_output_tokens_?\n",
    "`max_output_tokens` é o número máximo de tokens que podem ser gerados na resposta.\n",
    "\n",
    "##### Como _max_output_tokens_ afeta a resposta?\n",
    "\n",
    "Especifique um valor mais baixo para respostas mais curtas e um valor mais alto para respostas mais longas. Um token pode ser menor que uma palavra. Um token tem aproximadamente quatro caracteres. 100 tokens correspondem a aproximadamente 60-80 palavras.\n",
    "\n",
    "Para obter mais informações sobre o parâmetro `max_output_tokens` para modelos de texto, consulte a [documentação sobre parâmetros de modelo](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUjANX_mNLuI"
   },
   "outputs": [],
   "source": [
    "max_output_tokens_val = 5\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=\"Liste 10 formas que a Generative AI pode auxiliar na experiência de compras online dos usuários\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    "\n",
    "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DiHeUgSNX1m"
   },
   "outputs": [],
   "source": [
    "max_output_tokens_val = 1024\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=\"Liste 10 formas que a Generative AI pode auxiliar na experiência de compras online dos usuários\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    "\n",
    "print(f\"[max_output_tokens = {max_output_tokens_val}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar a leitura, você também pode renderizar o Markdown (respostas que vem entre \\*\\*) no Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ada40abecc"
   },
   "source": [
    "## O modelo de chat `chat-bison@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1923b5583a2d"
   },
   "source": [
    "O modelo `chat-bison@001` permite que você tenha uma conversa de forma livre em várias interações (ou turnos de conversa). O aplicativo rastreia o que foi dito anteriormente na conversa. Dessa forma, se você espera usar conversas em seu aplicativo, use o modelo `chat-bison@001` porque ele foi ajustado para casos de uso de conversas de vários turnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1325438b9188"
   },
   "outputs": [],
   "source": [
    "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
    "\n",
    "chat = chat_model.start_chat()\n",
    "\n",
    "print(chat.send_message(\n",
    "        \"\"\"Olá! Escreva um texto em poucos parágrafos curtos para um artigo científico que eu preciso escrever sobre os impactos de Generative AI para a sociedade?\"\"\"\n",
    "    ).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef81f564bed1"
   },
   "source": [
    "As shown below, the model should respond based on what was previously said in the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30ab5afac7dc"
   },
   "outputs": [],
   "source": [
    "print(chat.send_message(\n",
    "        \"\"\"Quais seriam três possíveis títulos interessantes para este artigo?\"\"\"\n",
    "    ).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe15a6b083e5"
   },
   "source": [
    "### Uso avançado do modelo de chats com a SDK da Vertex AI\n",
    "Você também pode fornecer um `context` (contexto) e `exemples` (exemplos) para o modelo. O modelo responderá com base no contexto e nos exemplos fornecidos. Você também pode usar `temperature`, `max_output_tokens`, `top_p` e `top_k`. Esses parâmetros devem ser usados quando você inicia seu chat com `chat_model.start_chat()`.\n",
    "\n",
    "Para obter mais informações sobre modelos de chat, consulte a [documentação sobre os parâmetros do modelo de chat](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#chat_model_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4047b525961e"
   },
   "outputs": [],
   "source": [
    "chat = chat_model.start_chat(\n",
    "    context=\"Meu nome é José. Você é meu assistente pessoal. Meus filmes preferidos são Senhor dos Anéis e o Hobbit.\",\n",
    "    examples=[\n",
    "        InputOutputTextPair(\n",
    "            input_text=\"Para quem você trabalha?\",\n",
    "            output_text=\"Eu trabalho para José.\"\n",
    "        ),\n",
    "        InputOutputTextPair(\n",
    "            input_text=\"Do que eu gosto?\",\n",
    "            output_text=\"José gosta de assistir filmes.\"\n",
    "        ),\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=200,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "resposta = chat.send_message(\"Os meus filmes favoritos são baseados em livros?\")\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "685184ee4159"
   },
   "outputs": [],
   "source": [
    "resposta = chat.send_message(\"Quando eles foram publicados?\")\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67b6eef99f56"
   },
   "source": [
    "## Modelo de embeddings com `textembedding-gecko@001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64a58515878c"
   },
   "source": [
    "Os embeddings de texto são uma representação vetorial densa, geralmente de baixa dimensão, de uma parte do conteúdo, de modo que, se duas partes do conteúdo forem semanticamente semelhantes, seus respectivos embeddings estarão localizadas próximas uma da outra no espaço vetorial de embeddings. Essa representação pode ser usada para resolver tarefas comuns de NLP, como:\n",
    "\n",
    "* **Pesquisa semântica**: busca classificada por similaridade semântica.\n",
    "* **Recomendação**: retorna itens com atributos de texto semelhantes ao texto fornecido.\n",
    "* **Classificação**: retorna a classe de itens cujos atributos de texto são semelhantes ao texto fornecido.\n",
    "* **Clustering**: itens de cluster cujos atributos de texto são semelhantes ao texto fornecido.\n",
    "* **Detecção de outliers**: retorna itens nos quais os atributos de texto são menos relacionados ao texto fornecido.\n",
    "\n",
    "Consulte a [documentação do modelo de incorporação de texto](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) para obter mais informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0f175d5b02a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "\n",
    "embeddings = embedding_model.get_embeddings([\"O que é a vida?\"])\n",
    "\n",
    "for embedding in embeddings:\n",
    "    vector = embedding.values\n",
    "    print(f\"Length = {len(vector)}\")\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "601bd7e7ef1d"
   },
   "source": [
    "#### Embeddings e DataFrames Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e69d2ba877f"
   },
   "source": [
    "Caso seu texto esteja armazenado em um DataFrame, você pode criar uma nova coluna com os embeddings com o exemplo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d99f08bab254"
   },
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"eu realmente curti o filme de ontem a noite\",\n",
    "    \"assistimos muitas cenas acrobáticas ontem\",\n",
    "    \"eu me diverti escrevendo o meu primeiro programa em Python\",\n",
    "    \"uma tremenda sensação de alívio ao finalmente fazer meus scripts Nodejs rodarem sem erros!\",\n",
    "    \"Oh Romeu, Romeu, porque es tu Romeu?\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(text, columns=[\"text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fabd92d8ddb6"
   },
   "source": [
    "Crie uma nova coluna, `embeddings`, usando a função [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) em pandas com o modelo de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "096cbf3f2698"
   },
   "outputs": [],
   "source": [
    "df[\"embeddings\"] = df.apply(lambda x: embedding_model.get_embeddings([x.text])[0].values, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69ebe1a6514d"
   },
   "source": [
    "#### Analisando a similaridade de exemplos de texto com `similaridade de cosseno`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04d0f13acedb"
   },
   "source": [
    "Ao converter texto em embeddings, você pode calcular pontuações de similaridade. Há muitas maneiras de calcular pontuações de similaridade, e uma técnica comum é usar [similaridade de cosseno](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "\n",
    "No exemplo acima, duas das sentenças na coluna `text` referem-se a gostar de um _filme_, e as outras duas se relacionam a gostar de _programar_. As pontuações de similaridade de cosseno devem ser mais altas (próximas de 1,0) ao fazer comparações pareadas entre sentenças semanticamente relacionadas, e as pontuações devem ser menores entre sentenças semanticamente diferentes.\n",
    "\n",
    "A saída do DataFrame abaixo mostra as pontuações de similaridade de cosseno resultantes entre os embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc499dae438a"
   },
   "outputs": [],
   "source": [
    "cos_sim_array = cosine_similarity(list(df.embeddings.values))\n",
    "\n",
    "# display as DataFrame\n",
    "df = pd.DataFrame(cos_sim_array, index=text, columns=text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97a5e2e32df5"
   },
   "source": [
    "Para tornar isso mais fácil de entender, você pode usar um mapa de calor. Naturalmente, o texto é mais semelhante quando são idênticos (pontuação de 1,0). As próximas pontuações mais altas são quando as sentenças são semanticamente semelhantes. As pontuações mais baixas são quando as sentenças são bastante diferentes em significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "560ea2a11535"
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df, annot=True, cmap=\"crest\")\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xticklabels(text, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de código com o modelo `code-bison@001`\n",
    "\n",
    "O modelo de geração de código (Codey) da PaLM API que você usará neste notebook é o `code-bison@001`. Ele é ajustado para seguir instruções de linguagem natural para gerar o código e é flexível para ajudar com diferentes tarefas de codificação, como:\n",
    "\n",
    "- escrever funções\n",
    "- criar classes\n",
    "- criar páginas\n",
    "- ajudar com a criação de testes unitários\n",
    "- criar docstrings\n",
    "- traduções de código entre linguagens diferentes\n",
    "- e muitos outros casos de uso.\n",
    "\n",
    "Atualmente suporta os seguintes linguagens:\n",
    "\n",
    "- C++\n",
    "- C#\n",
    "- Go\n",
    "- GoogleSQL\n",
    "- Java\n",
    "- JavaScript\n",
    "- Kotlin\n",
    "- PHP\n",
    "- Python\n",
    "- Ruby\n",
    "- Rust\n",
    "- Scala\n",
    "- Swift\n",
    "- TypeScript\n",
    "\n",
    "Você pode encontrar mais detalhes [aqui](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros do modelo `code-bison@001`\n",
    "\n",
    "Você pode customizar como a geração de código do `code-bison` funcionará utilizando os seguintes parâmetros:\n",
    "\n",
    "- `prefix`: Representa o início de um trecho de código a ser completado ou de uma explicação em linguagem natural que será utilizada na criação do código\n",
    "- `temperature`: quanto maior o valor de temperatura, mais \"criativa\" será a geração de código. Valores possíveis no intervalo: (0.0 - 1.0, default 0).\n",
    "- `max_output_tokens`: Define o número máximo de tokens na resposta. Valores possíveis no intervalo: (1 - 2048, default 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o `code-bison@001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generation_model = CodeGenerationModel.from_pretrained(\"code-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World com o `code-bison@001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Escreva uma função em python que recebe dois valores de entrada: uma lista ordenada e uma variável chamada 'chave' como argumentos e realiza uma busca binária pelo valor 'chave' dentro da lista\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste os seus próprios prompts\n",
    "\n",
    "Alguns exemplos:\n",
    "\n",
    "- Escreva um programa em Go para extrair endereços IP de um arquivo de texto\n",
    "- Crie um código em Java para extrair pincodes de endereços\n",
    "- Escreva uma função em SQL padrão que remova todos os caracteres não alfabéticos de uma string e a encoda em UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\"Escreva uma função em python chamada \"calculo_similaridade_cosseno\" e três \\\n",
    "            testes unitários que recebam como dois argumentos \"vetor1\" e \"vetor2\". \\\n",
    "            A função usará a função dot do numpy para calcular o dot produt entre os dois vetores. \\n\n",
    "          \"\"\"\n",
    "\n",
    "response = code_generation_model.predict(prefix=prefix, temperature=0.1, max_output_tokens=1024)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementando códigos com o modelo `code-gecko@001`\n",
    "\n",
    "Para realizar a complementação de uma parte de código, utilizamos o modelo fundacional `code-gecko`.\n",
    "\n",
    "O modelo `code-gecko` possui alguns parâmetros adicionais que podemos utilizar para refinar nossos prompts:\n",
    "\n",
    "- prefix: Um parâmetro **obrigatório** que representa o início de um código ou de um texto em linguagem natural que define um código.\n",
    "- suffix: Um parâmetro **opcional** que represença o final de um código (o modelo então tentará criar o código que falta entro o prefixo e o sufixo).\n",
    "- temperature: Um parâmetro **opcional** que define o grau de aleatoriedade na escolha de tokens na geração da resposta (similar à temperatura para os demais modelos). Valores possíveis no intervalo: (0.0 - 1.0, default 0)\n",
    "- maxOutputTokens: Um parâmetro **opcional** que define o número máximo de tokens na resposta do modelo. Valores possívels no intervalo: (1 - 64, default 64)\n",
    "- stopSequences: Um parâmetro **opcional** que define que identifica um padrão de string que o modelo usará para parar a geração da resposta quando o valor for encontrado na resposta. **Importante:** esse valor é *case-sensitive*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o modelo `code-gecko`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_completion_model = CodeGenerationModel.from_pretrained(\"code-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"\n",
    "          def find_x_in_string(string_s, x):\n",
    "          \n",
    "         \"\"\"\n",
    "\n",
    "response = code_completion_model.predict(prefix=prefix, temperature=0)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando códigos em uma interação via chat com o modelo `codechat-bison@001`\n",
    "\n",
    "O modelo `codechat-bison` permite uma interação em formato de chat e com suporte a geração de código. O backend do serviço mantem o histórico das informações passadas e, como em uma conversa com histórico, o contexto anterior é sempre levado em consideração nas novas solicitações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o modelo `codechat-bison@001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciando a interação via chat com o `codechat-bison@001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chat = code_chat_model.start_chat()\n",
    "\n",
    "code_chat.send_message(\n",
    "    \"Escreva uma função que calcule o menor valor entre dois números\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, sem precisar reenviar a solicitação novamente, podemos seguir na conversação fazendo referência a solicitação anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chat.send_message(\n",
    "    \"Você pode explicar o código linha e linha e em formato de bullets?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode também solicitar ao modelo que faça modificações ou alterações no código gerado para melhor aderir ao seu cenário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chat.send_message(\n",
    "    \"Você pode reescrever esse código, incluindo o cenário onde os dois números sejam iguais e, neste caso, retornando a string 'os dois números sào iguais'?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_palm_api.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
