{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# GenAI para segmentação de clientes com BigQuery\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Execute no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      Veja no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Execute no Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Visão geral\n",
    "\n",
    "Segmentação de clientes é o processo de agrupar clientes base em características similares para ajudar no engajamento de uma marca ou de um produto com sua audiência. Isso pode ser feito usando uma variedade de fatores, como dados demográficos e comportamentais. Mecanismos de *machine learning* podem ser usados para automatizar o processo de segmentação de clientes, tornando-o mais eficiente e eficaz.\n",
    "\n",
    "### BigQuery e o BigQueryML\n",
    "\n",
    "O [BigQuery](https://cloud.google.com/bigquery/). é o serviço de base de dados gerenciado, serverless e escalável disponível na Google Cloud. O BigQuery permite gerenciar e analisar dados com recursos integrados, como *machine learning*, análise geoespacial e *business intelligence*. Para maiores detalhes, visite a [documentação oficial](https://cloud.google.com/bigquery/docs) do BigQuery.\n",
    "\n",
    "O [BigQueryML](https://cloud.google.com/bigquery/docs/bqml-introduction?hl=pt-br) é o componente do BigQuery para criar e executar modelos de *machine learning* usando consultas com código GoogleSQL, permitindo profissionais de SQL criem modelos usando habilidades e ferramentas de SQL já existentes e sem a necessidade de movimentação de dados para fora do banco de dados. Para maiores detalhes, visite a [documentação oficial](https://cloud.google.com/bigquery/docs/bqml-introduction?hl=pt-br) do BigQueryML.\n",
    "\n",
    "\n",
    "### Vertex AI PaLM API\n",
    "A Vertex AI PaLM API, [lançada em 10 de maio de 2023](https://cloud.google.com/vertex-ai/docs/generative-ai/release-notes#may_10_2023), é desenvolvida com [PaLM 2]( https://ai.google/discover/palm2).\n",
    "\n",
    "### Usando a API Vertex AI PaLM\n",
    "\n",
    "Você pode interagir com a API Vertex AI PaLM usando os seguintes métodos:\n",
    "\n",
    "* Use a UI da [Vertex AI Studio](https://cloud.google.com/generative-ai-studio) para testes rápidos e geração de comandos.\n",
    "* Faça chamadas REST no Cloud Shell.\n",
    "* Use o Python SDK em um notebook Jupyter\n",
    "\n",
    "Este notebook se concentra no uso do Python SDK para chamar a Vertex AI PaLM API. Para obter mais informações sobre como usar o Vertex AI Studio sem escrever código, você pode explorar [Introdução às instruções da interface do usuário](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/getting-started/getting_started_ui.md)\n",
    "\n",
    "Para obter mais informações, confira a [documentação sobre suporte de IA generativa para Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objetivos\n",
    "\n",
    "Neste tutorial, você irá utilizar os serviços de Google Cloud para:\n",
    "\n",
    "* Explorar dados do dataset público `bigquery-public-data.thelook_ecommerce.order_items`\n",
    "* Treinar um modelo de segmentação utilizando o algoritmo `kmeans` no BigQueryML\n",
    "* Realizar inferências no modelo treinado\n",
    "* Utilizar as API do GenAI Studio para a geração de campanhas utilizando as informações do modelo do BigQueryML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y6_3dTwV2fI"
   },
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Studio\n",
    "* BigQuery\n",
    "* BigQueryML\n",
    "\n",
    "Saiba mais sobre possíveis custos envolvidos [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "[preços do BigQuery](https://cloud.google.com/bigquery/pricing),\n",
    "e use a [Calculadora de preços](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ae098456471"
   },
   "source": [
    "### Segurança de dados\n",
    "**P: O Google usa dados de clientes para melhorar seus modelos de base?**\n",
    "R: Não, o Google não usa dados de clientes para melhorar os modelos de fundação. Os dados do cliente são usados apenas para gerar uma resposta do modelo.\n",
    "\n",
    "**P: Os funcionários do Google veem os dados que envio ao modelo?**\n",
    "R: Não, os funcionários do Google não têm acesso aos dados do cliente e todos os dados são criptografados em trânsito, em uso e em repouso.\n",
    "\n",
    "**P: O Google armazena algum dos dados do cliente que são enviados para o modelo?**\n",
    "R: Não, o Google não armazena dados de clientes. No entanto, o Google pode armazenar em cache temporariamente os dados do cliente durante a solicitação, como pipeline de ajuste de prompt e uso em batch.\n",
    "\n",
    "**P: O Google registra dados?**\n",
    "R: Não, o Google não loga os dados dos clientes. Os logs do lado do sistema ajudam o Google a garantir a integridade e a disponibilidade do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc389a25bf64"
   },
   "source": [
    "### IA Responsável\n",
    "LLMs podem traduzir linguagem, resumir texto, gerar escrita criativa, gerar código, chatbots e assistentes virtuais, além de complementar mecanismos de pesquisa e sistemas de recomendação. Ao mesmo tempo, como uma tecnologia em estágio inicial, seus recursos e usos em evolução podem criar aplicações incorretas, uso indevido e consequências não intencionais ou imprevistas. LLMs podem gerar resultados inesperados, incluindo texto ofensivo, insensível ou incorreto.\n",
    "\n",
    "Além disso, a incrível versatilidade dos LLMs também é o que torna difícil prever exatamente que tipos de saídas não intencionais ou imprevistas eles podem produzir. Dados esses riscos e complexidades, a API PaLM foi projetada com os [AI Principles do Google](https://ai.google/principles/) em mente. No entanto, é importante que os desenvolvedores entendam e testem seus modelos para implantá-los com segurança e responsabilidade. Para ajudar os desenvolvedores, o Vertex AI Studio possui filtragem de conteúdo integrada e a API PaLM possui pontuação de atributo de segurança para ajudar os clientes a testar os filtros de segurança do Google e definir limites de confiança adequados para seu caso de uso e negócios. Consulte a seção [Filtros e atributos de segurança](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_filters_and_attributes) para saber mais.\n",
    "\n",
    "Quando a API PaLM é integrada ao caso de uso e contexto exclusivos de um cliente, considerações adicionais de IA Responsável e [limitações PaLM](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai #palm_limitations) precisam ser considerados. Incentivamos os clientes a usar *fairness*, interpretabilidade, privacidade e segurança [práticas recomendadas](https://ai.google/responsabilidades/responsible-ai-practices/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Instalando os SDK da Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para reiniciar o kernel ou use o botão para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "# # Reinicia automaticamente o kernel após as instalações para que seu ambiente possa acessar os novos pacotes\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se você estiver usando o **Colab** para executar este notebook, descomente a célula abaixo e continue.\n",
    "* Se você estiver usando o **Vertex AI Workbench**, confira as instruções de configuração [aqui](../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para realizar o processo adequado de inicialização da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[seu-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zjV4alsiVql"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "from google.cloud import bigquery\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mU6EZEhakVu"
   },
   "source": [
    "## Geração de texto com `text-bison@001`\n",
    "\n",
    "O modelo de geração de texto da API PaLM que você usará neste notebook é `text-bison@001`. Já deixaremos seu objeto instanciado neste Jupyter notebook para uso futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5d006f3813"
   },
   "source": [
    "#### Entendendo os dados disponíveis\n",
    "Primeiramente, vamos entender os dados que utilizaremos. Vamos criar a *query* no dataset a ser utilizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "  user_id,\n",
    "  order_id,\n",
    "  sale_price,\n",
    "  created_at as order_created_date\n",
    "FROM `bigquery-public-data.thelook_ecommerce.order_items`\n",
    "WHERE created_at BETWEEN CAST('2020-01-01 00:00:00' AS TIMESTAMP)\n",
    "AND CAST('2023-01-01 00:00:00' AS TIMESTAMP)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora esta *query* será executada. Isso inclui a definição de um `client` do BigQuery, usando sua SDK e pegar o resultado da *query* e armazenar em um *dataframe* Pandas para uma melhor visualização aqui no ambiente de Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"lucianomartins-demos-345000\"  # @param {type:\"string\"}\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "df = client.query(query).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é criar um modelo do BigQueryML, utilizando o algoritmo `kmeans`.\n",
    "\n",
    "Para isso, será utilizada a cláusula SQL `CREATE MODEL`, parte do BigQueryML, para iniciar um treinamento gerenciado de um modelo de *machine learning*. O *dataset de treinamento* será gerado para a partir de um select, em tempo de execução, e após o treinamento do modelo o BigQueryML irá armazenar seu modelo dentro do projeto do BigQuery em utilização.\n",
    "\n",
    "Para maiores detalhes, consulte a documentação sobre [Criando modelos de machine learning no BigQuery ML](https://cloud.google.com/bigquery/docs/create-machine-learning-model?hl=pt-br).\n",
    "\n",
    "Primeiro vamos criar um dataset no BigQuery para armazenar o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config list project\n",
    "project_id = project_id[1].split('=')[1].strip()\n",
    "parent = f'projects/' + project_id\n",
    "\n",
    "dataset_id = f\"{project_id}.segmentacao_de_clientes\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print(\"Dataset criado com sucesso {}.{}\".format(project_id, dataset.dataset_id))\n",
    "except:\n",
    "    print(f\"O dataset {dataset_id} já existe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora criaremos a *query* de criação do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"modelo_kmeans\"\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{dataset_id}.{model_name}`\n",
    "OPTIONS (\n",
    "  MODEL_TYPE = \"KMEANS\",\n",
    "  NUM_CLUSTERS = 5,\n",
    "  KMEANS_INIT_METHOD = \"KMEANS++\",\n",
    "  STANDARDIZE_FEATURES = TRUE )\n",
    "AS (\n",
    "SELECT * EXCEPT (user_id)\n",
    "FROM (\n",
    "  SELECT user_id,\n",
    "    DATE_DIFF(CURRENT_DATE(), CAST(MAX(order_created_date) as DATE), day) AS days_since_order, -- RECENCY\n",
    "    COUNT(order_id) AS count_orders, -- FREQUENCY\n",
    "    AVG(sale_price) AS avg_spend -- MONETARY\n",
    "  FROM (\n",
    "    SELECT user_id,\n",
    "      order_id,\n",
    "      sale_price,\n",
    "      created_at as order_created_date\n",
    "    FROM `bigquery-public-data.thelook_ecommerce.order_items`\n",
    "    WHERE created_at BETWEEN CAST('2020-01-01 00:00:00' AS TIMESTAMP)\n",
    "    AND CAST('2023-01-01 00:00:00' AS TIMESTAMP)\n",
    "  )\n",
    "  GROUP BY user_id, order_id\n",
    " )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E realizaremos o treinamento do modelo. Este processo deve demorar mais ou menos 2 minutos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig()\n",
    "client_result = client.query(query, job_config=job_config)\n",
    "job_id = client_result.job_id\n",
    "df = client_result.result().to_arrow().to_pandas()\n",
    "print(f\"Job de treinamento finalizado com sucesso: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEAJ0ipmbndQ"
   },
   "source": [
    "#### Avaliando o modelo treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCgBDJvNRCF5"
   },
   "source": [
    "Como em todo processo de treinamento de modelos customizados, é importante realizarmos avaliações do modelo. Isso nos dará métricas numéricas e/ou qualitativas quanto ao resultado do treinamento.\n",
    "\n",
    "Neste cenário, contaremos com a ajuda com BigQueryML para realizar a avaliação utilizando a cláusula SQL `ML.EVALUATE` e contaremos com as métricas [davies bouldin index](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index) e [mean squared  error](https://en.wikipedia.org/wiki/Mean_squared_error) - De forma abstrata, quanto mais próximos de zero estes valores, melhor.\n",
    "\n",
    "Primeiro criaremos a query de avaliação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM ML.EVALUATE(MODEL `{dataset_id}.{model_name}`)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E então executaremos a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.query(query).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observando os centroids (ou agrupamentos criados pelo modelo `kmeans`)\n",
    "\n",
    "Agora podemos observar um sumário de como o modelo agrupou os clientes em cinco diferente grupos. Primeiro vamos criar a query que fará essa análise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "  CONCAT('cluster ', CAST(centroid_id as STRING)) as centroid,\n",
    "  avg_spend as average_spend,\n",
    "  count_orders as count_of_orders,\n",
    "  days_since_order\n",
    "FROM (\n",
    "  SELECT centroid_id, feature, ROUND(numerical_value, 2) as value\n",
    "  FROM ML.CENTROIDS(MODEL `{dataset_id}.{model_name}`)\n",
    ")\n",
    "PIVOT (\n",
    "  SUM(value)\n",
    "  FOR feature IN ('avg_spend',  'count_orders', 'days_since_order')\n",
    ")\n",
    "ORDER BY centroid_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora podemos observar os grupos no resultado da execução da query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.query(query).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriquecendo a segmentação com Generative AI\n",
    "\n",
    "Até neste ponto, foram realizados:\n",
    "\n",
    "* a análise inicial dos dados\n",
    "* o treinamento de um modelo de machine learning utilizando o BigQueryML\n",
    "* a avaliação do modelo treinado\n",
    "* a visualização dos grupos de clientes criados pelo modelo\n",
    "\n",
    "Porém, ainda são atividades no mundo técnico dos dados. Como isso se torna mais próximo de um uso real de negócios?\n",
    "\n",
    "Para isso, utilizaremos as informações dos grupos criados e as API do Vertex GenAI Studio para gerar conteúdos a partir do conhecimento que foi gerado aqui.\n",
    "\n",
    "Primeiro vamos pegar as informações geradas na *query* dos grupos e salvar em uma variável chamada `clusters_info` de uma forma mais textual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.query(query).to_dataframe()\n",
    "df.to_string(header=False, index=False)\n",
    "\n",
    "cluster_info = []\n",
    "for i, row in df.iterrows():\n",
    "  cluster_info.append(\"{0}, gasto médio ${2}, número de pedidos por pessoa {1}, dias desde a última compra {3}\"\n",
    "    .format(row[\"centroid\"], row[\"count_of_orders\"], row[\"average_spend\"], row[\"days_since_order\"]) )\n",
    "\n",
    "print(str.join(\"\\n\", cluster_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, com a ajuda da Vertex GenAI Studio, utilizando as informações descritivas dos grupos, iremos criar uma explicação mais textual de cada grupo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = str.join(\"\\n\", cluster_info)\n",
    "\n",
    "prompt = (f\"\"\"\n",
    "Você é um estrategista de marca criativo. Dados os seguintes clusters, crie uma persona de marca criativa, um título cativante e a próxima ação de marketing, explicada passo a passo.\n",
    "\n",
    "Clusters:\n",
    "{clusters}\n",
    "\n",
    "Para cada cluster:\n",
    "* Título:\n",
    "* Persona:\n",
    "* Próximo passo de marketing:\n",
    "\"\"\")\n",
    "\n",
    "print(\n",
    "        generation_model.predict(\n",
    "            prompt,\n",
    "            max_output_tokens=1024,\n",
    "            temperature=0.55,\n",
    "            top_p=0.8,\n",
    "            top_k=40,\n",
    "        ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o mais interessante com esta abordagem é que a geração pode ser calibrada a partir do *prompt* que você encaminha ao modelo generativo. Um exemplo mais ainda criativo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = str.join('\\n', cluster_info)\n",
    "\n",
    "prompt = (f\"\"\"\n",
    "Você é um estrategista criativo, analise os seguintes clusters e crie \\\n",
    "persona de marca criativa para cada um que inclui os detalhes de qual personagem de Romeu e Julieta é \\\n",
    "provável que seja o favorito do grupo, um resumo de como isso se relaciona com seu comportamento de compra, \\\n",
    "e um título de e-mail espirituoso para campanha de marketing direcionada ao seu grupo.\n",
    "\n",
    "Clusters:\n",
    "{cluster_info}\n",
    "\"\"\")\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt,\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.45,\n",
    "        top_p=0.8, top_k=40,\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_palm_api.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
