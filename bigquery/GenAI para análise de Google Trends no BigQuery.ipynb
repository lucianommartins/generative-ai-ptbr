{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# GenAI para análise de Google Trends no BigQuery\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Execute no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      Veja no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Execute no Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Visão geral\n",
    "\n",
    "O Google Trends é a ferramenta do Google que auxilia usuários e desenvolvedores a terem uma visão do que acontece no mundo através dos padrões de busca, incluindo os termos buscados, no google.com. Neste laboratório vamos utilizar os dados do Google Trends, através do BigQuery, para obter informações sobre algumas dessas tendêncas de buscas.\n",
    "\n",
    "### BigQuery\n",
    "\n",
    "O [BigQuery](https://cloud.google.com/bigquery/). é o serviço de base de dados gerenciado, serverless e escalável disponível na Google Cloud. O BigQuery permite gerenciar e analisar dados com recursos integrados, como *machine learning*, análise geoespacial e *business intelligence*. Para maiores detalhes, visite a [documentação oficial](https://cloud.google.com/bigquery/docs) do BigQuery.\n",
    "\n",
    "### Vertex AI PaLM API\n",
    "A Vertex AI PaLM API, [lançada em 10 de maio de 2023](https://cloud.google.com/vertex-ai/docs/generative-ai/release-notes#may_10_2023), é desenvolvida com [PaLM 2]( https://ai.google/discover/palm2).\n",
    "\n",
    "### Usando a API Vertex AI PaLM\n",
    "\n",
    "Você pode interagir com a API Vertex AI PaLM usando os seguintes métodos:\n",
    "\n",
    "* Use a UI da [Generative AI Studio](https://cloud.google.com/generative-ai-studio) para testes rápidos e geração de comandos.\n",
    "* Faça chamadas REST no Cloud Shell.\n",
    "* Use o Python SDK em um notebook Jupyter\n",
    "\n",
    "Este notebook se concentra no uso do Python SDK para chamar a Vertex AI PaLM API. Para obter mais informações sobre como usar o Generative AI Studio sem escrever código, você pode explorar [Introdução às instruções da interface do usuário](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/getting-started/getting_started_ui.md)\n",
    "\n",
    "Para obter mais informações, confira a [documentação sobre suporte de IA generativa para Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objetivos\n",
    "\n",
    "Neste tutorial, você irá utilizar os serviços de Google Cloud para:\n",
    "\n",
    "* Explorar dados do dataset público `bigquery-public-data.google_trends`\n",
    "* Utilizar as API do GenAI Studio para a extração de informações contextuais sobre as tendências de busca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y6_3dTwV2fI"
   },
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "* BigQuery\n",
    "\n",
    "Saiba mais sobre possíveis custos envolvidos [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "[preços do BigQuery](https://cloud.google.com/bigquery/pricing),\n",
    "e use a [Calculadora de preços](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ae098456471"
   },
   "source": [
    "### Segurança de dados\n",
    "**P: O Google usa dados de clientes para melhorar seus modelos de base?**\n",
    "R: Não, o Google não usa dados de clientes para melhorar os modelos de fundação. Os dados do cliente são usados apenas para gerar uma resposta do modelo.\n",
    "\n",
    "**P: Os funcionários do Google veem os dados que envio ao modelo?**\n",
    "R: Não, os funcionários do Google não têm acesso aos dados do cliente e todos os dados são criptografados em trânsito, em uso e em repouso.\n",
    "\n",
    "**P: O Google armazena algum dos dados do cliente que são enviados para o modelo?**\n",
    "R: Não, o Google não armazena dados de clientes. No entanto, o Google pode armazenar em cache temporariamente os dados do cliente durante a solicitação, como pipeline de ajuste de prompt e uso em batch.\n",
    "\n",
    "**P: O Google registra dados?**\n",
    "R: Não, o Google não loga os dados dos clientes. Os logs do lado do sistema ajudam o Google a garantir a integridade e a disponibilidade do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc389a25bf64"
   },
   "source": [
    "### IA Responsável\n",
    "LLMs podem traduzir linguagem, resumir texto, gerar escrita criativa, gerar código, chatbots e assistentes virtuais, além de complementar mecanismos de pesquisa e sistemas de recomendação. Ao mesmo tempo, como uma tecnologia em estágio inicial, seus recursos e usos em evolução podem criar aplicações incorretas, uso indevido e consequências não intencionais ou imprevistas. LLMs podem gerar resultados inesperados, incluindo texto ofensivo, insensível ou incorreto.\n",
    "\n",
    "Além disso, a incrível versatilidade dos LLMs também é o que torna difícil prever exatamente que tipos de saídas não intencionais ou imprevistas eles podem produzir. Dados esses riscos e complexidades, a API PaLM foi projetada com os [AI Principles do Google](https://ai.google/principles/) em mente. No entanto, é importante que os desenvolvedores entendam e testem seus modelos para implantá-los com segurança e responsabilidade. Para ajudar os desenvolvedores, o Generative AI Studio possui filtragem de conteúdo integrada e a API PaLM possui pontuação de atributo de segurança para ajudar os clientes a testar os filtros de segurança do Google e definir limites de confiança adequados para seu caso de uso e negócios. Consulte a seção [Filtros e atributos de segurança](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_filters_and_attributes) para saber mais.\n",
    "\n",
    "Quando a API PaLM é integrada ao caso de uso e contexto exclusivos de um cliente, considerações adicionais de IA Responsável e [limitações PaLM](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai #palm_limitations) precisam ser considerados. Incentivamos os clientes a usar *fairness*, interpretabilidade, privacidade e segurança [práticas recomendadas](https://ai.google/responsabilidades/responsible-ai-practices/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Instalando os SDK da Vertex AI e da Cloud Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-translate langchain chromadb --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para reiniciar o kernel ou use o botão para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "# # Reinicia automaticamente o kernel após as instalações para que seu ambiente possa acessar os novos pacotes\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se você estiver usando o **Colab** para executar este notebook, descomente a célula abaixo e continue.\n",
    "* Se você estiver usando o **Vertex AI Workbench**, confira as instruções de configuração [aqui](../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para realizar o processo adequado de inicialização da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[seu-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zjV4alsiVql"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "from google.cloud import bigquery\n",
    "from vertexai import language_models\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.document_loaders.dataframe import DataFrameLoader\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mU6EZEhakVu"
   },
   "source": [
    "## Geração de texto com `text-bison@001`\n",
    "\n",
    "O modelo de geração de texto da API PaLM que você usará neste notebook é `text-bison@001`. Já deixaremos seu objeto instanciado neste Jupyter notebook para uso futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando a função *wrapper* para utilizar os modelos em Português\n",
    "\n",
    "Até o momento desde treinamento, as API do Generative AI Studio suportam somente interações no idioma inglês. Para fazermos interações utilizando o idioma português, vamos utilizar a [Cloud Translation API](https://cloud.google.com/translate) para traduzir as nossas solicitações do português para o inglês e as respostas da API de inglês para português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "\n",
    "project_id = !gcloud config list project\n",
    "project_id = project_id[1].split('=')[1].strip()\n",
    "parent = f'projects/' + project_id\n",
    "\n",
    "\n",
    "def traduza(texto, idioma_destino):\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    response = client.translate_text(\n",
    "        parent=parent,\n",
    "        contents=[texto],\n",
    "        target_language_code=idioma_destino,\n",
    "        mime_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "    return response.translations[0].translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando as funções necessárias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define LLM classes and instantiate\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai import language_models\n",
    "\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.document_loaders.dataframe import DataFrameLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "\n",
    "def rate_limit(max_per_minute):\n",
    "  period = 60 / max_per_minute\n",
    "  while True:\n",
    "    before = time.time()\n",
    "    yield\n",
    "    after = time.time()\n",
    "    elapsed = after - before\n",
    "    sleep_time = max(0, period - elapsed)\n",
    "    if sleep_time > 0:\n",
    "      print(f'Sleeping {sleep_time:.1f} seconds')\n",
    "      time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class VertexEmbeddings(Embeddings):\n",
    "\n",
    "  def __init__(self, model, *, requests_per_minute=15):\n",
    "    self.model = model\n",
    "    self.requests_per_minute = requests_per_minute\n",
    "\n",
    "  def embed_documents(self, texts):\n",
    "    limiter = rate_limit(self.requests_per_minute)\n",
    "    results = []\n",
    "    docs = list(texts)\n",
    "\n",
    "    while docs:\n",
    "      # Working in batches of 2 because the API apparently won't let\n",
    "      # us send more than 2 documents per request to get embeddings.\n",
    "      head, docs = docs[:2], docs[2:]\n",
    "      chunk = self.model.get_embeddings(head)\n",
    "      results.extend(chunk)\n",
    "      next(limiter)\n",
    "\n",
    "    return [r.values for r in results]\n",
    "\n",
    "  def embed_query(self, text):\n",
    "    single_result = self.embed_documents([text])\n",
    "    return single_result[0]\n",
    "\n",
    "\n",
    "class VertexLLM(LLM):\n",
    "\n",
    "  model: language_models.TextGenerationModel\n",
    "  predict_kwargs: dict\n",
    "\n",
    "  def __init__(self, model, **predict_kwargs):\n",
    "    super().__init__(model=model, predict_kwargs=predict_kwargs)\n",
    "\n",
    "  @property\n",
    "  def _llm_type(self):\n",
    "    return 'vertex'\n",
    "\n",
    "  def _call(self, prompt, stop=None):\n",
    "    result = self.model.predict(prompt, **self.predict_kwargs)\n",
    "    return str(result)\n",
    "\n",
    "  @property\n",
    "  def _identifying_params(self):\n",
    "    return {}\n",
    "\n",
    "# NOTE: Use staging to get 100qps max throughput for embedding indexing\n",
    "# The embedding content is the same as production so you can use staging\n",
    "# for indexing and production for querying if desired.\n",
    "#language_models.TextEmbeddingModel._LLM_ENDPOINT_NAME = (\n",
    "#  'projects/678515165750/locations/us-central1/endpoints/8156038716377268224')\n",
    "\n",
    "REQUESTS_PER_MINUTE = 6000\n",
    "\n",
    "model = language_models.TextGenerationModel.from_pretrained('google/text-bison@001')\n",
    "llm = VertexLLM(\n",
    "  model,\n",
    "  max_output_tokens=256,\n",
    "  temperature=0.1,\n",
    "  top_p=0.8,\n",
    "  top_k=40\n",
    ")\n",
    "embedding = VertexEmbeddings(language_models.TextEmbeddingModel.from_pretrained('google/textembedding-gecko@001'), requests_per_minute=REQUESTS_PER_MINUTE)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do dataset no BigQuery\n",
    "\n",
    "Para termos uma estrutura pronta no projeto do laboratório para salvar e explorar dados, vamos criar um projeto no BigQuery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config list project\n",
    "project_id = project_id[1].split('=')[1].strip()\n",
    "parent = f'projects/' + project_id\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "dataset_id = f\"{project_id}.google_trends\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"\n",
    "dataset = client.create_dataset(dataset, timeout=30)\n",
    "print(\"Dataset criado com sucesso {}.{}\".format(project_id, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando os dados de Google Trends\n",
    "\n",
    "Os dados de Google Trends, ou seja os termos de busca mais utilizados em determinado dia/semana, são partes de um dataset público do BigQuery chamado `bigquery-public-data.google_trends`. Aqui vamos visualizar os dados disponíveis das duas últimas semanas e salvar essas informações em um Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "   refresh_date AS Day,\n",
    "   term AS Top_Term,\n",
    "   rank,\n",
    "FROM `bigquery-public-data.google_trends.top_terms`\n",
    "WHERE\n",
    "   rank <= 3\n",
    "   # AND refresh_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK)\n",
    "GROUP BY Day, Top_Term, rank\n",
    "ORDER BY Day DESC, rank\n",
    "\"\"\"\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "df = client.query(query).to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reailzando explorações dos dados utilizando as API do Vertex GenAI Studio\n",
    "\n",
    "Com os dados disponíveis em um dataframe, vamos primeiro utilizar as 5 (cinco) primeiras linhas do DataFrame para fazer uma experimentação rápida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googletrends_table = df.head(5).to_markdown()\n",
    "\n",
    "prompt = traduza(f\"\"\"\n",
    "  Use a tabela a seguir, em formato markdown, para responder a questão seguinte.\n",
    "\n",
    "  {googletrends_table}\n",
    "\n",
    "  Gere uma sentença que descreva as top-3 trends de 2023-07-06\n",
    "\"\"\", \"en\")\n",
    "\n",
    "traduza(model.predict(prompt).text, \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um dataframe com todo o conteúdo de Google Trends das últimas 2 semanas\n",
    "\n",
    "Com a experimentação realizada com as 5 (cinco) primeiras linhas, agora podemos seguir em frente e criar um dataframe que possua todos os top-3 das últmas duas semanas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for index, rows in df.iterrows():\n",
    "  text = \"\"\"On {0}, top {1} search term is {2}.\n",
    "  \"\"\".format(\n",
    "      rows['Day'],\n",
    "      rows['rank'],\n",
    "      rows['Top_Term'])\n",
    "  data.append(text)\n",
    "\n",
    "nl_googletrends_df = pd.DataFrame(data, columns=['text'])\n",
    "nl_googletrends_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando LangChain para criar uma estrutura de busca de informações\n",
    "\n",
    "Neste próximo passo vamos utilizar a biblioteca LangChain para gerar embeddings de todas as informações do DataFrame e depois armazenar como índices de busca dentro do diretório `dados-locais/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_path = './dados-locais'\n",
    "df_loader = DataFrameLoader(nl_googletrends_df, page_content_column=\"text\")\n",
    "\n",
    "googletrends_index = VectorstoreIndexCreator(\n",
    "    embedding=embedding, \n",
    "    vectorstore_kwargs={\n",
    "        'persist_directory': f'{persistent_path}/googletrends'\n",
    "    }).from_loaders([df_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(pergunta, max_results=5, threshold=0.5):\n",
    "    pergunta = traduza(pergunta, \"en\")\n",
    "    similar_docs = googletrends_index.vectorstore.similarity_search_with_score(pergunta, llm=llm, k=max_results)\n",
    "    filtered_docs = list(filter(lambda doc: doc[1] <= threshold, similar_docs))\n",
    "    context = \"\\n\".join([doc.page_content for doc, score in filtered_docs])\n",
    "    prompt = traduza(f\"\"\"\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "     Responda com base apenas nos dados de contexto.\n",
    "     Explique por que é um dos principais termos de pesquisa, especialmente naquele dia. O que aconteceu naquele dia?\n",
    "     Não invente informações. Não diga informações sobre as quais você não tem certeza.\n",
    "\n",
    "     Pergunta: {pergunta}\n",
    "     Responder:    \"\"\", \"en\")\n",
    "    resposta = model.predict(prompt)\n",
    "    return(traduza(resposta.text, 'pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando perguntas em linguagem natural\n",
    "\n",
    "Agora, considerando as informações de Google Trends das últimas duas semanas, podemos realizar perguntas, em linguagem natural, e buscar as informações obtidas a partir do BigQuery e que foram processadas pelo LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Qual é o top-1 termo de pesquisa em \"2023-07-04\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Qual é o top-2 termo de pesquisa de \"2023-06-12\"?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question('Quando que a busca pelo termo \"LSU Baseball\" estava no top-3 de termos de busca?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_palm_api.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
