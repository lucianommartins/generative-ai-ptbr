{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qxv4Sn9b8CE"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/get_started\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Acesse ai.google.dev</a>\n",
    "  </td>\n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/1_intro_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Execute este notebook no Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/1_intro_gemma.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Execute este notebook na Vertex AI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/1_intro_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />Consulte o código no GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXNm5_p_oxMF"
   },
   "source": [
    "# Primeiros passos com Gemma utilizando KerasNLP\n",
    "\n",
    "Este tutorial mostra como começar a usar Gemma usando [KerasNLP](https://keras.io/keras_nlp/). Gemma é uma família de modelos abertos leves e de última geração, construídos a partir da mesma pesquisa e tecnologia usada para criar os modelos Gemini. KerasNLP é uma coleção de modelos de processamento de linguagem natural (PNL) implementados em [Keras](https://keras.io/) e executáveis em JAX, PyTorch e TensorFlow.\n",
    "\n",
    "Neste tutorial, você usará o Gemma para gerar respostas de texto para vários prompts. Se você é novo no Keras, talvez queira ler [Introdução ao Keras](https://keras.io/getting_started/) antes de começar, mas não é necessário. Você aprenderá mais sobre Keras à medida que avança neste tutorial.\n",
    "\n",
    "**Importante:** Para uma melhor experiência de execução deste laboratório, é recomendado que se utilize uma máquina com aceleradores (GPU ou TPU). Em uma máquina sem aceleradores (como GPU) esta geração de conteúdo pode levar cerca de 80 segundos para ser concluída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mERVCCsGUPIJ"
   },
   "source": [
    "## Configurando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ6W7NzRe1VM"
   },
   "source": [
    "### Configurações para o uso do Gemma\n",
    "\n",
    "Para concluir este tutorial, primeiro você precisa seguir as instruções de configuração em [Configuração do Gemma](https://ai.google.dev/gemma/docs/setup). As instruções de configuração do Gemma mostram como fazer o seguinte:\n",
    "\n",
    "* Obtenha acesso a Gemma em kaggle.com.\n",
    "* Selecione um tempo de execução do Colab com recursos suficientes para execução\n",
    "   o modelo Gemma 2B.\n",
    "* Gere e configure um nome de usuário Kaggle e uma chave de API.\n",
    "\n",
    "Depois de concluir a configuração do Gemma, vá para a próxima seção, onde você definirá variáveis de ambiente para seu ambiente Colab.\n",
    "\n",
    "**TL;DR: Acesse https://www.kaggle.com/models/google/gemma e faça a solicitação de acesso ao modelo Gemma**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9oy3QUmXtSd"
   },
   "source": [
    "### Instalando dependências\n",
    "\n",
    "Instale os pacotes `pip` Keras e KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcGLzDeQ8NwN",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instale a última versão do Keras 3. Acesse https://keras.io/getting_started/ para maiores detalhes.\n",
    "!pip install -U keras-nlp\n",
    "!pip install -U keras>=3\n",
    "!pip install -U jax\n",
    "!pip install -U jaxlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gN-IVRC3dQe"
   },
   "source": [
    "### Configurando variáveis de ambiente necessárias para utilizar o Kaggle\n",
    "\n",
    "Configure as variáveis `KAGGLE_USERNAME` (contendo seu usuário Kaggle) e `KAGGLE_KEY` (com sua chave de acesso ao Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrBoa_Urw9Vx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Nota: `userdata.get` é uma API Colab. Se você não estiver usando o Colab, defina o ambiente\n",
    "# vars conforme apropriado para seu sistema.\n",
    "\n",
    "# descomente o par de linhas que faça mais sentido pra você:\n",
    "# # opção A: utilizando direto no Colab\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "\n",
    "# # opção B: utilizando na Vertex AI ou no seu ambiente pessoal\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = \"seu usuário\"\n",
    "# os.environ[\"KAGGLE_KEY\"] = \"sua chave de autenticação\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm5cVOFt5YvZ"
   },
   "source": [
    "### Escolhe o backend a ser utilizado com o Keras\n",
    "\n",
    "Keras é uma API de deep learning multi-backend de alto nível projetada para simplicidade e facilidade de uso. [Keras 3](https://keras.io/keras_3) permite escolher o backend: TensorFlow, JAX ou PyTorch. Todos os três funcionarão para este tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rS7ryTs5wjf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Ou \"tensorflow\" ou \"torch\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "599765c72722"
   },
   "source": [
    "### Importando os módulos necessários\n",
    "\n",
    "Importe o Keras e o KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2fa267d75bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsxDCbLN555T"
   },
   "source": [
    "## Defina um modelo\n",
    "\n",
    "KerasNLP fornece implementações de muitas [arquiteturas de modelos populares](https://keras.io/api/keras_nlp/models/). Neste tutorial, você criará um modelo usando `GemmaCausalLM`, um modelo Gemma ponta a ponta para modelagem de linguagem causal. Um modelo de linguagem causal prevê o próximo token com base nos tokens anteriores.\n",
    "\n",
    "Crie o modelo usando o método `from_preset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yygIK9DEIldp",
    "outputId": "f3963129-ec49-4411-a293-20036dc733da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrAWvsU6pI0E"
   },
   "source": [
    "`from_preset` instancia o modelo a partir de uma arquitetura e pesos predefinidos. No código acima, a string `\"gemma_2b_en\"` especifica a arquitetura predefinida: um modelo Gemma com 2 bilhões de parâmetros.\n",
    "\n",
    "Nota: Também está disponível um modelo Gemma com 7 bilhões de parâmetros. Para executar o modelo maior no Colab, você precisa de acesso às GPUs premium disponíveis em planos pagos. Como alternativa, você pode realizar [ajuste distribuído em um modelo Gemma 7B](https://ai.google.dev/gemma/docs/distributed_tuning) no Kaggle ou no Google Cloud.\n",
    "\n",
    "Use `summary` para obter mais informações sobre o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5nEbTdApL7W",
    "outputId": "7e053cb5-1bf9-40d2-edc4-e9b9da49b90c"
   },
   "outputs": [],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81KHdRYOrWYm"
   },
   "source": [
    "Como você pode ver no resumo, o modelo possui 2,5 bilhões de parâmetros treináveis.\n",
    "\n",
    "Nota: Para fins de nomeação do modelo (\"2B\"), a camada de embeddings não é contabilizada no número de parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOBW7piN5-sl"
   },
   "source": [
    "## Gerando textos\n",
    "\n",
    "Agora é hora de gerar algum texto! O modelo possui um método `generate` que gera texto com base em um prompt. O argumento opcional `max_length` especifica o comprimento máximo da sequência gerada.\n",
    "\n",
    "Experimente com a pergunta `\"Qual é o sentido da vida?\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aae5GHrdpj2_",
    "outputId": "3f794f43-e252-403e-8626-ee18ef7554aa"
   },
   "outputs": [],
   "source": [
    "gemma_lm.generate(\"Qual é o sentido da vida?\", max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH0eFH_DvYwM"
   },
   "source": [
    "Tente chamar `generate` novamente com um prompt diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEyTnnNGvgGG",
    "outputId": "71a38ab8-6513-4e38-972a-04a197f9a67f"
   },
   "outputs": [],
   "source": [
    "gemma_lm.generate(\"Como o cérebro funciona?\", max_length=64) # altere o valor do prompt para as perguntas que você quiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVlCnY7Gvm7U"
   },
   "source": [
    "Se você estiver executando back-ends JAX ou TensorFlow, notará que a segunda chamada `generate` retorna quase instantaneamente. Isso ocorre porque cada chamada para `generate` para um determinado tamanho de lote e `max_length` é compilada com XLA. A primeira execução é cara, mas as execuções subsequentes são muito mais rápidas.\n",
    "\n",
    "Você pode também utilizar prompts em \"batch\", utilizando uma lista como entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xV6vs8_C2BGt",
    "outputId": "9c55ebc2-3a85-4c3d-8069-6c52e921255d"
   },
   "outputs": [],
   "source": [
    "gemma_lm.generate(\n",
    "    [\"Qual é o sentido da vida?\",\n",
    "     \"Como o cérebro funciona?\"],\n",
    "    max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaVWoSpo3XyY"
   },
   "source": [
    "### Opcional: experimente um sampler diferente\n",
    "\n",
    "Você pode controlar a estratégia de geração para `GemmaCausalLM` definindo o argumento `sampler` em `compile()`. Por padrão, a amostragem [`\"greedy\"`](https://keras.io/api/keras_nlp/samplers/greedy_sampler/#greedysampler-class) será usada.\n",
    "\n",
    "Como experiência, tente definir uma estratégia [`\"top_k\"`](https://keras.io/api/keras_nlp/samplers/top_k_sampler/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mx55VQpN4DAK",
    "outputId": "17b3aaeb-eab2-4570-eb81-9145388bc1d9"
   },
   "outputs": [],
   "source": [
    "gemma_lm.compile(sampler=\"top_k\")\n",
    "gemma_lm.generate(\"Qual o sentido da vida?\", max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-okKgK4LfO0f"
   },
   "source": [
    "Enquanto o algoritmo `\"greedy\"` padrão sempre escolha o token com a maior probabilidade, o algoritmo top-K escolhe aleatoriamente o próximo token entre os tokens com a maior probabilidade K.\n",
    "\n",
    "Você não precisa especificar um sampler e pode ignorar o último trecho de código se ele não for útil para seu caso de uso. Se quiser saber mais sobre os samplers disponíveis, consulte [Samplers](https://keras.io/api/keras_nlp/samplers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBrbTYasoo-J"
   },
   "source": [
    "## Próximos passos\n",
    "\n",
    "Neste tutorial, você aprendeu como gerar texto usando KerasNLP e Gemma. Aqui estão algumas sugestões sobre o que aprender a seguir:\n",
    "\n",
    "* Aprenda como [realizar tuning de um modelo Gemma](https://ai.google.dev/gemma/docs/lora_tuning).\n",
    "* Aprenda como realizar [tuning distribuído e inferência em um modelo Gemma](https://ai.google.dev/gemma/docs/distributed_tuning).\n",
    "* Saiba mais sobre [integração do Gemma com Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n",
    "* Saiba como [usar modelos Gemma com Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "get_started.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "google": {
   "image_path": "/site-assets/images/marketing/gemma.png",
   "keywords": [
    "examples",
    "gemma",
    "python",
    "quickstart",
    "text"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
