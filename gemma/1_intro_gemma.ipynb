{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qxv4Sn9b8CE"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/get_started\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Acesse ai.google.dev</a>\n",
    "  </td>\n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/1_intro_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Execute este notebook no Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/1_intro_gemma.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Execute este notebook na Vertex AI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/1_intro_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />Consulte o código no GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXNm5_p_oxMF"
   },
   "source": [
    "# Primeiros passos com Gemma utilizando KerasNLP\n",
    "\n",
    "Este tutorial mostra como começar a usar Gemma usando [KerasNLP](https://keras.io/keras_nlp/). Gemma é uma família de modelos abertos leves e de última geração, construídos a partir da mesma pesquisa e tecnologia usada para criar os modelos Gemini. KerasNLP é uma coleção de modelos de processamento de linguagem natural (PNL) implementados em [Keras](https://keras.io/) e executáveis em JAX, PyTorch e TensorFlow.\n",
    "\n",
    "Neste tutorial, você usará o Gemma para gerar respostas de texto para vários prompts. Se você é novo no Keras, talvez queira ler [Introdução ao Keras](https://keras.io/getting_started/) antes de começar, mas não é necessário. Você aprenderá mais sobre Keras à medida que avança neste tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mERVCCsGUPIJ"
   },
   "source": [
    "## Configurando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ6W7NzRe1VM"
   },
   "source": [
    "### Configurações para o uso do Gemma\n",
    "\n",
    "Para concluir este tutorial, primeiro você precisa seguir as instruções de configuração em [Configuração do Gemma](https://ai.google.dev/gemma/docs/setup). As instruções de configuração do Gemma mostram como fazer o seguinte:\n",
    "\n",
    "* Obtenha acesso a Gemma em kaggle.com.\n",
    "* Selecione um tempo de execução do Colab com recursos suficientes para execução\n",
    "   o modelo Gemma 2B.\n",
    "* Gere e configure um nome de usuário Kaggle e uma chave de API.\n",
    "\n",
    "Depois de concluir a configuração do Gemma, vá para a próxima seção, onde você definirá variáveis de ambiente para seu ambiente Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9oy3QUmXtSd"
   },
   "source": [
    "### Instalando dependências\n",
    "\n",
    "Instale os pacotes `pip` Keras e KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcGLzDeQ8NwN"
   },
   "outputs": [],
   "source": [
    "# Instale a última versão do Keras 3. Acesse https://keras.io/getting_started/ para maiores detalhes.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gN-IVRC3dQe"
   },
   "source": [
    "### Configurando variáveis de ambiente necessárias para utilizar o Kaggle\n",
    "\n",
    "Configure as variáveis `KAGGLE_USERNAME` (contendo seu usuário Kaggle) e `KAGGLE_KEY` (com sua chave de acesso ao Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrBoa_Urw9Vx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Nota: `userdata.get` é uma API Colab. Se você não estiver usando o Colab, defina o ambiente\n",
    "# vars conforme apropriado para seu sistema.\n",
    "\n",
    "# descomente o par de linhas que faça mais sentido pra você:\n",
    "# # opção A: utilizando direto no Colab\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "\n",
    "# # opção B: utilizando na Vertex AI ou no seu ambiente pessoal\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = \"seu usuário\"\n",
    "# os.environ[\"KAGGLE_KEY\"] = \"sua chave de autenticação\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm5cVOFt5YvZ"
   },
   "source": [
    "### Escolhe o backend a ser utilizado com o Keras\n",
    "\n",
    "Keras é uma API de deep learning multi-backend de alto nível projetada para simplicidade e facilidade de uso. [Keras 3](https://keras.io/keras_3) permite escolher o backend: TensorFlow, JAX ou PyTorch. Todos os três funcionarão para este tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rS7ryTs5wjf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Ou \"tensorflow\" ou \"torch\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "599765c72722"
   },
   "source": [
    "### Importando os módulos necessários\n",
    "\n",
    "Importe o Keras e o KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2fa267d75bc"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsxDCbLN555T"
   },
   "source": [
    "## Defina um modelo\n",
    "\n",
    "KerasNLP fornece implementações de muitas [arquiteturas de modelos populares](https://keras.io/api/keras_nlp/models/). Neste tutorial, você criará um modelo usando `GemmaCausalLM`, um modelo Gemma ponta a ponta para modelagem de linguagem causal. Um modelo de linguagem causal prevê o próximo token com base nos tokens anteriores.\n",
    "\n",
    "Crie o modelo usando o método `from_preset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yygIK9DEIldp",
    "outputId": "f3963129-ec49-4411-a293-20036dc733da"
   },
   "outputs": [],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrAWvsU6pI0E"
   },
   "source": [
    "`from_preset` instancia o modelo a partir de uma arquitetura e pesos predefinidos. No código acima, a string `\"gemma_2b_en\"` especifica a arquitetura predefinida: um modelo Gemma com 2 bilhões de parâmetros.\n",
    "\n",
    "Nota: Também está disponível um modelo Gemma com 7 bilhões de parâmetros. Para executar o modelo maior no Colab, você precisa de acesso às GPUs premium disponíveis em planos pagos. Como alternativa, você pode realizar [ajuste distribuído em um modelo Gemma 7B](https://ai.google.dev/gemma/docs/distributed_tuning) no Kaggle ou no Google Cloud.\n",
    "\n",
    "Use `summary` para obter mais informações sobre o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5nEbTdApL7W",
    "outputId": "7e053cb5-1bf9-40d2-edc4-e9b9da49b90c"
   },
   "outputs": [],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81KHdRYOrWYm"
   },
   "source": [
    "Como você pode ver no resumo, o modelo possui 2,5 bilhões de parâmetros treináveis.\n",
    "\n",
    "Nota: Para fins de nomeação do modelo (\"2B\"), a camada de embeddings não é contabilizada no número de parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOBW7piN5-sl"
   },
   "source": [
    "## Gerando textos\n",
    "\n",
    "Agora é hora de gerar algum texto! O modelo possui um método `generate` que gera texto com base em um prompt. O argumento opcional `max_length` especifica o comprimento máximo da sequência gerada.\n",
    "\n",
    "Experimente com a pergunta `\"Qual é o sentido da vida?\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aae5GHrdpj2_",
    "outputId": "3f794f43-e252-403e-8626-ee18ef7554aa"
   },
   "outputs": [],
   "source": [
    "gemma_lm.generate(\"Qual é o sentido da vida?\", max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH0eFH_DvYwM"
   },
   "source": [
    "Tente chamar `generate` novamente com um prompt diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEyTnnNGvgGG",
    "outputId": "71a38ab8-6513-4e38-972a-04a197f9a67f"
   },
   "outputs": [],
   "source": [
    "gemma_lm.generate(\"Como o cérebro funciona?\", max_length=64) # altere o valor do prompt para as perguntas que você quiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVlCnY7Gvm7U"
   },
   "source": [
    "Se você estiver executando back-ends JAX ou TensorFlow, notará que a segunda chamada `generate` retorna quase instantaneamente. Isso ocorre porque cada chamada para `generate` para um determinado tamanho de lote e `max_length` é compilada com XLA. A primeira execução é cara, mas as execuções subsequentes são muito mais rápidas.\n",
    "\n",
    "Você pode também utilizar prompts em \"batch\", utilizando uma lista como entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xV6vs8_C2BGt",
    "outputId": "9c55ebc2-3a85-4c3d-8069-6c52e921255d"
   },
   "outputs": [],
   "source": [
    "gemma_lm.generate(\n",
    "    [\"Qual é o sentido da vida?\",\n",
    "     \"Como o cérebro funciona?\"],\n",
    "    max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaVWoSpo3XyY"
   },
   "source": [
    "### Opcional: experimente um sampler diferente\n",
    "\n",
    "Você pode controlar a estratégia de geração para `GemmaCausalLM` definindo o argumento `sampler` em `compile()`. Por padrão, a amostragem [`\"greedy\"`](https://keras.io/api/keras_nlp/samplers/greedy_sampler/#greedysampler-class) será usada.\n",
    "\n",
    "Como experiência, tente definir uma estratégia [`\"top_k\"`](https://keras.io/api/keras_nlp/samplers/top_k_sampler/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mx55VQpN4DAK",
    "outputId": "17b3aaeb-eab2-4570-eb81-9145388bc1d9"
   },
   "outputs": [],
   "source": [
    "gemma_lm.compile(sampler=\"top_k\")\n",
    "gemma_lm.generate(\"Qual o sentido da vida?\", max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-okKgK4LfO0f"
   },
   "source": [
    "Enquanto o algoritmo `\"greedy\"` padrão sempre escolha o token com a maior probabilidade, o algoritmo top-K escolhe aleatoriamente o próximo token entre os tokens com a maior probabilidade K.\n",
    "\n",
    "Você não precisa especificar um sampler e pode ignorar o último trecho de código se ele não for útil para seu caso de uso. Se quiser saber mais sobre os samplers disponíveis, consulte [Samplers](https://keras.io/api/keras_nlp/samplers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBrbTYasoo-J"
   },
   "source": [
    "## Próximos passos\n",
    "\n",
    "Neste tutorial, você aprendeu como gerar texto usando KerasNLP e Gemma. Aqui estão algumas sugestões sobre o que aprender a seguir:\n",
    "\n",
    "* Aprenda como [realizar tuning de um modelo Gemma](https://ai.google.dev/gemma/docs/lora_tuning).\n",
    "* Aprenda como realizar [tuning distribuído e inferência em um modelo Gemma](https://ai.google.dev/gemma/docs/distributed_tuning).\n",
    "* Saiba mais sobre [integração do Gemma com Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n",
    "* Saiba como [usar modelos Gemma com Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "get_started.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "google": {
   "image_path": "/site-assets/images/marketing/gemma.png",
   "keywords": [
    "examples",
    "gemma",
    "python",
    "quickstart",
    "text"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
