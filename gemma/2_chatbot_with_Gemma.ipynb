{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60KmTK7o6ppd",
    "papermill": {
     "duration": 0.007422,
     "end_time": "2024-02-21T09:55:26.786189",
     "exception": false,
     "start_time": "2024-02-21T09:55:26.778767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This colab is the chat only version of this notebook: [bit.ly/gemma-pirate-demo](https://bit.ly/gemma-pirate-demo)\n",
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoAJlrr6LUyj",
    "papermill": {
     "duration": 0.012321,
     "end_time": "2024-02-21T09:55:26.80406",
     "exception": false,
     "start_time": "2024-02-21T09:55:26.791739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/get_started\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Acesse ai.google.dev</a>\n",
    "  </td>\n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/2_chatbot_with_Gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Execute este notebook no Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/2_chatbot_with_Gemma.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Execute este notebook na Vertex AI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/lucianommartins/generative-ai-ptbr/blob/main/gemma/2_chatbot_with_Gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />Consulte o c√≥digo no GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um chatbot com modelos Gemma\n",
    "\n",
    "Este tutorial mostra como criar um chatbot utilizando os modelos Google Gemma. LLMs s√£o muito bons para criar assistentes ou chatbots porque podem dar respostas muito boas e uma conversa com um LLM s√≥ faz sentido quando ele consegue lembrar o que foi perguntado antes.\n",
    "\n",
    "**Importante:** Para uma melhor experi√™ncia de execu√ß√£o deste laborat√≥rio, √© recomendado que se utilize uma m√°quina com aceleradores (GPU ou TPU). Em uma m√°quina sem aceleradores (como GPU) esta gera√ß√£o de conte√∫do pode levar cerca de 80 segundos para ser conclu√≠da."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQNKonJS-3Qv"
   },
   "source": [
    "### Instalando depend√™ncias\n",
    "\n",
    "Instale os pacotes `pip` Keras e KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWEzVJR4Fx9g",
    "outputId": "7988c955-5dd0-464a-9854-20adc7d85ac3",
    "papermill": {
     "duration": 37.05282,
     "end_time": "2024-02-21T09:56:03.93859",
     "exception": false,
     "start_time": "2024-02-21T09:55:26.88577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instale a √∫ltima vers√£o do Keras 3. Acesse https://keras.io/getting_started/ para maiores detalhes.\n",
    "!pip install -U keras-nlp\n",
    "!pip install -U keras>=3\n",
    "!pip install -U jax\n",
    "!pip install -U jaxlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configura√ß√µes para o uso do Gemma\n",
    "\n",
    "Para concluir este tutorial, primeiro voc√™ precisa seguir as instru√ß√µes de configura√ß√£o em [Configura√ß√£o do Gemma](https://ai.google.dev/gemma/docs/setup). As instru√ß√µes de configura√ß√£o do Gemma mostram como fazer o seguinte:\n",
    "\n",
    "* Obtenha acesso a Gemma em kaggle.com.\n",
    "* Selecione um tempo de execu√ß√£o do Colab com recursos suficientes para execu√ß√£o\n",
    "   o modelo Gemma 2B.\n",
    "* Gere e configure um nome de usu√°rio Kaggle e uma chave de API.\n",
    "\n",
    "Depois de concluir a configura√ß√£o do Gemma, v√° para a pr√≥xima se√ß√£o, onde voc√™ definir√° vari√°veis de ambiente para seu ambiente Colab.\n",
    "\n",
    "**TL;DR: Acesse https://www.kaggle.com/models/google/gemma e fa√ßa a solicita√ß√£o de acesso ao modelo Gemma**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando vari√°veis de ambiente necess√°rias para utilizar o Kaggle\n",
    "\n",
    "Configure as vari√°veis `KAGGLE_USERNAME` (contendo seu usu√°rio Kaggle) e `KAGGLE_KEY` (com sua chave de acesso ao Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Nota: `userdata.get` √© uma API Colab. Se voc√™ n√£o estiver usando o Colab, defina o ambiente\n",
    "# vars conforme apropriado para seu sistema.\n",
    "\n",
    "# descomente o par de linhas que fa√ßa mais sentido pra voc√™:\n",
    "# # op√ß√£o A: utilizando direto no Colab\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "\n",
    "# # op√ß√£o B: utilizando na Vertex AI ou no seu ambiente pessoal\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = \"seu usu√°rio\"\n",
    "# os.environ[\"KAGGLE_KEY\"] = \"sua chave de autentica√ß√£o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhe o backend a ser utilizado com o Keras\n",
    "\n",
    "Keras √© uma API de deep learning multi-backend de alto n√≠vel projetada para simplicidade e facilidade de uso. [Keras 3](https://keras.io/keras_3) permite escolher o backend: TensorFlow, JAX ou PyTorch. Todos os tr√™s funcionar√£o para este tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEgg_OVIL2HY",
    "papermill": {
     "duration": 0.01342,
     "end_time": "2024-02-21T09:56:12.117529",
     "exception": false,
     "start_time": "2024-02-21T09:56:12.104109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "# pr√©-aloque 100% da mem√≥eria da TPU pra diminuir fragmenta√ß√£o de mem√≥ria\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# por reproducibilidade\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJWcCDctpzBK"
   },
   "source": [
    "## Defina um modelo\n",
    "\n",
    "KerasNLP fornece implementa√ß√µes de muitas [arquiteturas de modelos populares](https://keras.io/api/keras_nlp/models/). Neste tutorial, voc√™ criar√° um modelo usando `GemmaCausalLM`, um modelo Gemma ponta a ponta para modelagem de linguagem causal. Um modelo de linguagem causal prev√™ o pr√≥ximo token com base nos tokens anteriores.\n",
    "\n",
    "Voc√™ usar√° a vers√£o Afinada com Instru√ß√£o porque √© a vers√£o que foi preparada para manter conversas e tirar d√∫vidas de forma mais natural.\n",
    "\n",
    "Crie o modelo usando o m√©todo `from_preset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5H7oE9MtnBA",
    "outputId": "ee4f3b23-1daa-4fd8-d5db-4eb3ba1a8514"
   },
   "outputs": [],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uma fun√ß√£o de apoio para facilitar a formata√ß√£o das intera√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzhbIuGotgda"
   },
   "outputs": [],
   "source": [
    "# formatting utility\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "def display_chat(prompt, text):\n",
    "  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  text = textwrap.indent(text, '> ', predicate=lambda _: True)\n",
    "  formatted_text = \"<font size='+1' color='teal'>ü§ñ\\n\\n\" + text + \"\\n</font>\"\n",
    "  return Markdown(formatted_prompt+formatted_text)\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc0ZzYIW0TSN",
    "papermill": {
     "duration": 0.005944,
     "end_time": "2024-02-21T09:58:46.139044",
     "exception": false,
     "start_time": "2024-02-21T09:58:46.1331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vamos conversar\n",
    "\n",
    "Carregamos o modelo ajustado por instru√ß√£o `gemma_instruct_2b_en`, que compreende os seguintes tokens de turno:\n",
    "```\n",
    "<start_of_turn>usu√°rio\\n  ... <end_of_turn>\\n\n",
    "<start_of_turn>modelo\\n ... <end_of_turn>\\n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CPDXJ-z-Ydv"
   },
   "source": [
    "### Fun√ß√£o de apoio para o chat (para gerenciar o estado da discuss√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoMKoLLNE0S4"
   },
   "outputs": [],
   "source": [
    "class ChatState():\n",
    "  \"\"\"\n",
    "  Manages the conversation history for a turn-based chatbot\n",
    "  Follows the turn-based conversation guidelines of the Gemma family of models.\n",
    "  documented here: https://ai.google.dev/gemma/docs/formatting\n",
    "  \"\"\"\n",
    "\n",
    "  __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n",
    "  __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n",
    "  __END_TURN__ = \"<end_of_turn>\\n\"\n",
    "\n",
    "  def __init__(self, model, system=\"\"):\n",
    "    \"\"\"\n",
    "    Initializes the chat state.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to use for generating responses.\n",
    "        system: (Optional) System instructions or bot description.\n",
    "    \"\"\"\n",
    "    self.model = model\n",
    "    self.system = system\n",
    "    self.history = []\n",
    "\n",
    "  def add_to_history_as_user(self, message):\n",
    "      \"\"\"\n",
    "      Adds a user message to the history with start/end turn markers.\n",
    "      \"\"\"\n",
    "      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n",
    "\n",
    "  def add_to_history_as_model(self, message):\n",
    "      \"\"\"\n",
    "      Adds a model response to the history with start/end turn markers.\n",
    "      \"\"\"\n",
    "      self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n",
    "\n",
    "  def get_history(self):\n",
    "      \"\"\"\n",
    "      Returns the entire chat history as a single string.\n",
    "      \"\"\"\n",
    "      return \"\".join([*self.history])\n",
    "\n",
    "  def get_full_prompt(self):\n",
    "    \"\"\"\n",
    "    Builds the prompt for the language model, including history and system description.\n",
    "    \"\"\"\n",
    "    prompt = self.get_history() + self.__START_TURN_MODEL__\n",
    "    if len(self.system)>0:\n",
    "      prompt = self.system + \"\\n\" + prompt\n",
    "    return prompt\n",
    "\n",
    "  def send_message(self, message):\n",
    "    \"\"\"\n",
    "    Handles sending a user message and getting a model response.\n",
    "\n",
    "    Args:\n",
    "        message: The user's message.\n",
    "\n",
    "    Returns:\n",
    "        The model's response.\n",
    "    \"\"\"\n",
    "    self.add_to_history_as_user(message)\n",
    "    prompt = self.get_full_prompt()\n",
    "    response = self.model.generate(prompt, max_length=1024)\n",
    "    result = response.replace(prompt, \"\")  # Extract only the new response\n",
    "    self.add_to_history_as_model(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "eI1p0iZMh3ef",
    "outputId": "26208091-3a16-47b2-aef7-a568a53008ea"
   },
   "outputs": [],
   "source": [
    "chat = ChatState(gemma_lm)\n",
    "message = \"Diga-me, em poucas palavras, como calcular todos os n√∫meros primos at√© 1000?\"\n",
    "display_chat(message, chat.send_message(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "0drcbj-66wCA",
    "outputId": "ba1ff700-1cbe-4006-b7d6-dc7d73c791bd"
   },
   "outputs": [],
   "source": [
    "message = \"Agora em Python! Sem usar Numpy, por favor!\"\n",
    "display_chat(message, chat.send_message(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "KMHW4chPjk8E",
    "outputId": "9821fc87-5140-454b-a392-db3b6d362b87"
   },
   "outputs": [],
   "source": [
    "message = \"Obrigado, assim funciona. Pode me explicar esse c√≥digo?\"\n",
    "display_chat(message, chat.send_message(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "id": "2G39JfiQidlc",
    "outputId": "54fdfd1f-cabe-407f-81e0-ab391150d2e8"
   },
   "outputs": [],
   "source": [
    "message = \"Massa! Agora adicione essas explica√ß√µes como coment√°rios no c√≥digo.\"\n",
    "display_chat(message, chat.send_message(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r89h5ia8zBim"
   },
   "source": [
    "Vamos tentar a resposta gerada. Apenas copiar o c√≥digo para a pr√≥xima c√©lula e execut√°-lo deve nos dar as respostas corretas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMGEujcwivLp",
    "outputId": "0f3c0593-50bb-4902-feea-00bb4d745add"
   },
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "  \"\"\"\n",
    "  Checks if a number is prime.\n",
    "\n",
    "  Args:\n",
    "    n: The number to check.\n",
    "\n",
    "  Returns:\n",
    "    True if n is prime, False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  # If n is less than or equal to 1, it is not prime.\n",
    "  if n <= 1:\n",
    "    return False\n",
    "\n",
    "  # Iterate through all the numbers from 2 to the square root of n.\n",
    "  for i in range(2, int(n**0.5) + 1):\n",
    "    # If n is divisible by any of the numbers in the range from 2 to the square root of n, it is not prime.\n",
    "    if n % i == 0:\n",
    "      return False\n",
    "\n",
    "  # If no divisors are found, n is prime.\n",
    "  return True\n",
    "\n",
    "\n",
    "# Initialize an empty list to store prime numbers.\n",
    "primes = []\n",
    "\n",
    "# Iterate through all the numbers from 2 to 1000.\n",
    "for i in range(2, 1001):\n",
    "  # If the number is prime, add it to the list.\n",
    "  if is_prime(i):\n",
    "    primes.append(i)\n",
    "\n",
    "# Print the prime numbers.\n",
    "print(primes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSmnAXd4zbxD"
   },
   "source": [
    "voc√™ tamb√©m pode ver como todo o contexto foi mantido pela classe Chat, mas chamando o m√©todo `get_history`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kAK9CUsh7ak",
    "outputId": "1c243e4d-958a-4357-84d2-9ebb0ff9d493"
   },
   "outputs": [],
   "source": [
    "print(chat.get_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzKsCGIN0yX5",
    "papermill": {
     "duration": 0.148912,
     "end_time": "2024-02-21T10:06:32.078788",
     "exception": false,
     "start_time": "2024-02-21T10:06:31.929876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pr√≥ximos passos\n",
    "\n",
    "Neste tutorial, voc√™ aprendeu como conversar com o modelo ajustado de instru√ß√£o Gemma 2B usando Keras no JAX.\n",
    "\n",
    "Na pr√≥xima etapa, voc√™ pode ajustar o modelo para ter um tom ou voz espec√≠fico e usar esse modelo na classe de bate-papo.\n",
    "\n",
    "Aqui est√£o algumas sugest√µes sobre como ajustar o modelo usando Keras e JAX:\n",
    "* [Treinamento distribu√≠do com Keras 3](https://keras.io/guides/distribution/).\n",
    "* [Escrever um loop de treinamento personalizado para um modelo Keras em JAX](https://keras.io/guides/writing_a_custom_training_loop_in_jax/).\n",
    "\n",
    "E mais alguns tutoriais b√°sicos do Gemma:\n",
    "\n",
    "* [Primeiros passos com Keras Gemma](https://ai.google.dev/gemma/docs/get_started).\n",
    "* [Tuning do modelo Gemma na GPU](https://ai.google.dev/gemma/docs/lora_tuning).\n",
    "* Saiba mais sobre [integra√ß√£o do Gemma com Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n",
    "* Saiba como [usar modelos Gemma com Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "modelInstanceId": 5172,
     "sourceId": 10262,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 672.253242,
   "end_time": "2024-02-21T10:06:37.294427",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T09:55:25.041185",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
